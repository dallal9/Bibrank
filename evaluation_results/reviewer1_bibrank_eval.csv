Abstract,gold,Rating1,scake,Rating2,sgrank,Rating3,yake,Rating4,textrank,Rating5,PositionRank,Rating6,Bibrank,Rating7,Keybert,Rating8
" primarily a graph-theoretic concept, helps define the {\em fault tolerance\/} of wireless sensor networks (wsns) in the sense that it enables the sensors to communicate with each other so their sensed data can reach the sink. on the other hand, {\em sensing coverage}, an intrinsic architectural feature of wsns plays an important role in meeting application-specific requirements, for example, to reliably extract relevant data about a sensed field. sensing coverage and network connectivity are not quite orthogonal concepts. in fact, it has been proven that connectivity strongly depends on coverage and hence considerable attention has been paid to establish tighter connection between them although only loose lower bound on network connectivity of wsns is known. in this article, we investigate connectivity based on the degree of sensing coverage by studying {\em k-covered\/} wsns, where every location in the field is simultaneously covered (or sensed) by at least {\em k\/} sensors (property known as {\em k-coverage}, where {\em k\/} is the {\em degree of coverage\/}). we observe that to derive network connectivity of {\em k\/} -covered wsns, it is necessary to compute the sensor spatial density required to guarantee {\em k\/} -coverage. more precisely, we propose to use a model, called the {\em reuleaux triangle}, to characterize {\em k\/} -coverage with the help of helly's theorem and the analysis of the intersection of sensing disks of {\em k\/} sensors. using a deterministic approach, we show that the sensor spatial density to guarantee {\em k\/} -coverage of a convex field is proportional to {\em k\/} and inversely proportional to the sensing range of the sensors. we also prove that network connectivity of {\em k\/} -covered wsns is higher than their sensing coverage {\em k}. furthermore, we propose a new measure of fault tolerance for {\em k\/} -covered wsns, called {\em conditional fault tolerance}, based on the concepts of {\em conditional connectivity\/} and {\em forbidden faulty sensor set\/} that includes all the neighbors of a given sensor. we prove that {\em k\/} -covered wsns can sustain a large number of sensor failures provided that the faulty sensor set does not include a forbidden faulty sensor set.","connectivity', 'coverage', 'fault tolerance', 'k -covered wireless sensor networks'",5,"em connectivity', 'em k', 'em fault tolerance', 'em conditional fault tolerance', 'em degree', 'em conditional connectivity', 'em reuleaux triangle', 'network connectivity', 'wireless sensor network', 'sensor spatial density'",5,"sensor spatial density', '\\em k\\/', 'network connectivity', 'orthogonal concept', 'tight connection', 'considerable attention', '\\em degree', 'wsns', '\\em reuleaux', 'sensed field'",4,"em', 'k', 'sensor', 'wsns', 'sense', 'connectivity', 'coverage', 'network', 'em k', 'concept'",5,"wireless sensor network', 'em conditional fault tolerance', 'forbidden faulty sensor', 'faulty sensor set', 'sensor spatial density', 'em connectivity', 'em fault tolerance', 'sensor failure', 'em conditional connectivity', 'network connectivity'",5,"network connectivity', 'wireless sensor networks', 'conditional connectivity', 'faulty sensor set', 'connectivity', 'faulty sensor', 'conditional fault tolerance', 'sensor failures', 'sensors', 'theoretic concept'",5,"conditional fault tolerance', 'network connectivity', 'fault tolerance', 'wireless sensor networks', 'coverage', 'conditional connectivity', 'connectivity', 'faulty sensor set', 'faulty sensor', 'sensor failures'",4,"prove network connectivity', 'derive network connectivity', 'investigate connectivity based', 'sensing coverage network', 'wireless sensor networks', 'graph theoretic concept', 'sensing coverage em', 'guarantee em coverage', 'covered wsns called', 'wsns called em'",3
"wireless sensor network (wsn) testbeds are useful because they provide a way to test applications in an environment that makes it easy to deploy experiments, configure them statically or dynamically, and gather performance information. however, wsns are typically composed of low-cost devices and tend to be unreliable, with failures a common phenomenon. accurate knowledge of network health status, including nodes and links of each type, is critical for correctly configuring applications on wsn testbeds and for interpreting the data collected from them.\par in this article we present a stabilizing protocol, chowkidar, that provides accurate and efficient network health monitoring in wsns. our approach adapts the well-known problem of message-passing rooted spanning tree construction and its use in propagation of information with feedback (pif) for the case of a wsn. the chowkidar protocol is initiated upon demand; that is, it does not involve ongoing maintenance, and it terminates with accurate results, including detection of failure and restart during the monitoring process. chowkidar is distinguished from others in two important ways. given the resource constraints of wsns, it is message-efficient in that it uses only a few messages per node. also, it tolerates ongoing node and link failure and node restart, in contrast to requiring that faults stop during convergence.\par we have implemented the chowkidar protocol as part of enabling a network health status service that is tightly integrated with a remotely accessible wireless sensor network testbed, kansei, at the ohio state university. we present experimental results from this testbed that validate the correctness and performance of chowkidar. we also report on initial experiences and lessons learnt from the integration of chowkidar with kansei, including feedback from both testbed users and administrators who have found chowkidar to be a useful tool for improving the accuracy and efficiency of testbed experimentation and maintenance, and the need for well-defined policies to address issues such as minimizing interference with concurrently running experiments. finally, we discuss extensions that enhance the functionality and usability of chowkidar.","health monitoring', 'pif', 'protocol architecture', 'stabilization', 'tree protocols', 'wireless sensor networks'",5,"accessible wireless sensor network', 'efficient network health monitoring', 'network health status service', 'wsn testbed', 'chowkidar protocol', 'ongoing node', 'node restart', 'accurate result', 'wsns', 'accurate knowledge'",5,"wireless sensor network', 'network health status', 'chowkidar', 'useful', 'wsns', 'way', 'wsn testbed', 'chowkidar protocol', 'configuring application', 'node'",5,"chowkidar', 'network', 'node', 'wsn', 'wsns', 'health', 'protocol', 'accurate', 'failure', 'message'",5,"efficient network health monitoring', 'network health status service', 'accessible wireless sensor network', 'chowkidar protocol', 'ongoing node', 'node restart', 'accurate result', 'wsn testbed', 'ohio state university', 'accurate knowledge'",5,"wireless sensor network', 'network health status', 'network', 'wireless', 'sensor', 'wsn testbeds', 'chowkidar protocol', 'chowkidar', 'wsn', 'health'",4,"wireless sensor network', 'resource constraints', 'usability', 'tree construction', 'wsn testbeds', 'network health status', 'useful tool', 'performance information', 'testbeds', 'applications'",3,"correctly configuring applications', 'network wsn testbeds', 'sensor network testbed', 'enhance functionality usability', 'dynamically gather performance', 'makes easy deploy', 'wsns typically', 'wireless sensor network', 'easy deploy experiments', 'implemented chowkidar protocol'",3
"we investigate the complexity of algorithms in message-driven models. in such models, events in the computation can only be caused by message receptions, but not by the passage of time. hutle and widder 2005a have shown that there is no deterministic message-driven self-stabilizing implementation of the eventually strong failure detector and thus \omega in systems with uncertainty in message delays and channels of unknown capacity using only bounded space. under stronger assumptions it was shown that even the eventually perfect failure detector can be implemented in message-driven systems consisting of at least {\em f\/} + 2 processes ({\em f\/} being the upper bound on the number of processes that crash during an execution).\par in this article we show that {\em f\/} + 2 is in fact a lower bound in message-driven systems, even if nonstabilizing algorithms are considered. this contrasts time-driven models where {\em f\/} + 1 is sufficient for failure detector implementations.\par moreover, we investigate algorithms where not all processes send message, that is, are active, but some (in a predetermined set) remain passive. here, we show that the {\em f\/} + 2 processes required for message-driven systems must be active, while in time-driven systems it suffices that {\em f\/} processes are active.\par we also provide message-driven implementations of \omega . our algorithms are efficient in the sense that not all processes have to send messages forever, which is an improvement to previous message-driven failure detector implementations.","fault tolerance', 'lower bound', 'message-driven distributed algorithm', 'unreliable failure detectors'",3,"message reception', 'deterministic message', 'message delay', 'previous message', 'algorithm', 'em f', 'model', 'strong failure detector', 'failure detector implementation', 'perfect failure detector'",3,"\\em f\\/', 'perfect failure detector', 'failure detector implementation', 'message', 'strong failure detector', 'algorithm', 'model', 'strong assumption', 'unknown capacity', 'system'",3,"message', 'f', 'em', 'process', 'system', 'failure', 'detector', 'algorithm', 'model', 'em f'",2,"failure detector implementation', 'strong failure detector', 'deterministic message', 'previous message', 'message delay', 'message reception', 'perfect failure detector', 'process', 'strong assumption', 'system'",5,"message receptions', 'deterministic message', 'message delays', 'previous message', 'message', 'failure detector implementations', 'algorithms', 'strong failure detector', 'perfect failure detector', 'such models'",5,"algorithms', 'complexity', 'message receptions', 'deterministic message', 'message delays', 'previous message', 'message', 'failure detector implementations', 'strong failure detector', 'perfect failure detector'",5,"complexity algorithms message', 'send messages forever', 'message driven implementations', 'investigate algorithms', 'investigate complexity algorithms', 'bound number processes', 'omega algorithms efficient', 'message receptions', 'em upper bound', 'processes send messages'",5
"in this article, we show that some fundamental self- and snap-stabilizing wave protocols (e.g., token circulation, {\em pif}, etc.) implicitly assume a very light property that we call {\em breakingin}. we prove that {\em breakingin\/} is strictly induced by self- and snap-stabilization. combined with a transformer, {\em breakingin\/} allows to easily turn the non-fault-tolerant versions of those protocols into snap-stabilizing versions. unlike the previous solutions, the transformed protocols are very efficient and work at least with the same daemon as the initial versions extended to satisfy {\em breakingin}. finally, we show how to use an additional property of the transformer to design snap-stabilizing extensions of those fundamental protocols like mutual exclusion.","self- and snap-stabilization', 'transformer', 'wave protocols'",4,"em breakingin', 'em pif', 'fundamental protocol', 'wave protocol', 'transformed protocol', 'fundamental self-', 'article', 'tolerant version', 'initial version', 'snap'",5,"\\em breakingin\\/', 'mutual exclusion', 'additional property', 'initial version', 'fundamental protocol', 'transformed protocol', 'previous solution', 'tolerant version', 'light property', '\\em pif'",5,"em', 'snap', 'protocol', 'breakingin', 'version', 'self-', 'transformer', 'fundamental', 'property', 'em breakingin'",5,"fundamental protocol', 'wave protocol', 'transformed protocol', 'tolerant version', 'initial version', 'fundamental self-', 'additional property', 'light property', 'em pif', 'em breakingin'",5,"fundamental protocols', 'fundamental self', 'wave protocols', 'fundamental', 'protocols', 'non-fault-tolerant versions', 'light property', 'initial versions', 'self', 'additional property'",5,"fundamental protocols', 'fundamental self', 'wave protocols', 'fundamental', 'protocols', 'stabilization', 'non-fault-tolerant versions', 'light property', 'initial versions', 'self'",4,"extended satisfy em', 'fundamental protocols like', 'protocols efficient work', 'wave protocols token', 'em breakingin allows', 'property em breakingin', 'breakingin allows easily', 'transformed protocols efficient', 'efficient work daemon', 'stabilizing wave protocols'",5
"in this article, we quantify the amount of ``practical'' information (i.e., views obtained from the neighbors, colors attributed to the nodes and links) to obtain ``theoretical'' information (i.e., the local topology of the network up to distance {\em k\/}) in anonymous networks. in more detail, we show that a coloring at distance 2 {\em k\/} + 1 is necessary and sufficient to obtain the local topology at distance {\em k\/} that includes outgoing links. this bound drops to 2 {\em k\/} when outgoing links are not needed. a second contribution of this article deals with color bootstrapping (from which local topology can be obtained using the aforementioned mechanisms). on the negative side, we show that ({\em i\/}) with a distributed daemon, it is impossible to achieve deterministic color bootstrap, even if the whole network topology can be instantaneously obtained, and ({\em ii\/}) with a central daemon, it is impossible to achieve distance {\em m\/} when instantaneous topology knowledge is limited to {\em m\/} - 1. on the positive side, we show that ({\em i\/}) under the {\em k\/} -central daemon, deterministic self-stabilizing bootstrap of colors up to distance {\em k\/} is possible provided that {\em k\/} -local topology can be instantaneously obtained, and ({\em ii\/}) under the distributed daemon, probabilistic self-stabilizing bootstrap is possible for any range.","anonymous networks', 'daemon', 'stabilization', 'topology'",4,"em k', 'em m', 'em i', 'article', 'local topology', 'network topology', 'instantaneous topology knowledge', 'deterministic color bootstrap', 'distance', 'color bootstrapping'",4,"\\em k\\/', 'local topology', 'outgoing link', 'anonymous network', 'instantaneous topology knowledge', '\\em i\\/', 'deterministic color bootstrap', 'second contribution', 'color bootstrapping', 'aforementioned mechanism'",5,"em', 'k', 'distance', 'topology', 'color', 'daemon', 'local', 'link', 'em k', 'network'",4,"deterministic color bootstrap', 'network topology', 'instantaneous topology knowledge', 'local topology', 'color bootstrapping', 'outgoing link', 'anonymous network', 'em m', 'em k', 'em i'",5,"article deals', 'deterministic color bootstrap', 'local topology', 'whole network topology', 'article', 'outgoing links', 'instantaneous topology knowledge', 'color bootstrapping', 'amount', 'information'",4,"local topology', 'whole network topology', 'instantaneous topology knowledge', 'coloring', 'deterministic color bootstrap', 'color bootstrapping', 'article deals', 'bootstrap', 'article', 'outgoing links'",3,"em distributed daemon', 'obtain theoretical information', 'topology obtained using', 'ii distributed daemon', 'obtain local topology', 'deterministic color bootstrap', 'achieve distance em', 'network distance em', 'achieve deterministic color', 'local topology network'",5
"reaching agreement among a set of mobile robots is one of the most fundamental issues in distributed robotic systems. this problem is often illustrated by the gathering problem, where the robots must self-organize and meet at some location not determined in advance, and without the help of some global coordinate system. while very simple to express, this problem has the advantage of retaining the inherent difficulty of agreement, namely the question of breaking symmetry between robots. in previous works, it has been proved that the gathering problem is solvable in asynchronous model with oblivious (i.e., memory-less) robots and limited visibility, as long as the robots share the knowledge of some direction, as provided by a compass. however, the problem has no solution in the semi-synchronous model when robots do not share a compass, or when they cannot detect multiplicity.\par in this article, we define a model in which compasses may be unreliable, and study the solvability of gathering oblivious mobile robots with limited visibility in the semi-synchronous model. in particular, we give an algorithm that solves the problem in finite time in a system where compasses are unstable for some arbitrary long periods, provided that they stabilize eventually. in addition, we show that our algorithm solves the gathering problem for at most three robots in the asynchronous model. our algorithm is intrinsically self-stabilizing.","autonomous mobile robots', 'cooperation and control', 'point formation', 'self-organizing robots', 'self-stabilization', 'unreliable compasses'",3,"oblivious mobile robot', 'gathering problem', 'agreement', 'asynchronous model', 'global coordinate system', 'robotic system', 'compass', 'set', 'limited visibility', 'self'",4,"gathering problem', 'mobile robot', 'arbitrary long period', 'global coordinate system', 'oblivious mobile robot', 'asynchronous model', 'limited visibility', 'robotic system', 'inherent difficulty', 'fundamental issue'",5,"problem', 'robot', 'model', 'compass', 'gathering', 'system', 'algorithm', 'agreement', 'semi', 'limited'",4,"oblivious mobile robot', 'global coordinate system', 'asynchronous model', 'gathering problem', 'arbitrary long period', 'robotic system', 'compass', 'limited visibility', 'fundamental issue', 'algorithm'",5,"oblivious mobile robots', 'mobile robots', 'robots', 'gathering problem', 'agreement', 'problem', 'global coordinate system', 'asynchronous model', 'semi-synchronous model', 'inherent difficulty'",5,"limited visibility', 'oblivious mobile robots', 'symmetry', 'mobile robots', 'asynchronous model', 'semi-synchronous model', 'model', 'robots', 'gathering problem', 'algorithm'",5,"provided compass problem', 'oblivious mobile robots', 'robotic systems problem', 'set mobile robots', 'oblivious memory robots', 'gathering problem robots', 'solvable asynchronous model', 'algorithm solves problem', 'robots self organize', 'model oblivious memory'",5
"this article explores a performance isolation-based approach to creating robust distributed applications. for each application, the approach is to understand the performance dependencies that pervade it and then impose constraints on the possible `spread' of such dependencies through the application. the mechanisms used for this purpose, termed isolation points, are software abstractions inserted at key program locations: (1) in application interfaces, (2) in middleware implementations for making remote requests, and (3) in the system interfaces used by middleware and applications. this article demonstrates the utility of isolation points by using them to implement higher level abstractions that improve the performance-robustness of representative enterprise applications. the i-queue abstraction uses isolation points to implement performance-robust messaging, targeting the message queues used in distributed enterprise codes. by appropriately orchestrating message dispatching, i-queue can achieve an improvement of 16--32\% in dispatched message locality based on traces obtained from the large-scale e-pricing{\reg} search engine operated by worldspan l.p.","autonomic computing', 'dynamic behavior', 'performance isolation'",3,"performance isolation', 'application interface', 'representative enterprise application', 'performance dependency', 'isolation point', 'article', 'approach', 'queue abstraction', 'software abstraction', 'high level abstraction'",4,"dispatched message locality', 'representative enterprise application', 'isolation point', 'high level abstraction', 'key program location', 'software abstraction', 'remote request', 'system interface', 'application interface', 'middleware implementation'",5,"application', 'isolation', 'performance', 'point', 'abstraction', 'queue', 'message', 'approach', 'article', 'robust'",2,"representative enterprise application', 'application interface', 'high level abstraction', 'performance isolation', 'queue abstraction', 'message queue', 'dispatched message locality', 'performance dependency', 'key program location', 'isolation point'",5,"performance isolation', 'representative enterprise applications', 'performance dependencies', 'isolation points', 'applications', 'performance', 'isolation', 'queue abstraction', 'higher level abstractions', 'article'",4,"constraints', 'performance isolation', 'middleware implementations', 'middleware', 'representative enterprise applications', 'search engine', 'performance dependencies', 'applications', 'performance', 'isolation points'",4,"orchestrating message dispatching', 'abstractions improve performance', 'improve performance robustness', 'message locality', 'understand performance dependencies', 'appropriately orchestrating message', 'explores performance isolation', 'improve performance', 'understand performance', 'performance dependencies pervade'",5
"the use of resources in multiagent learning systems is a relevant research problem, with a number of applications in resource allocation, communication and synchronization. multiagent distributed resource allocation requires that agents act on limited, localized information with minimum communication overhead in order to optimize the distribution of available resources. when requirements and constraints are dynamic, learning agents may be needed to allow for adaptation. one way of accomplishing learning is to observe past outcomes, using such information to improve future decisions. when limits in agents' memory or observation capabilities are assumed, one must decide on how large should the observation window be. we investigate how this decision influences both agents' and system's performance in the context of a special class of distributed resource allocation problems, namely dispersion games. we show by using several numerical experiments over a specific dispersion game (the minority game) that in such scenario an agent's performance is non-monotonically correlated with her memory size when all other agents are kept unchanged. we then provide an information-theoretic explanation for the observed behaviors, showing that a downward causation effect takes place.","dispersion games', 'mechanism design', 'multiagent learning', 'multiagent systems'",4,"resource allocation problem', 'available resource', 'multiagent learning system', 'use', 'relevant research problem', 'minimum communication', 'specific dispersion game', 'information', 'minority game', 'memory size'",4,"relevant research problem', 'specific dispersion game', 'downward causation effect', 'resource allocation problem', 'multiagent learning system', 'information', 'minimum communication', 'available resource', 'past outcome', 'future decision'",5,"resource', 'agent', 'allocation', 'game', 'information', 'multiagent', 'communication', 'dispersion', 'learning', 'problem'",4,"resource allocation problem', 'multiagent learning system', 'specific dispersion game', 'downward causation effect', 'relevant research problem', 'available resource', 'minority game', 'future decision', 'memory size', 'observation window'",5,"resource allocation problems', 'resource allocation', 'available resources', 'resources', 'multiagent learning systems', 'relevant research problem', 'other agents', 'allocation', 'minimum communication overhead', 'agents'",3,"constraints', 'resource allocation problems', 'memory size', 'resource allocation', 'multiagent learning systems', 'agent', 'available resources', 'memory', 'context', 'resource'",4,"multiagent learning systems', 'dynamic learning agents', 'resource allocation problems', 'communication overhead', 'multiagent learning', 'distributed resource allocation', 'resources multiagent learning', 'minimum communication overhead', 'minimum communication', 'games using numerical'",5
"this article presents a semantics-based context-aware dynamic service composition framework that composes an application through combining distributed components based on the semantics of components and contexts of users. the proposed framework consists of component service model with semantics (cosmos), component runtime environment (core), and semantic graph based service composition (segsec). cosmos models the semantics of components and contexts of users. core is a middleware to support cosmos on various distributed computing technologies. segsec is a mechanism to compose an application by synthesizing its workflow based on the semantics of components and contexts of users. the proposed framework is capable of composing applications requested in a natural language by leveraging the semantic information of components. the proposed framework composes applications differently to individual users based on their contexts and preferences. the proposed framework acquires user preferences from user-specified rules and also via learning. the proposed framework also adapts to dynamic environments by autonomously composing a new application upon detecting context change. this article describes the design and mechanism of the proposed framework, and also presents simulation experiments to evaluate the proposed framework.","context-aware', 'dynamic service composition', 'semantics', 'service oriented framework'",3,"semantic graph', 'semantic information', 'aware dynamic service composition framework', 'component service model', 'component runtime environment', 'context change', 'article', 'new application', 'user preference', 'individual user'",3,"aware dynamic service composition framework', 'semantic', 'context', 'component runtime environment', 'component service model', 'application', 'article', 'user', 'core', 'segsec'",3,"framework', 'semantic', 'component', 'context', 'user', 'application', 'service', 'composition', 'core', 'segsec'",4,"aware dynamic service composition framework', 'component service model', 'component runtime environment', 'user preference', 'new application', 'semantic graph', 'semantic information', 'individual user', 'context change', 'dynamic environment'",5,"component service model', 'component runtime environment', 'context change', 'component', 'semantics', 'contexts', 'user preferences', 'framework', 'individual users', 'service composition'",5,"simulation experiments', 'component service model', 'semantics', 'context change', 'context', 'middleware', 'service composition', 'component runtime environment', 'components', 'design'",5,"service composition framework', 'dynamic service composition', 'framework composes application', 'capable composing applications', 'framework adapts dynamic', 'framework composes applications', 'composition framework composes', 'composing new application', 'composing applications', 'component service model'",5
"a ``smart spaces system'', called mitos, for improved user connectivity in large wireless lan installations is proposed. mitos extends the scope of resource management to the dynamic relocation of nomadic users: the system suggests to a user the best location to move to for obtaining a satisfactory quality of service level, when the controlling access point of its current location becomes congested. the system monitors the traffic and user location across the network, and formulates the appropriate relocation proposal urging specific users to move to better locations at reasonable distances. two enhancements to the basic mitos system are introduced for maintaining an almost uniform load level across the considered infrastructure: the first uses microeconomic concepts, while the second borrows game theoretic mechanisms from the santa fe bar problem. simulation results on the efficiency of the proposed schemes are provided.","auctions', 'game theory', 'microeconomics', 'mobile computing', 'pervasive computing', 'sante fe bar problem', 'wireless communications'",2,"basic mito system', 'smart space system', 'user location', 'improved user connectivity', 'nomadic user', 'specific user', 'good location', 'current location', 'well location', 'uniform load level'",4,"large wireless lan installation', 'santa fe bar problem', 'improved user connectivity', 'smart space system', 'appropriate relocation proposal', 'basic mito system', 'uniform load level', 'game theoretic mechanism', 'current location', 'access point'",5,"user', 'system', 'location', 'mito', 'level', 'smart', 'space', 'improved', 'connectivity', 'large'",2,"user location', 'basic mito system', 'improved user connectivity', 'specific user', 'nomadic user', 'smart space system', 'large wireless lan installation', 'santa fe bar problem', 'current location', 'well location'",4,"smart spaces system', 'basic mitos system', 'user location', 'user connectivity', 'system', 'nomadic users', 'specific users', 'user', 'spaces', 'smart'",4,"simulation results', 'user connectivity', 'smart spaces system', 'user location', 'basic mitos system', 'user', 'system', 'nomadic users', 'specific users', 'dynamic relocation'",5,"location network formulates', 'dynamic relocation', 'formulates appropriate relocation', 'appropriate relocation proposal', 'relocation proposal', 'wireless lan installations', 'management dynamic relocation', 'maintaining uniform load', 'relocation nomadic users', 'lan installations proposed'",5
"software systems dealing with distributed applications in changing environments normally require human supervision to continue operation in all conditions. these (re-)configuring, troubleshooting, and in general maintenance tasks lead to costly and time-consuming procedures during the operating phase. these problems are primarily due to the open-loop structure often followed in software development. therefore, there is a high demand for management complexity reduction, management automation, robustness, and achieving all of the desired quality requirements within a reasonable cost and time range during operation. self-adaptive software is a response to these demands; it is a closed-loop system with a feedback loop aiming to adjust itself to changes during its operation. these changes may stem from the software system's {\em self\/} (internal causes, e.g., failure) or {\em context\/} (external events, e.g., increasing requests from users). such a system is required to {\em monitor\/} itself and its context, {\em detect\/} significant changes, {\em decide\/} how to react, and {\em act\/} to execute such decisions. these processes depend on adaptation properties (called self-* properties), domain characteristics (context information or models), and preferences of stakeholders. noting these requirements, it is widely believed that new models and frameworks are needed to design self-adaptive software. this survey article presents a taxonomy, based on concerns of adaptation, that is, {\em how}, {\em what}, {\em when\/} and {\em where}, towards providing a unified view of this emerging area. moreover, as adaptive systems are encountered in many disciplines, it is imperative to learn from the theories and models developed in these other areas. this survey article presents a landscape of research in self-adaptive software by highlighting relevant disciplines and some prominent research projects. this landscape helps to identify the underlying research gaps and elaborates on the corresponding challenges.","adaptation processes', 'research challenges', 'self-adaptive software', 'self-properties', 'survey'",2,"software system', 'adaptive software', 'software development', 'loop system', 'adaptive system', 'significant change', 'em self', 'em context', 'em monitor', 'em detect'",3,"software system', 'adaptive software', 'human supervision', 'complexity reduction', 'maintenance task', 'management complexity', 'operating phase', 'general maintenance', 'software development', 'high demand'",5,"em', 'software', 'system', 'adaptive', 'change', 'operation', 'self', 'loop', 'model', 'research'",2,"software system', 'loop system', 'em when', 'em self', 'em monitor', 'em detect', 'em context', 'em act', 'adaptive system', 'adaptive software'",3,"software systems', 'adaptive software', 'adaptive systems', 'loop system', 'software development', 'software', 'system', 'human supervision', 'general maintenance tasks', 'loop structure'",5,"management complexity reduction', 'context information', 'survey article', 'context', 'software systems', 'adaptive software', 'feedback loop', 'adaptive systems', 'loop system', 'applications'",5,"desired quality requirements', 'self adaptive software', 'dealing distributed applications', 'design self adaptive', 'identify underlying research', 'applications changing environments', 'adaptive systems', 'normally require human', 'execute decisions processes', 'followed software development'",4
"this article presents agilla, a mobile agent middleware designed to support self-adaptive applications in wireless sensor networks. agilla provides a programming model in which applications consist of evolving communities of agents that share a wireless sensor network. coordination among the agents and access to physical resources are supported by a tuple space abstraction. agents can dynamically enter and exit a network and can autonomously clone and migrate themselves in response to environmental changes. agilla's ability to support self-adaptive applications in wireless sensor networks has been demonstrated in the context of several applications, including fire detection and tracking, monitoring cargo containers, and robot navigation. agilla, the first mobile agent system to operate in resource-constrained wireless sensor platforms, was implemented on top of tinyos. agilla's feasibility and efficiency was demonstrated by experimental evaluation on two physical testbeds consisting of mica2 and telosb nodes.","agent', 'middleware', 'mobile agent', 'wireless sensor network'",2,"agilla', 'mobile agent middleware', 'wireless sensor network', 'mobile agent system', 'wireless sensor platform', 'adaptive application', 'article', 'self', 'physical resource', 'programming model'",3,"wireless sensor network', 'tuple space abstraction', 'wireless sensor platform', 'mobile agent middleware', 'adaptive application', 'agilla', 'mobile agent system', 'programming model', 'self', 'physical resource'",4,"agilla', 'sensor', 'agent', 'wireless', 'network', 'application', 'self', 'adaptive', 'mobile', 'resource'",3,"mobile agent middleware', 'mobile agent system', 'wireless sensor network', 'wireless sensor platform', 'physical resource', 'adaptive application', 'tuple space abstraction', 'physical testbed', 'agilla', 'fire detection'",5,"wireless sensor networks', 'mobile agent middleware', 'agilla', 'wireless sensor platforms', 'adaptive applications', 'agents', 'several applications', 'network', 'applications', 'wireless'",5,"experimental evaluation', 'mobile agent middleware', 'wireless sensor networks', 'coordination', 'wireless sensor platforms', 'adaptive applications', 'context', 'tracking', 'several applications', 'agilla'",5,"evaluation physical testbeds', 'mobile agent middleware', 'wireless sensor platforms', 'sensor networks demonstrated', 'mobile agent', 'self adaptive applications', 'mobile agent operate', 'agent middleware', 'wireless sensor networks', 'wireless sensor network'",4
"we describe how a set of mobile robots can arrange themselves on any specified curve on the plane in the presence of dynamic changes both in the underlying ad hoc network and in the set of participating robots. our strategy is for the mobile robots to implement a {\em self-stabilizing virtual layer\/} consisting of mobile client nodes, stationary virtual nodes (vns), and local broadcast communication. the vns are associated with predetermined regions in the plane and coordinate among themselves to distribute the client nodes relatively uniformly among the vns' regions. each vn directs its local client nodes to align themselves on the local portion of the target curve. the resulting motion coordination protocol is self-stabilizing, in that each robot can begin the execution in any arbitrary state and at any arbitrary location in the plane. in addition, self-stabilization ensures that the robots can adapt to changes in the desired target formation.","cooperative mobile robotics', 'distributed algorithms', 'formal methods', 'pattern formation', 'replicated state machines', 'self-stabilization'",2,"mobile robot', 'mobile client node', 'local client node', 'stationary virtual node', 'set', 'plane', 'target curve', 'local broadcast communication', 'em self', 'virtual layer'",3,"motion coordination protocol', 'local client node', 'local broadcast communication', 'mobile robot', 'stationary virtual node', 'mobile client node', 'set', 'dynamic change', 'plane', 'virtual layer\\/'",3,"robot', 'node', 'plane', 'mobile', 'self', 'client', 'local', 'set', 'vns', 'virtual'",2,"local client node', 'mobile client node', 'stationary virtual node', 'local broadcast communication', 'mobile robot', 'motion coordination protocol', 'local portion', 'target curve', 'em self', 'virtual layer'",5,"mobile robots', 'mobile client nodes', 'local client nodes', 'robot', 'stationary virtual nodes', 'client nodes', 'mobile', 'set', 'dynamic changes', 'target curve'",5,"motion coordination protocol', 'mobile robots', 'mobile client nodes', 'dynamic changes', 'target curve', 'local client nodes', 'robots', 'local broadcast communication', 'curve', 'stationary virtual nodes'",5,"local broadcast communication', 'self stabilizing robot', 'local broadcast', 'vns local broadcast', 'set mobile robots', 'stabilization ensures robots', 'motion coordination protocol', 'robots adapt changes', 'mobile robots implement', 'strategy mobile robots'",5
"the wireless network community has become increasingly aware of the benefits of data-driven link estimation and routing as compared with beacon-based approaches, but the issue of {\em biased link sampling\/} (bls) estimation has not been well studied even though it affects routing convergence in the presence of network and environment dynamics. focusing on traffic-induced dynamics, we examine the open, unexplored question of how serious the bls issue is and how to effectively address it when the routing metric etx is used. for a wide range of traffic patterns and network topologies and using both node-oriented and network-wide analysis and experimentation, we discover that the optimal routing structure remains quite stable even though the properties of individual links and routes vary significantly as traffic pattern changes. in cases where the optimal routing structure does change, data-driven link estimation and routing is either guaranteed to converge to the optimal structure or empirically shown to converge to a close-to-optimal structure. these findings provide the foundation for addressing the bls issue in the presence of traffic-induced dynamics and suggest approaches other than existing ones. these findings also demonstrate that it is possible to maintain an optimal, stable routing structure despite the fact that the properties of individual links and paths vary in response to network dynamics.","biased link sampling', 'convergence', 'data-driven link estimation and routing', 'stability', 'wireless sensor networks'",3,"wireless network community', 'network dynamic', 'network topology', 'link estimation', 'optimal routing structure', 'stable routing structure', 'em biased link sampling', 'individual link', 'optimal structure', 'bls issue'",3,"\\em biased link sampling\\/', 'optimal routing structure', 'link estimation', 'wireless network community', 'traffic pattern change', 'bls issue', 'data', 'individual link', 'network topology', 'environment dynamic'",4,"link', 'optimal', 'routing', 'structure', 'network', 'dynamic', 'traffic', 'estimation', 'bls', 'issue'",2,"optimal routing structure', 'stable routing structure', 'em biased link sampling', 'network dynamic', 'optimal structure', 'traffic pattern change', 'wireless network community', 'link estimation', 'network topology', 'bls issue'",5,"wireless network community', 'network dynamics', 'network topologies', 'network', 'optimal routing structure', 'stable routing structure', 'link estimation', 'traffic pattern changes', 'biased link sampling', 'individual links'",5,"biased link sampling', 'optimal routing structure', 'stable routing structure', 'wireless network community', 'routing', 'network dynamics', 'network topologies', 'traffic patterns', 'network', 'presence'",4,"effectively address routing', 'optimal stable routing', 'beacon based approaches', 'optimal routing structure', 'compared beacon based', 'maintain optimal stable', 'stable routing structure', 'quite stable properties', 'discover optimal routing', 'affects routing convergence'",4
"topology control protocol aims to efficiently adjust the network topology of wireless networks in a self-adaptive fashion to improve the performance and scalability of networks. this is especially essential to large-scale multihop wireless networks (e.g., wireless sensor networks). fault-tolerant topology control has been studied recently. in order to achieve both sparseness (i.e., the number of links is linear with the number of nodes) and fault tolerance (i.e., can survive certain level of node/link failures), different geometric topologies were proposed and used as the underlying network topologies for wireless networks. however, most of the existing topology control algorithms can only be applied to two-dimensional (2d) networks where all nodes are distributed in a 2d plane. in practice, wireless networks may be deployed in three-dimensional (3d) space, such as under water wireless sensor networks in ocean or mobile ad hoc networks among space shuttles in space. this article seeks to investigate self-organizing fault-tolerant topology control protocols for large-scale 3d wireless networks. our new protocols not only guarantee {\em k\/} -connectivity of the network, but also ensure the bounded node degree and constant power stretch factor even under {\em k\/} -1 node failures. all of our proposed protocols are localized algorithms, which only use one-hop neighbor information and constant messages with small time complexity. thus, it is easy to update the topology efficiently and self-adaptively for large-scale dynamic networks. our simulation confirms our theoretical proofs for all proposed 3d topologies.","fault tolerance', 'power efficiency', 'three-dimensional wireless networks', 'topology control'",2,"network topology', 'tolerant topology control protocol', 'topology control algorithm', 'different geometric topology', 'scale multihop wireless network', 'water wireless sensor network', 'scale dynamic network', 'new protocol', 'self', '-1 node failure'",5,"topology control protocol', 'wireless sensor network', 'tolerant topology control', 'wireless network', 'network topology', 'adaptive fashion', 'multihop wireless', 'scale multihop', 'self', 'geometric topology'",4,"network', 'topology', 'wireless', 'node', 'control', 'protocol', 'fault', 'large', 'scale', 'self'",2,"scale multihop wireless network', 'water wireless sensor network', 'network topology', 'tolerant topology control protocol', 'scale dynamic network', 'topology control algorithm', 'different geometric topology', '-1 node failure', 'constant power stretch factor', 'node degree'",5,"network topologies', 'topology control protocol', 'wireless sensor networks', 'wireless networks', 'tolerant topology control', 'topology control algorithms', 'dynamic networks', '3d topologies', 'network', 'different geometric topologies'",5,"topology control algorithms', 'simulation', 'topology control protocol', 'tolerant topology control', 'network topology', 'topology', 'wireless sensor networks', 'algorithms', 'small time complexity', 'wireless networks'",5,"adjust network topology', 'topology control algorithms', 'efficiently adjust network', 'topology control protocol', 'guarantee em connectivity', 'topology control protocols', 'proposed 3d topologies', 'improve performance scalability', 'distributed 2d plane', 'multihop wireless networks'",4
"a major challenge in wireless terrestrial networks is to provide large-scale reliable multicast and broadcast services. the main problem limiting the scalability of such networks is feedback implosion, a problem arising when a large number of users transmit their feedback messages through the network, occupying a significant portion of system resources.\par inspired by social psychology, specifically from the bystander effect phenomenon, an autonomic framework for large-scale reliable multicast services is presented. the self-configuring and self-optimizing procedures of the proposed autonomic scheme are modeled using game theory. through appropriate modeling and simulations of the proposed scheme carried out to evaluate its performance, it is found that the new approach suppresses feedback messages very effectively, while at the same time, it does not degrade the timely data transfer.","autonomic communication', 'autonomic manager', 'bystander effect', 'feedback suppression', 'game theory', 'nash equilibrium', 'reliable multicast', 'wimax networks'",2,"wireless terrestrial network', 'scale reliable multicast service', 'major challenge', 'large number', 'feedback message', 'main problem', 'feedback implosion', 'broadcast service', 'autonomic scheme', 'autonomic framework'",4,"scale reliable multicast', 'scale reliable multicast service', 'bystander effect phenomenon', 'wireless terrestrial network', 'new approach suppresse', 'social psychology', 'system resources.\\par', 'feedback message', 'major challenge', 'autonomic framework'",3,"scale', 'large', 'scale reliable', 'network', 'feedback', 'multicast', 'service', 'autonomic', 'message', 'reliable multicast'",2,"scale reliable multicast service', 'wireless terrestrial network', 'feedback message', 'autonomic scheme', 'feedback implosion', 'new approach suppresse', 'large number', 'timely datum transfer', 'bystander effect phenomenon', 'autonomic framework'",3,"major challenge', 'wireless terrestrial networks', 'challenge', 'reliable multicast services', 'wireless', 'major', 'such networks', 'feedback messages', 'network', 'large number'",4,"appropriate modeling', 'reliable multicast services', 'wireless terrestrial networks', 'major challenge', 'reliable multicast', 'broadcast services', 'scalability', 'feedback messages', 'challenge', 'feedback implosion'",5,"carried evaluate performance', 'networks provide large', 'proposed autonomic scheme', 'reliable multicast broadcast', 'evaluate performance new', 'evaluate performance', 'terrestrial networks provide', 'multicast broadcast', 'autonomic framework', 'suppresses feedback messages'",5
"network applications are increasingly required to be autonomous, scalable, adaptive to dynamic changes in the network, and survivable against partial system failures. based on the observation that various biological systems have already satisfied these requirements, this article proposes and evaluates a biologically-inspired framework that makes network applications to be autonomous, scalable, adaptive, and survivable. with the proposed framework, called inet, each network application is designed as a decentralized group of software agents, analogous to a bee colony (application) consisting of multiple bees (agents). each agent provides a particular functionality of a network application, and implements biological behaviors such as reproduction, migration, energy exchange, and death. inet is designed after the mechanisms behind how the immune system detects antigens (e.g., viruses) and produces specific antibodies to eliminate them. it models a set of environment conditions (e.g., network traffic and resource availability) as an antigen and an agent behavior (e.g., migration) as an antibody. inet allows each agent to autonomously sense its surrounding environment conditions (an antigen) to evaluate whether it adapts well to the sensed environment, and if it does not, adaptively perform a behavior (an antibody) suitable for the environment conditions. in inet, a configuration of antibodies is encoded as a set of genes, and antibodies evolve via genetic operations such as crossover and mutation. empirical measurement results show that inet is lightweight enough. simulation results show that agents adapt to dynamic and heterogeneous network environments by evolving their antibodies across generations. the results also show that inet allows agents to scale to workload volume and network size and to survive partial link failures in the network.","artificial immune systems', 'autonomic networking', 'biologically-inspired networking', 'evolvable network applications'",2,"network application', 'heterogeneous network environment', 'network traffic', 'network size', 'agent behavior', 'software agent', 'partial system failure', 'biological system', 'immune system detect antigen', 'autonomous'",3,"network application', 'system detect antigen', 'partial system failure', 'immune system detect', 'environment condition', 'dynamic change', 'biological system', 'inet', 'resource availability', 'agent behavior'",5,"network', 'agent', 'inet', 'application', 'environment', 'antibody', 'system', 'condition', 'antigen', 'result'",4,"heterogeneous network environment', 'immune system detect antigen', 'network application', 'partial system failure', 'agent behavior', 'network traffic', 'network size', 'software agent', 'partial link failure', 'biological system'",5,"network application', 'heterogeneous network environments', 'network size', 'network traffic', 'network', 'applications', 'partial system failures', 'agent behavior', 'various biological systems', 'software agents'",4,"simulation results', 'agent behavior', 'energy exchange', 'network applications', 'network size', 'agent', 'heterogeneous network environments', 'behavior', 'network traffic', 'network'",3,"evolve genetic operations', 'adapt dynamic heterogeneous', 'adaptively perform behavior', 'implements biological behaviors', 'environments evolving antibodies', 'agent behavior', 'adapts sensed environment', 'biological behaviors', 'adaptive dynamic changes', 'agents adapt dynamic'",4
"in this article we study the data dissemination problem in which data items are flooded to all the moving objects in a mobile ad hoc network by peer-to-peer transfer. we show that if memory and bandwidth are bounded at moving objects, then the problem of determining whether a set of data items can be disseminated to all the moving objects is np-complete. for a heuristic solution we postulate that a moving object should save and transmit the data items that are most likely to be new (i.e., previously unknown) to future encountered moving objects. we propose a method to be used by each moving object to prioritize data items based on their probabilities of being new to future receivers. the method employs a machine learning system for estimation of the novelty probability and the machine learning system is progressively trained by received data items. through simulations based on real mobility traces, we show the superiority of the method against some natural alternatives.","mobile ad hoc networks', 'mobile data management', 'mobile peer-to-peer networks', 'publish/subscribe', 'resource discovery'",3,"data item', 'data dissemination problem', 'object', 'article', 'peer transfer', 'machine learning system', 'method', 'mobile ad', 'future receiver', 'novelty probability'",3,"machine learning system', 'data dissemination problem', 'datum item', 'real mobility trace', 'object', 'mobile ad', 'peer transfer', 'heuristic solution', 'future receiver', 'method'",4,"object', 'item', 'data', 'method', 'peer', 'datum', 'problem', 'future', 'new', 'system'",2,"data item', 'data dissemination problem', 'machine learning system', 'real mobility trace', 'object', 'novelty probability', 'peer transfer', 'future receiver', 'mobile ad', 'natural alternative'",5,"data items', 'data dissemination problem', 'data', 'object', 'items', 'machine learning system', 'peer transfer', 'problem', 'article', 'peer'",4,"data items', 'data dissemination problem', 'data', 'memory', 'machine learning system', 'objects', 'items', 'novelty probability', 'bandwidth', 'peer transfer'",5,"moving objects np', 'moving objects problem', 'ad hoc network', 'used moving object', 'based real mobility', 'disseminated moving objects', 'bounded moving objects', 'moving objects', 'data dissemination problem', 'peer peer transfer'",5
"anomaly-based intrusion detection is about the discrimination of malicious and legitimate behaviors on the basis of the characterization of system normality in terms of particular observable subjects. as the system normality is constructed solely from an observed sample of normally occurring patterns, anomaly detectors always suffer excessive false alerts. adaptability is therefore a desirable feature that enables an anomaly detector to alleviate, if not eliminate, such annoyance. to achieve that, we either design self-learning anomaly detectors to capture the drifts of system normality or develop postprocessing mechanisms to deal with the outputs. as the former methodology is usually scenario- and application-specific, in this article, we focus on the latter one. in particular, our design starts from three key observations: (1) most of anomaly detectors are threshold based and parametric, that is, configurable by a set of parameters; (2) anomaly detectors differ in operational environment and operational capability in terms of detection coverage and blind spots; (3) an intrusive anomaly may leave traces across multiple system layers, incurring different observable events of interest. firstly, we present a statistical framework to formally characterize and analyze the basic behaviors of anomaly detectors by examining the properties of their operational environments. the framework then serves as a theoretical basis for developing an adaptive middleware, which is called m-aid, to optimally integrate a number of observation-specific parameterizable anomaly detectors. specifically, m-aid treats these fine-grained anomaly detectors as a whole and casts their collective behaviors in a framework which is formulated as a multiagent partially observable markov decision process (mpo-mdp). the generic anomaly detection models of m-aid are thus automatically inferred via a reinforcement learning algorithm which dynamically adjusts the behaviors of anomaly detectors in accordance with a reward signal that is defined and quantified by a suit of evaluation metrics. fundamentally, the distributed and autonomous architecture enables m-aid to be scalable, dependable, and adaptable, and the reward signal allows security administrators to specify cost factors and take into account the operational context for taking rational response. finally, a host-based prototype of m-aid is developed, along with comprehensive experimental evaluation and comparative studies.","anomaly detection', 'intrusion detection', 'network security', 'pomdp', 'security metrics', 'security policy'",4,"specific parameterizable anomaly detector', 'generic anomaly detection model', 'intrusive anomaly', 'intrusion detection', 'detection coverage', 'system normality', 'legitimate behavior', 'multiple system layer', 'basic behavior', 'collective behavior'",5,"anomaly detector', 'system normality', 'reward signal', 'operational environment', 'detection', 'behavior', 'observable', 'basis', 'term', 'particular'",4,"anomaly', 'detector', 'aid', 'system', 'behavior', 'operational', 'normality', 'observable', 'framework', 'detection'",3,"specific parameterizable anomaly detector', 'generic anomaly detection model', 'observable markov decision process', 'intrusive anomaly', 'particular observable subject', 'different observable event', 'multiple system layer', 'operational environment', 'comprehensive experimental evaluation', 'system normality'",5,"intrusion detection', 'grained anomaly detectors', 'anomaly detectors', 'detection coverage', 'detection', 'intrusive anomaly', 'anomaly', 'system normality', 'legitimate behaviors', 'detectors'",4,"evaluation metrics', 'comprehensive experimental evaluation', 'detection coverage', 'autonomous architecture', 'security administrators', 'adaptive middleware', 'operational context', 'design self', 'design', 'intrusion detection'",4,"generic anomaly detection', 'parameterizable anomaly detectors', 'behaviors basis characterization', 'behaviors framework formulated', 'dynamically adjusts behaviors', 'behaviors framework', 'anomaly detection models', 'behaviors anomaly detectors', 'autonomous architecture enables', 'present statistical framework'",4
"evolutionary algorithms (eas) and swarm intelligence (si) are widely used to tackle black-box global optimization problems when no prior knowledge is available. in order to increase search diversity and avoid stagnation in local optima, the mutation operator was introduced and has been extensively studied in eas and si-based algorithms. however, the performance after introducing mutation can be affected in many aspects and the parameters used to perform mutations are very hard to determine. for the purpose of developing efficient mutation operators, this article proposes a unified tabu and mutation framework with parameter adaptations in the context of the particle swarm optimizer (pso). the proposed framework is a significant extension of our preliminary work wang et al. 2007. empirical studies on 25 benchmark functions indicate that under the proposed framework: (1) excellent performance can be achieved even with a small number of mutations; (2) the derived algorithm consistently performs well on diverse types of problems and overall performance even surpasses the state-of-the-art pso variants and representative mutation-based eas; and (3) fast convergence rates can be preserved despite the use of a long jump mutation operator (the cauchy mutation).","evolutionary algorithm', 'global optimization', 'mutation operator', 'parameter adaptation', 'swarm intelligence'",3,"evolutionary algorithm', 'efficient mutation operator', 'long jump mutation operator', 'mutation framework', 'representative mutation', 'cauchy mutation', 'eas', 'swarm intelligence', 'particle swarm optimizer', 'box global optimization problem'",4,"global optimization problem', 'mutation operator', 'box global optimization', 'swarm optimizer', 'parameter adaptation', 'mutation framework', 'eas', 'unified tabu', 'et al', 'particle swarm'",4,"mutation', 'eas', 'algorithm', 'operator', 'framework', 'performance', 'si', 'swarm', 'pso', 'parameter'",3,"long jump mutation operator', 'efficient mutation operator', 'mutation framework', 'representative mutation', 'cauchy mutation', 'box global optimization problem', 'wang et al', 'particle swarm optimizer', 'art pso variant', 'overall performance'",5,"evolutionary algorithms', 'efficient mutation operators', 'mutation operator', 'mutation framework', 'algorithms', 'cauchy mutation', 'representative mutation', 'particle swarm optimizer', 'mutations', 'swarm intelligence'",5,"global optimization problems', 'evolutionary algorithms', 'algorithms', 'search diversity', 'context', 'efficient mutation operators', 'overall performance', 'excellent performance', 'mutation operator', 'mutation framework'",5,"local optima mutation', 'efficient mutation operators', 'mutation operators', 'particle swarm optimizer', 'swarm optimizer', 'global optimization problems', 'unified tabu mutation', 'local optima', 'evolutionary algorithms eas', 'tabu mutation framework'",5
"one of the major challenges in engineering distributed multiagent systems is the coordination necessary to align the behavior of different agents. decentralization of control implies a style of coordination in which the agents cooperate as peers with respect to each other and no agent has global control over the system, or global knowledge about the system. the dynamic interactions and collaborations among agents are usually structured and managed by means of roles and organizations. in existing approaches agents typically have a dual responsibility: on the one hand playing roles within the organization, on the other hand managing the life-cycle of the organization itself, for example, setting up the organization and managing organization dynamics. engineering realistic multiagent systems in which agents encapsulate this dual responsibility is a complex task.\par in this article, we present a middleware for context-driven dynamic agent organizations. the middleware is part of an integrated approach, called macodo: middleware architecture for context-driven dynamic agent organizations. the complementary part of the macodo approach is an organization model that defines abstractions to support application developers in describing dynamic organizations, as described in weyns et al. 2010.\par the macodo middleware offers the life-cycle management of dynamic organizations as a reusable service separated from the agents, which makes it easier to understand, design, and manage dynamic organizations in multiagent systems. we give a detailed description of the software architecture of the madoco middleware. the software architecture describes the essential building blocks of a distributed middleware platform that supports the macodo organization model. we used the middleware architecture to develop a prototype middleware platform for a traffic monitoring application. we evaluate the macodo middleware architecture by assessing the adaptability, scalability, and robustness of the prototype platform.","context', 'middleware', 'organization', 'role', 'software architecture'",2,"dynamic agent organization', 'organization dynamic', 'macodo organization model', 'approach agent', 'different agent', 'realistic multiagent system', 'macodo middleware architecture', 'prototype middleware platform', 'dynamic interaction', 'madoco middleware'",5,"dynamic agent organization', 'multiagent system', 'dual responsibility', 'coordination necessary', 'middleware architecture', 'different agent', 'organization model', 'dynamic organization', 'playing role', 'dynamic interaction'",4,"organization', 'middleware', 'agent', 'dynamic', 'system', 'macodo', 'architecture', 'multiagent', 'approach', 'platform'",3,"dynamic agent organization', 'macodo middleware architecture', 'macodo organization model', 'prototype middleware platform', 'approach agent', 'organization dynamic', 'different agent', 'madoco middleware', 'realistic multiagent system', 'macodo approach'",5,"dynamic agent organizations', 'major challenges', 'realistic multiagent systems', 'organization dynamics', 'dynamic organizations', 'multiagent systems', 'macodo organization model', 'approaches agents', 'different agents', 'challenges'",5,"macodo middleware architecture', 'middleware architecture', 'dynamic agent organizations', 'prototype middleware platform', 'coordination', 'software architecture', 'macodo middleware', 'middleware platform', 'madoco middleware', 'middleware'",4,"distributed multiagent systems', 'multiagent systems', 'evaluate macodo middleware', 'macodo middleware architecture', 'distributed middleware platform', 'architecture madoco middleware', 'organizations multiagent systems', 'macodo middleware offers', 'multiagent systems agents', 'multiagent systems coordination'",4
"we address the problem of cooperation in decentralized systems, specifically looking at interactions between independent pairs of peers where mutual exchange of resources (e.g., updating or sharing content) is required. in the absence of any enforcement mechanism or protocol, there is no incentive for one party to directly reciprocate during a transaction with another. consequently, for such decentralized systems to function, protocols for self-organization need to explicitly promote cooperation in a manner where adherence to the protocol is incentivized.\par in this article we introduce a new generic model to achieve this. the model is based on peers repeatedly interacting to build up and maintain a dynamic social network of others that they can trust based on similarity of cooperation. this mechanism effectively incentivizes unselfish behavior, where peers with higher levels of cooperation gain higher payoff. we examine the model's behavior and robustness in detail. this includes the effect of peers self-adapting their cooperation level in response to maximizing their payoff, representing a nash-equilibrium of the system. the study shows that the formation of a social network based on reflexive cooperation levels can be a highly effective and robust incentive mechanism for autonomous decentralized systems.","cooperation', 'decentralized systems', 'self-organization'",2,"reflexive cooperation level', 'decentralized system', 'peer self', 'robust incentive mechanism', 'enforcement mechanism', 'new generic model', 'high level', 'dynamic social network', 'protocol', 'problem'",4,"decentralized system', 'dynamic social network', 'robust incentive mechanism', 'reflexive cooperation level', 'mutual exchange', 'new generic model', 'sharing content', 'independent pair', 'enforcement mechanism', 'peer'",5,"cooperation', 'system', 'peer', 'protocol', 'model', 'mechanism', 'level', 'decentralized', 'social', 'payoff'",3,"reflexive cooperation level', 'robust incentive mechanism', 'new generic model', 'peer self', 'dynamic social network', 'decentralized system', 'enforcement mechanism', 'high level', 'high payoff', 'unselfish behavior'",4,"reflexive cooperation levels', 'cooperation level', 'cooperation gain', 'cooperation', 'such decentralized systems', 'peers self', 'robust incentive mechanism', 'higher levels', 'systems', 'enforcement mechanism'",4,"reflexive cooperation levels', 'unselfish behavior', 'cooperation level', 'behavior', 'equilibrium', 'cooperation gain', 'cooperation', 'dynamic social network', 'new generic model', 'model'",5,"mechanism effectively incentivizes', 'self adapting cooperation', 'protocols self organization', 'function protocols self', 'effect peers self', 'payoff representing nash', 'network trust', 'social network trust', 'based reflexive cooperation', 'network trust based'",5
"organic computing (oc) and other research initiatives like autonomic computing or proactive computing have developed the vision of systems possessing life-like properties: they self-organize, adapt to their dynamically changing environments, and establish other so-called self-x properties, like self-healing, self-configuration, self-optimization, etc. what we are searching for in oc are methodologies and concepts for systems that allow to cope with increasingly complex networked application systems by introduction of self-x properties and at the same time guarantee a trustworthy and adaptive response to externally provided system objectives and control actions. therefore, in oc, we talk about {\em controlled self-organization}.\par although the terms {\em self-organization\/} and {\em adaptivity\/} have been discussed for years, we miss a clear definition of self-organization in most publications, which have a technically motivated background.\par in this article, we briefly summarize the state of the art and suggest a characterization of (controlled) self-organization and adaptivity that is motivated by the main objectives of the oc initiative. we present a system classification of robust, adaptable, and adaptive systems and define a degree of autonomy to be able to quantify how autonomously a system is working. the degree of autonomy distinguishes and measures external control that is exerted directly by the user ({\em no autonomy\/}) from internal control of a system which might be fully controlled by an observer/controller architecture that is part of the system ({\em full autonomy\/}). the quantitative degree of autonomy provides the basis for characterizing the notion of controlled self-organization. furthermore, we discuss several alternatives for the design of organic systems.","adaptation', 'adaptivity', 'observer/controller architecture', 'organic computing', 'robustness', 'self-organization', 'system classification'",4,"em self', 'organic system', 'organic computing', 'adaptive system', 'system objective', 'complex networked application system', 'system classification', 'oc initiative', 'autonomic computing', 'proactive computing'",4,"measure external control', 'networked application system', 'complex networked application', 'self', 'x property', 'oc', 'like property', 'proactive computing', 'autonomic computing', 'organic computing'",4,"self', 'system', 'control', 'em', 'oc', 'property', 'like', 'computing', 'degree', 'organization'",2,"complex networked application system', 'system objective', 'adaptive system', 'organic system', 'system classification', 'measure external control', 'em self', 'internal control', 'control action', 'like property'",4,"organic systems', 'organic computing', 'proactive computing', 'autonomic computing', 'system objectives', 'adaptive systems', 'system classification', 'oc initiative', 'self', 'systems'",5,"optimization', 'system classification', 'controller architecture', 'control actions', 'external control', 'internal control', 'design', 'organic systems', 'organic computing', 'proactive computing'",5,"adaptable adaptive systems', 'self organize adapt', 'life like properties', 'self organization adaptivity', 'definition self organization', 'autonomy internal control', 'systems allow cope', 'initiatives like autonomic', 'adaptive systems', 'self organization em'",4
"in recent years, tremendous progress has been made in understanding the dynamics of vehicle traffic flow and traffic congestion by interpreting traffic as a multiparticle system. this helps to explain the onset and persistence of many undesired phenomena, for example, traffic jams. it also reflects the apparent helplessness of drivers in traffic, who feel like passive particles that are pushed around by exterior forces; one of the crucial aspects is the inability to communicate and coordinate with other traffic participants.\par we present distributed methods for solving these fundamental problems, employing modern wireless, ad-hoc, multi-hop networks. the underlying idea is to use these capabilities as the basis for self-organizing methods for coordinating data collection and processing, recognizing traffic phenomena, and changing their structure by coordinated behavior. the overall objective is a multi-level approach that reaches from protocols for local wireless communication, data dissemination, pattern recognition, over hierarchical structuring and coordinated behavior, all the way to large-scale traffic regulation.\par in this article, we describe three types of results: (i) self-organizing and distributed methods for maintaining and collecting data (using our concept of {\em hovering data clouds\/}); (ii) adaptive data dissemination for traffic information systems; (iii) methods for self-recognition of traffic jams. we conclude by describing higher-level aspects of our work.","hovering data clouds', 'organic computing', 'organic information complexes', 'pattern recognition', 'self-organizing systems', 'traffic', 'traffic jams'",3,"traffic information system', 'traffic phenomenon', 'traffic jam', 'vehicle traffic flow', 'traffic participants.par', 'traffic congestion', 'scale traffic regulation.par', 'recent year', 'method', 'multiparticle system'",4,"traffic jam', 'coordinated behavior', 'pattern recognition', 'data dissemination', 'apparent helplessness', 'wireless communication', 'overall objective', 'level approach', 'crucial aspect', 'modern wireless'",3,"traffic', 'method', 'datum', 'self', 'jam', 'system', 'behavior', 'coordinated', 'phenomenon', 'multi'",3,"traffic information system', 'scale traffic regulation.par', 'vehicle traffic flow', 'traffic phenomenon', 'traffic congestion', 'traffic participants.par', 'traffic jam', 'adaptive datum dissemination', 'datum collection', 'local wireless communication'",4,"recent years', 'tremendous progress', 'vehicle traffic flow', 'years', 'traffic information systems', 'traffic phenomena', 'traffic jams', 'other traffic participants', 'scale traffic regulation', 'traffic congestion'",4,"pattern recognition', 'recognition', 'recent years', 'tremendous progress', 'coordinated behavior', 'vehicle traffic flow', 'years', 'traffic information systems', 'traffic phenomena', 'traffic jams'",5,"maintaining collecting data', 'structuring coordinated behavior', 'local wireless communication', 'understanding dynamics vehicle', 'processing recognizing traffic', 'understanding dynamics', 'solving fundamental problems', 'recognition traffic jams', 'interpreting traffic', 'adaptive data dissemination'",5
"most construction of artificial, multicomponent structures is based upon an external entity that directs the assembly process, usually following a script/blueprint under centralized control. in contrast, recent research has focused increasingly on an alternative paradigm, inspired largely by the nest building behavior of social insects, in which components ``self-assemble'' into a given target structure. adapting such a nature-inspired approach to precisely self-assemble artificial structures (bridge, building, etc.) presents a formidable challenge: one must create a set of local control rules to direct the behavior of the individual components/agents during the self-assembly process. in recent work, we developed a fully automated procedure that generates such rules, allowing a given structure to successfully self-assemble in a simulated environment having constrained, continuous motion; however, the resulting rule sets were typically quite large. in this article, we present a more effective methodology for automatic rule generation, which makes an attempt to parsimoniously capture both the repeating patterns that exist within a structure, and the behaviors necessary for appropriate coordination. we then empirically show that the procedure developed here generates sets of rules that are not only correct, but significantly reduced in size, relative to our earlier approach. such rule sets allow for simpler agents that are nonetheless still capable of performing complex tasks, and therefore demonstrate the problem-solving potential of self-organized systems.","coordination', 'parsimony', 'self-assembly', 'self-organization', 'stigmergy', 'swarm intelligence'",2,"artificial structure', 'multicomponent structure', 'target structure', 'rule set', 'self', 'local control rule', 'automatic rule generation', 'nest building behavior', 'construction', 'behavior necessary'",3,"automatic rule generation', 'local control rule', 'assembly process', 'nest building behavior', 'rule set', 'alternative paradigm', 'recent research', 'centralized control', 'social insect', 'self'",4,"rule', 'self', 'structure', 'set', 'assemble', 'behavior', 'process', 'artificial', 'control', 'component'",2,"local control rule', 'rule set', 'automatic rule generation', 'artificial structure', 'target structure', 'multicomponent structure', 'nest building behavior', 'recent research', 'centralized control', 'simple agent'",5,"most construction', 'artificial structures', 'multicomponent structures', 'construction', 'target structure', 'such rule sets', 'structure', 'artificial', 'most', 'local control rules'",5,"appropriate coordination', 'local control rules', 'nest building behavior', 'centralized control', 'behavior', 'most construction', 'artificial structures', 'such rule sets', 'rule sets', 'multicomponent structures'",5,"construction artificial multicomponent', 'behavior social insects', 'structure adapting nature', 'behavior individual components', 'demonstrate problem solving', 'automatic rule generation', 'agents self assembly', 'artificial multicomponent structures', 'attempt parsimoniously capture', 'multicomponent structures'",5
this article presents an introduction to the special issue on augmentative and alternative communication (aac).,"alternative and augmentative communication', 'human-computer interaction'",4,"article', 'special issue', 'introduction', 'alternative communication', 'augmentative', 'aac'",5,"alternative communication', 'special issue', 'introduction', 'augmentative', 'aac', 'article'",5,"aac', 'article', 'alternative', 'communication', 'introduction', 'special', 'issue', 'augmentative', 'alternative communication', 'special issue'",5,"alternative communication', 'special issue', 'article', 'aac', 'introduction', 'augmentative'",4,"special issue', 'introduction', 'special', 'alternative communication', 'article', 'issue', 'augmentative', 'alternative', 'communication', 'aac'",5,"special issue', 'introduction', 'special', 'alternative communication', 'article', 'issue', 'augmentative', 'alternative', 'communication', 'aac'",5,"augmentative alternative communication', 'special issue augmentative', 'alternative communication aac', 'alternative communication', 'article presents', 'special issue', 'augmentative alternative', 'issue augmentative alternative', 'article presents introduction', 'communication aac'",4
"for some people with motor disabilities and speech disorders, the only way to communicate and to have some control over their environment is through the use of a controlled scanning system operated by a single switch. the main problem with these systems is that the communication process tends to be exceedingly slow, since the system must scan through the available choices one at a time until the desired message is reached. one way of raising the speed of message selection is to optimize the elementary scanning delay in real time so that it allows the user to make selections as quickly as possible without making too many errors. with this objective in mind, this article presents a method for optimizing the scanning delay, which is based on an analysis of the data recorded in ``log files'' while applying the edith system digital teleaction environment for people with disabilities. this analysis makes it possible to develop a human-machine interaction model specific to the study, and then to establish an adaptive algorithm for the calculation of the scanning delay. the results obtained with imposed scenarios and then in ecological situations provides a confirmation that our algorithms are effective in dynamically adapting a scan speed. the main advantage offered by the procedure proposed is that it works on timing information alone and thus does not require any knowledge of the scanning device itself. this allows it to work with any scanning device.","adaptive scanning rate', 'alternative communication', 'model human processor', 'modeling', 'scanning system'",3,"people', 'motor disability', 'edith system', 'scanning delay', 'scan speed', 'digital teleaction environment', 'scanning device', 'main problem', 'way', 'main advantage'",2,"machine interaction model specific', 'scanning delay', 'digital teleaction environment', 'edith system', 'real time', 'log file', 'available choice', 'message selection', 'communication process', 'speech disorder'",4,"scan', 'system', 'scanning', 'delay', 'time', 'control', 'disability', 'people', 'way', 'message'",3,"scan speed', 'machine interaction model specific', 'edith system', 'digital teleaction environment', 'real time', 'message selection', 'scanning delay', 'scanning device', 'adaptive algorithm', 'motor disability'",5,"motor disabilities', 'scanning system', 'people', 'elementary scanning delay', 'disabilities', 'digital teleaction environment', 'speech disorders', 'scanning delay', 'only way', 'scanning device'",5,"algorithms', 'elementary scanning delay', 'scanning delay', 'machine interaction model', 'scan speed', 'control', 'motor disabilities', 'scanning system', 'speed', 'people'",4,"digital teleaction environment', 'machine interaction model', 'establish adaptive algorithm', 'presents method optimizing', 'optimizing scanning delay', 'interaction model', 'scanning device allows', 'adaptive algorithm', 'problem systems communication', 'teleaction environment'",4
"the purpose of this study was to determine whether the presence or absence of digitized 1--2-word voice output on a direct selection, customized augmentative and alternative communication (aac) device would affect the impoverished conversations of persons with dementia. thirty adults with moderate alzheimer's disease participated in two personally relevant conversations with an aac device. for twelve of the participants the aac device included voice output. the aac device was the flexiboard$^{tm}$ containing sixteen messages needed to discuss a favorite autobiographical topic chosen by the participant and his/her family caregivers. ten-minute conversations were videotaped in participants' residences and analyzed for four conversational measures related to the participants' communicative behavior. results show that aac devices with digitized voice output depress conversational performance and distract participants with moderate alzheimer's disease as compared to similar devices without voice output. there were significantly more 1-word utterances and fewer total utterances when aac devices included voice output, and the rate of topic elaborations/initiations was significantly lower when voice output was present. discussion about the novelty of voice output for this population of elders and the need to train elders to use this technology is provided.","alzheimer' augmentative and alternative communication (aac), 'dementia', 'digitized speech synthesis', 'language', 's disease'",4,"word voice output', 'voice output depress conversational performance', 'aac device', 'similar device', 'purpose', 'distract participant', 'word utterance', 'relevant conversation', 'impoverished conversation', 'minute conversation'",3,"aac device', 'voice output', 'voice output depress conversational performance', 'moderate alzheimer', 'word voice output', 'favorite autobiographical topic', 'few total utterance', 'direct selection', 'alternative communication', 'relevant conversation'",5,"device', 'voice', 'aac', 'output', 'participant', 'conversation', 'alzheimer', 'moderate', 'disease', 'word'",3,"voice output depress conversational performance', 'word voice output', 'aac device', 'similar device', 'favorite autobiographical topic', 'few total utterance', 'relevant conversation', 'minute conversation', 'distract participant', 'impoverished conversation'",5,"word voice output', 'voice output', 'study', 'aac device', 'presence', 'purpose', 'word utterances', 'similar devices', 'voice', 'output'",4,"word voice output', 'presence', 'communicative behavior', 'voice output', 'study', 'aac device', 'conversational performance', 'purpose', 'word utterances', 'similar devices'",4,"elders use technology', 'conversational performance distract', 'included voice output', 'devices voice output', 'augmentative alternative communication', 'voice output present', 'affect impoverished conversations', 'voice output', 'videotaped participants residences', 'depress conversational performance'",4
"the interactive standup software was developed to provide children who use augmentative and alternative communication (aac) with a ``language playground.'' the software provides appropriate functionality for users with physical, speech, and language impairments to generate and tell novel punning riddles at different levels of complexity. standup was evaluated with nine children with cerebral palsy during an eight-week study. results show that the participants were able to generate and tell novel jokes with minimal or no support. the use of standup impacted favorably on general aac use. the study results also suggested that standup could potentially have a positive effect on social and pragmatic skills. further research to investigate the impact of standup on communication skills is proposed. suggestions for future software development include providing users with opportunities to complete jokes and to integrate online dictionaries when new vocabulary is encountered.","alternative and augmentative communication', 'computational humor', 'speech generation devices'",3,"interactive standup software', 'future software development', 'general aac use', 'child', 'alternative communication', 'language impairment', 'language playground', 'communication skill', 'novel joke', 'appropriate functionality'",4,"future software development', 'interactive standup software', 'general aac use', 'child', 'week study', 'different level', 'language impairment', 'appropriate functionality', 'cerebral palsy', 'novel joke'",5,"standup', 'use', 'software', 'language', 'aac', 'novel', 'study', 'child', 'result', 'communication'",4,"interactive standup software', 'general aac use', 'future software development', 'study result', 'communication skill', 'novel joke', 'week study', 'language impairment', 'alternative communication', 'language playground'",5,"interactive standup software', 'standup', 'future software development', 'general aac use', 'alternative communication', 'software', 'communication skills', 'study results', 'language playground', 'children'",5,"interactive standup software', 'complexity', 'online dictionaries', 'standup', 'future software development', 'general aac use', 'alternative communication', 'software', 'communication skills', 'study results'",5,"children use augmentative', 'playground software provides', 'support use standup', 'results suggested standup', 'study results suggested', 'augmentative alternative communication', 'standup communication skills', 'users physical speech', 'aac language playground', 'minimal support use'",4
this article provides an introduction to the special issue on aging.,"aging', 'cognitive aging', 'instruction', 'menu design', 'older adults', 'pen interfaces', 'quality of life technology', 'spoken dialog systems', 'user privacy preferences', 'video modeling', 'voice interfaces'",4,"article', 'special issue', 'introduction'",2,"special issue', 'introduction', 'article'",2,"article', 'introduction', 'special', 'issue', 'special issue'",3,"special issue', 'article', 'introduction'",3,"special issue', 'introduction', 'special', 'issue', 'article', 'aging'",5,"special issue', 'introduction', 'special', 'aging', 'issue', 'article'",5,"special issue aging', 'special issue', 'article provides introduction', 'article provides', 'introduction special issue', 'provides introduction special', 'issue aging', 'aging', 'introduction special', 'article'",4
"most studies on adapting voice interfaces to older users work top-down by comparing the interaction behavior of older and younger users. in contrast, we present a bottom-up approach. a statistical cluster analysis of 447 appointment scheduling dialogs between 50 older and younger users and 9 simulated spoken dialog systems revealed two main user groups, a ``social'' group and a ``factual'' group. ``factual'' users adapted quickly to the systems and interacted efficiently with them. ``social'' users, on the other hand, were more likely to treat the system like a human, and did not adapt their interaction style. while almost all ``social'' users were older, over a third of all older users belonged in the ``factual'' group. cognitive abilities and gender did not predict group membership. we conclude that spoken dialog systems should adapt to users based on observed behavior, not on age.","aging', 'clustering', 'cognitive aging', 'spoken dialog systems', 'voice interfaces'",2,"old user', 'main user group', 'young user', 'group membership', 'spoken dialog system', 'appointment scheduling dialog', 'study', 'voice interface', 'interaction behavior', 'statistical cluster analysis'",3,"main user group', 'spoken dialog system', 'old user', 'young user', 'appointment scheduling dialog', 'statistical cluster analysis', 'interaction behavior', 'voice interface', 'social', 'factual'",4,"user', 'group', 'old', 'factual', 'social', 'system', 'dialog', 'young', 'interaction', 'behavior'",3,"main user group', 'old user', 'spoken dialog system', 'young user', 'appointment scheduling dialog', 'group membership', 'interaction behavior', 'statistical cluster analysis', 'interaction style', 'observed behavior'",5,"older users', 'most studies', 'main user groups', 'younger users', 'users', 'studies', 'older', 'most', 'dialog systems', 'voice'",4,"appointment scheduling dialogs', 'interaction behavior', 'main user groups', 'older users', 'most studies', 'interaction style', 'behavior', 'younger users', 'users', 'studies'",5,"groups social group', 'main user groups', 'efficiently social users', 'systems adapt users', 'dialog systems adapt', 'user groups social', 'group factual users', 'user groups', 'predict group membership', 'did predict group'",5
"tablet pcs are gaining popularity, but many individuals still struggle with pen-based interaction. in a previous baseline study, we examined the types of difficulties younger and older adults encounter when using pen-based input. the research reported in this article seeks to address one of these errors, namely, missing just below. this error occurs in a menu selection task when a user's selection pattern is downwardly shifted, such that the top edge of the menu item below the target is selected relatively often, while the corresponding top edge of the target itself is seldom selected. we developed two approaches for addressing missing just below errors: reassigning selections along the top edge and deactivating them. in a laboratory evaluation, only the deactivated edge approach showed promise overall. further analysis of our data revealed that individual differences played a large role in our results and identified a new source of selection difficulty. specifically, we observed two error-prone groups of users: the low hitters, who, like participants in the baseline study, made missing just below errors, and the high hitters, who, in contrast, had difficulty with errors on the item above. all but one of the older participants fell into one of these error-prone groups, reinforcing that older users do need better support for selecting menu items with a pen. preliminary analysis of the performance data suggests both of our approaches were beneficial for the low hitters, but that additional techniques are needed to meet the needs of the high hitters and to address the challenge of supporting both groups in a single interface.","aging', 'interaction techniques', 'menu design', 'older users', 'pen-based target acquisition'",2,"error', 'tablet pcs', 'selection difficulty', 'pen', 'previous baseline study', 'menu selection task', 'old user', 'menu item', 'difficulty young', 'old adult'",2,"menu selection task', 'previous baseline study', 'menu item', 'low hitter', 'prone group', 'old adult', 'high hitter', 'difficulty young', 'error', 'pen'",5,"error', 'edge', 'hitter', 'selection', 'pen', 'difficulty', 'item', 'group', 'user', 'menu'",4,"menu selection task', 'selection difficulty', 'old user', 'edge approach', 'old participant', 'selection pattern', 'reassigning selection', 'menu item', 'error', 'old adult'",4,"tablet pcs', 'pcs', 'many individuals', 'popularity', 'tablet', 'individual differences', 'previous baseline study', 'many', 'menu selection task', 'selection difficulty'",5,"laboratory evaluation', 'tablet pcs', 'interaction', 'pcs', 'many individuals', 'performance data', 'user', 'popularity', 'tablet', 'single interface'",5,"older users need', 'menu selection task', 'selecting menu items', 'analysis performance data', 'performance data suggests', 'meet needs high', 'pen based input', 'menu items pen', 'differences played large', 'error prone groups'",5
"the increasing permeation of technology in our society leads to the challenge that everybody needs to interact with technology systems. older adults often meet difficulties while trying to interact with complex, demanding systems in their daily life. one approach to enable older adults to use new technologies in a safe and efficient way is the provision of training programs. in this article we report about a promising training strategy using video modeling in conjunction with other instructional methods to enhance learning. cognitive as well as socio-motivational aspects will be addressed. we assessed if guided error training in video modeling will improve learning outcomes for a ticket vending machine (tvm). to investigate if the training method might be beneficial for younger adults as well, we compared 40 younger and 40 older adult learners in a guided error training course with error-free training. younger and older participants made fewer mistakes in guided error training, but no differences occurred in task completion times. moreover, self-efficacy increased with training for both age groups, but no significant differences were found for the training condition. analysis of knowledge gains showed a significant benefit of guided error training in structural knowledge. overall, the results showed that guided error training may enhance learning for younger and older adults who are learning to use technology.","guided error training', 'instruction', 'older adults', 'self-efficacy', 'technology use', 'video modeling'",3,"old adult learner', 'technology system', 'new technology', 'error training course', 'training method', 'promising training strategy', 'training program', 'free training', 'young adult', 'training condition'",3,"old adult', 'task completion time', 'error training course', 'video modeling', 'old adult learner', 'promising training strategy', 'ticket vending machine', 'daily life', 'new technology', 'efficient way'",4,"training', 'error', 'old', 'adult', 'technology', 'young', 'system', 'video', 'modeling', 'method'",3,"error training course', 'promising training strategy', 'training method', 'free training', 'training program', 'training condition', 'old adult learner', 'young adult', 'technology system', 'ticket vending machine'",5,"technology systems', 'new technologies', 'older adult learners', 'older adults', 'technologies', 'younger adults', 'error training course', 'error training', 'adults', 'older participants'",5,"video modeling', 'promising training strategy', 'technology systems', 'new technologies', 'error training course', 'error training', 'older adult learners', 'older adults', 'training method', 'technology'",5,"methods enhance learning', 'training enhance learning', 'enhance learning cognitive', 'improve learning', 'enhance learning', 'learning use technology', 'instructional methods enhance', 'training condition analysis', 'using video', 'modeling improve learning'",4
"the need for security features to stop spam and bots has prompted research aimed at developing human interaction proofs (hips) that are both secure and easy to use. the primarily visual techniques used in these hip tools present difficulties for users with visual impairments. this article reports on the development of human-interaction proof, universally usable (hipuu), a new approach to human-interaction proofs based on identification of a series of sound/image pairs. simultaneous presentation of a single, unified task in two alternative modalities provides multiple paths to successful task completion. we present two alternative task completion strategies, based on differing input strategies (menu-based vs. free text entry). empirical results from studies involving both blind and sighted users validate both the usability and accessibility of these differing strategies, with blind users achieving successful task completion rates above 90\%. the strengths of the alternate task completion strategies are discussed, along with possible approaches for improving the robustness of hipuu.","blind users', 'captcha', 'hip', 'security', 'universal usability'",3,"human interaction proof', 'alternative task completion strategy', 'successful task completion rate', 'need', 'unified task', 'security feature', 'hip tool present difficulty', 'blind user', 'visual technique', 'user validate'",3,"alternate task completion strategy', 'hip tool present difficulty', 'successful task completion rate', 'human interaction proof', 'free text entry', 'security feature', 'visual technique', 'visual impairment', 'multiple path', 'new approach'",4,"task', 'completion', 'interaction', 'strategy', 'proof', 'human', 'user', 'hipuu', 'visual', 'successful'",3,"alternative task completion strategy', 'successful task completion rate', 'hip tool present difficulty', 'human interaction proof', 'blind user', 'free text entry', 'unified task', 'user validate', 'input strategy', 'visual technique'",5,"security features', 'human interaction proofs', 'security', 'features', 'need', 'interaction proofs', 'spam', 'bots', 'successful task completion', 'human'",3,"security features', 'accessibility', 'usability', 'human interaction proofs', 'interaction proof', 'spam', 'security', 'hip tools', 'features', 'input strategies'",4,"improving robustness', 'validate usability accessibility', 'primarily visual techniques', 'spam bots', 'modalities provides multiple', 'visual techniques', 'approaches improving robustness', 'blind users achieving', 'visual techniques used', 'menu based vs'",5
"there is a variety of brain-based interface methods which depend on measuring small changes in brain signals or properties. these methods have typically been used for nontraditional assistive technology applications. non-traditional assistive technology is generally targeted for users with severe motor disabilities which may last long-term due to illness or injury or short-term due to situational disabilities. control of a nontraditional assistive technology can vary widely across users depending upon many factors ranging from health to experience. unfortunately, there is no systematic method for assessing usability of nontraditional assistive technologies to achieve the best control. the current methods to accommodate users through trial-and-error result in the loss of valuable time and resources as users sometimes have diminishing abilities or suffer from terminal illnesses. this work describes a methodology for objectively measuring an individual's ability to control a specific nontraditional assistive technology, thus expediting the technology-fit process.","assistive technology', 'brain-based interfaces', 'brain-computer interface', 'direct-brain interface', 'functional near-infrared', 'galvanic skin response', 'individual characteristics', 'user profiles'",4,"nontraditional assistive technology application', 'specific nontraditional assistive technology', 'interface method', 'current method', 'systematic method', 'user', 'severe motor disability', 'brain signal', 'situational disability', 'term'",5,"nontraditional assistive technology', 'nontraditional assistive technology application', 'specific nontraditional assistive technology', 'severe motor disability', 'brain signal', 'user', 'small change', 'interface method', 'situational disability', 'systematic method'",5,"technology', 'assistive', 'nontraditional', 'method', 'user', 'control', 'brain', 'term', 'disability', 'illness'",4,"nontraditional assistive technology application', 'specific nontraditional assistive technology', 'interface method', 'systematic method', 'current method', 'severe motor disability', 'user', 'good control', 'situational disability', 'terminal illness'",4,"brain signals', 'nontraditional assistive technologies', 'interface methods', 'brain', 'non-traditional assistive technology', 'systematic method', 'current methods', 'methods', 'small changes', 'variety'",5,"usability', 'interface methods', 'brain signals', 'best control', 'nontraditional assistive technologies', 'control', 'brain', 'non-traditional assistive technology', 'systematic method', 'current methods'",4,"nontraditional assistive technology', 'assistive technology applications', 'nontraditional assistive technologies', 'assistive technology', 'traditional assistive technology', 'assistive technology vary', 'assistive technologies', 'assistive technology generally', 'assessing usability nontraditional', 'used nontraditional assistive'",4
"mobile computing devices can offer older adults (ages 65+) support in their daily lives, but older adults often find such devices difficult to learn and use. one potential design approach to improve the learnability of mobile devices is a multi-layered (ml) interface, where novice users start with a reduced-functionality interface layer that only allows them to perform basic tasks, before progressing to a more complex interface layer when they are comfortable. we studied the effects of a ml interface on older adults' performance in learning tasks on a mobile device. we conducted a controlled experiment with 16 older (ages 65--81) and 16 younger participants (age 21--36), who performed tasks on either a 2-layer or a nonlayered (control) address book application, implemented on a commercial smart phone. we found that the ml interface's reduced-functionality layer, compared to the control's full-functionality layer, better helped users to master a set of basic tasks and to retain that ability 30 minutes later. when users transitioned from the reduced-functionality to the full-functionality interface layer, their performance on the previously learned tasks was negatively affected, but no negative impact was found on learning new, advanced tasks. overall, the ml interface provided greater benefit for older participants than for younger participants in terms of task completion time during initial learning, perceived complexity, and preference. we discuss how the ml interface approach is suitable for improving the learnability of mobile applications, particularly for older adults.","age-related differences', 'learnability', 'menu design', 'mobile devices', 'multi-layered interfaces', 'older adults', 'user study'",3,"mobile computing device', 'mobile device', 'mobile application', 'old adult', 'device difficult', 'old participant', 'functionality interface layer', 'complex interface layer', 'ml interface approach', 'functionality layer'",3,"functionality interface layer', 'complex interface layer', 'old adult', 'commercial smart phone', 'address book application', 'mobile computing device', 'potential design approach', 'mobile device', 'ml interface approach', 'basic task'",3,"interface', 'task', 'functionality', 'layer', 'old', 'ml', 'mobile', 'device', 'adult', 'age'",3,"functionality interface layer', 'complex interface layer', 'ml interface approach', 'task completion time', 'mobile computing device', 'basic task', 'old adult', 'old participant', 'advanced task', 'mobile device'",5,"mobile computing devices', 'mobile device', 'older adults', 'functionality interface layer', 'mobile applications', 'ml interface approach', 'older participants', 'such devices', 'complex interface layer', 'ml interface'",5,"complexity', 'mobile computing devices', 'functionality interface layer', 'potential design approach', 'mobile applications', 'ml interface approach', 'mobile device', 'complex interface layer', 'ml interface', 'control'",5,"improve learnability mobile', 'interface novice users', 'improving learnability mobile', 'task completion time', 'interface provided greater', 'performance learning tasks', 'learnability mobile devices', 'interface older adults', 'older adults performance', 'older participants'",5
"motion-capture recordings of sign language are used in research on automatic recognition of sign language or generation of sign language animations, which have accessibility applications for deaf users with low levels of written-language literacy. motion-capture gloves are used to record the wearer's handshape. unfortunately, they require a time-consuming and inexact calibration process each time they are worn. this article describes the design and evaluation of a new calibration protocol for motion-capture gloves, which is designed to make the process more efficient and to be accessible for participants who are deaf and use american sign language (asl). the protocol was evaluated experimentally; deaf asl signers wore the gloves, were calibrated (using the new protocol and using a calibration routine provided by the glove manufacturer), and were asked to perform sequences of asl handshapes. five native asl signers rated the correctness and understandability of the collected handshape data. in an additional evaluation, asl signers were asked to perform asl stories while wearing the gloves and a motion-capture bodysuit (in some cases our new calibration protocol was used, in other cases, the standard protocol). later, twelve native asl signers watched animations produced from this motion-capture data and answered comprehension questions about the stories. in both evaluation studies, the new protocol received significantly higher scores than the standard calibration. the protocol has been made freely available online, and it includes directions for the researcher, images and videos of how participants move their hands during the process, and directions for participants (as asl videos and english text).","accessibility technology for the deaf', 'american sign language', 'animation', 'calibration', 'cyberglove', 'motion-capture glove'",3,"motion', 'capture glove', 'capture recording', 'sign language animation', 'capture datum', 'american sign language', 'capture bodysuit', 'language literacy', 'new calibration protocol', 'deaf asl signer'",4,"new calibration protocol', 'native asl signer', 'deaf asl signer', 'american sign language', 'sign language animation', 'inexact calibration process', 'motion', 'capture glove', 'new protocol', 'automatic recognition'",5,"protocol', 'asl', 'motion', 'capture', 'language', 'glove', 'sign', 'calibration', 'signer', 'new'",3,"deaf asl signer', 'new calibration protocol', 'native asl signer', 'asl handshape', 'asl story', 'inexact calibration process', 'sign language animation', 'asl video', 'new protocol', 'american sign language'",5,"sign language animations', 'american sign language', 'sign language', 'capture gloves', 'capture recordings', 'capture data', 'deaf asl signers', 'language literacy', 'capture bodysuit', 'new calibration protocol'",5,"additional evaluation', 'evaluation studies', 'evaluation', 'accessibility applications', 'automatic recognition', 'sign language animations', 'american sign language', 'motion', 'sign language', 'design'",4,"asl signers wore', 'evaluation asl signers', 'participants deaf use', 'use american sign', 'asl signers asked', 'signers watched animations', 'sign language animations', 'asl signers watched', 'perform asl stories', 'automatic recognition'",4
"this article presents the first optimal algorithm for trace scheduling. the trace is a global scheduling region used by compilers to exploit instruction-level parallelism across basic block boundaries. several heuristic techniques have been proposed for trace scheduling, but the precision of these techniques has not been studied relative to optimality. this article describes a technique for finding provably optimal trace schedules, where optimality is defined in terms of a weighted sum of schedule lengths across all code paths in a trace. the optimal algorithm uses branch-and-bound enumeration to efficiently explore the entire solution space. experimental evaluation of the algorithm shows that, with a time limit of 1 second per problem, 91\% of the hard trace scheduling problems in the spec cpu 2006 integer benchmarks are solved optimally. for 58\% of these hard problems, the optimal schedule is improved compared to that produced by a heuristic scheduler with a geometric mean improvement of 3.2\% in weighted schedule length and 18\% in compensation code size.","branch-and-bound enumeration', 'compiler optimizations', 'global instruction scheduling', 'instruction-level parallelism', 'instruction scheduling', 'optimal instruction scheduling', 'trace scheduling'",2,"optimal trace schedule', 'hard trace scheduling problem', 'optimal algorithm', 'optimal schedule', 'global scheduling region', 'article', 'heuristic technique', 'weighted schedule length', 'hard problem', 'basic block boundary'",3,"hard trace scheduling problem', 'optimal algorithm', 'entire solution space', 'optimal trace schedule', 'basic block boundary', 'global scheduling region', 'geometric mean improvement', 'level parallelism', 'article', 'heuristic technique'",5,"trace', 'scheduling', 'optimal', 'schedule', 'algorithm', 'technique', 'problem', 'article', 'optimality', 'hard'",2,"hard trace scheduling problem', 'optimal trace schedule', 'weighted schedule length', 'optimal schedule', 'optimal algorithm', 'global scheduling region', 'compensation code size', 'heuristic technique', 'hard problem', 'basic block boundary'",5,"optimal trace schedules', 'first optimal algorithm', 'trace scheduling', 'optimal schedule', 'optimal algorithm', 'global scheduling region', 'several heuristic techniques', 'trace', 'weighted schedule length', 'optimal'",5,"trace scheduling', 'global scheduling region', 'experimental evaluation', 'first optimal algorithm', 'optimal trace schedules', 'optimal algorithm', 'compilers', 'optimal schedule', 'algorithm', 'optimality'",5,"instruction level parallelism', 'compilers exploit instruction', 'boundaries heuristic techniques', 'heuristic techniques', '2006 integer benchmarks', 'efficiently explore entire', 'optimal algorithm uses', 'article presents optimal', 'algorithm uses branch', 'algorithm shows time'",3
"choosing the most appropriate optimization phase ordering has been a long-standing problem in compiler optimizations. exhaustive evaluation of all possible orderings of optimization phases for each function is generally dismissed as infeasible for production-quality compilers targeting accepted benchmarks. in this article, we show that it is possible to exhaustively evaluate the optimization phase order space for each function in a reasonable amount of time for most of the functions in our benchmark suite. to achieve this goal, we used various techniques to significantly prune the optimization phase order search space so that it can be inexpensively enumerated in most cases and reduce the number of program simulations required to evaluate program performance for each distinct phase ordering. the techniques described are applicable to other compilers in which it is desirable to find the best phase ordering for most functions in a reasonable amount of time. we also describe some interesting properties of the optimization phase order space, which will prove useful for further studies of related problems in compilers.","exhaustive search', 'iterative compilation', 'phase ordering'",2,"appropriate optimization phase ordering', 'optimization phase order search space', 'compiler optimization', 'distinct phase', 'good phase', 'possible ordering', 'quality compiler', 'function', 'exhaustive evaluation', 'benchmark suite'",4,"optimization phase order space', 'appropriate optimization phase ordering', 'function', 'possible ordering', 'compiler', 'quality compiler', 'problem', 'exhaustive evaluation', 'compiler optimization', 'benchmark suite'",4,"phase', 'optimization', 'order', 'function', 'compiler', 'space', 'reasonable', 'time', 'benchmark', 'problem'",3,"optimization phase order search space', 'appropriate optimization phase ordering', 'compiler optimization', 'quality compiler', 'possible ordering', 'distinct phase', 'good phase', 'program simulation', 'benchmark suite', 'program performance'",5,"optimization phases', 'best phase ordering', 'distinct phase ordering', 'compiler optimizations', 'optimizations', 'phases', 'possible orderings', 'most functions', 'ordering', 'quality compilers'",4,"optimization phases', 'exhaustive evaluation', 'best phase ordering', 'distinct phase ordering', 'quality compilers', 'other compilers', 'compiler optimizations', 'compilers', 'optimization', 'phase'",5,"evaluate program performance', 'exhaustively evaluate', 'evaluation possible orderings', 'optimization phase', 'evaluate optimization phase', 'possible orderings optimization', 'optimization phases function', 'related problems compilers', 'choosing appropriate optimization', 'prune optimization phase'",4
"retargetable c compilers are currently widely used to quickly obtain compiler support for new embedded processors and to perform early processor architecture exploration. a partially inherent problem of the retargetable compilation approach, though, is the limited code quality as compared to hand-written compilers or assembly code due to the lack of dedicated optimizations techniques. this problem can be circumvented by designing flexible, retargetable code optimization techniques that apply to a certain range of target architectures. this article focuses on target machines with simd instruction support, a common feature in embedded processors for multimedia applications. however, simd optimization is known to be a difficult task since simd architectures are largely nonuniform, support only a limited set of data types and impose several memory alignment constraints. additionally, such techniques require complicated loop transformations, which are tailored to the simd architecture in order to exhibit the necessary amount of parallelism in the code. thus, integrating the simd optimization {\em and\/} the required loop transformations together in a single retargeting formalism is an ambitious challenge. in this article, we present an efficient and quickly retargetable simd code optimization framework that is integrated into an industrial retargetable c compiler. experimental results for different processors demonstrate that the proposed technique applies to real-life target machines and that it produces code quality improvements close to the theoretical limit.","asip', 'retargetable compilers', 'simd', 'subword parallelism', 'vectorization'",3,"industrial retargetable c compiler', 'retargetable simd code optimization framework', 'retargetable code optimization technique', 'retargetable compilation approach', 'compiler support', 'limited code quality', 'simd architecture', 'simd optimization', 'early processor architecture exploration', 'code quality improvement close'",5,"retargetable c compiler', 'code quality', 'code optimization', 'optimization technique', 'simd optimization', 'simd architecture', 'target machine', 'loop transformation', 'instruction support', 'common feature'",5,"code', 'simd', 'optimization', 'retargetable', 'compiler', 'technique', 'architecture', 'processor', 'support', 'problem'",3,"retargetable simd code optimization framework', 'retargetable code optimization technique', 'industrial retargetable c compiler', 'code quality improvement close', 'early processor architecture exploration', 'limited code quality', 'simd instruction support', 'dedicated optimization technique', 'simd optimization', 'simd architecture'",5,"retargetable c compilers', 'retargetable compilation approach', 'compiler support', 'simd instruction support', 'retargetable', 'limited code quality', 'simd optimization', 'simd architectures', 'compiler', 'code quality improvements'",5,"simd optimization', 'simd architecture', 'retargetable c compilers', 'simd instruction support', 'compilers', 'multimedia applications', 'retargetable compilation approach', 'compiler support', 'retargetable', 'limited code quality'",4,"retargetable code optimization', 'embedded processors perform', 'retargetable compilation approach', 'problem retargetable compilation', 'retargetable compilers currently', 'retargetable compilers', 'quickly retargetable simd', 'retargetable simd code', 'flexible retargetable code', 'code optimization techniques'",4
"customizing architectures for particular applications is a promising approach to yield highly energy-efficient designs for embedded systems. this work explores the benefits of architectural customization for a class of embedded architectures typically used in energy- and area-constrained application domains, such as sensor nodes and multimedia processing. we implement a process flow that performs an automatic synthesis and evaluation of the different architectures based on runtime profiles of applications and determines an efficient architecture, with consideration for both energy and area constraints. an expressive architectural model, used by our engine, is introduced that takes advantage of efficient opcode allocation, several memory addressing modes, and operand types. by profiling embedded benchmarks from a variety of sensor and multimedia applications, we show that the energy savings resulting from various architectural optimizations relative to the base architectures (e.g., mips and msp430) are significant and can reach 50\%, depending on the application. we then identify the set of architectures that achieves near-optimal savings for a group of applications. finally, we propose the use of heterogeneous isa processors implementing those architectures as a solution to capitalize on energy savings provided by application customization while executing a range of applications efficiently.","efficient custom architectures', 'heterogeneous isa processors'",3,"efficient architecture', 'base architecture', 'different architecture', 'multimedia application', 'particular application', 'application customization', 'application domain', 'energy saving', 'architectural customization', 'architectural optimization relative'",5,"heterogeneous isa processor', 'architectural optimization relative', 'memory addressing mode', 'efficient opcode allocation', 'expressive architectural model', 'architecture', 'application', 'energy saving', 'area constraint', 'efficient architecture'",5,"application', 'architecture', 'energy', 'efficient', 'saving', 'area', 'multimedia', 'sensor', 'base', 'profile'",4,"efficient architecture', 'application customization', 'base architecture', 'multimedia application', 'different architecture', 'application domain', 'particular application', 'energy saving', 'architectural optimization relative', 'expressive architectural model'",5,"efficient architecture', 'particular applications', 'different architectures', 'base architectures', 'application customization', 'multimedia applications', 'architectures', 'application domains', 'applications', 'energy savings'",5,"evaluation', 'efficient architecture', 'energy savings', 'energy', 'area constraints', 'multimedia applications', 'particular applications', 'application customization', 'applications', 'several memory'",4,"embedded architectures typically', 'heterogeneous isa processors', 'benefits architectural customization', 'customizing architectures', 'performs automatic synthesis', 'application customization', 'architectural customization', 'various architectural optimizations', 'efficient opcode', 'evaluation different architectures'",4
"one important trend in today's microprocessor architectures is the increase in size of the processor caches. these caches also tend to be set associative. as technology scales, process variations are expected to increase the fault rates of the sram cells that compose such caches. as an important component of the processor, the parametric yield of sram cells is crucial to the overall performance and yield of the microchip. in this article, we propose a microarchitectural solution, called the buddy cache that permits large, set-associative caches to tolerate faults in sram cells due to process variations. in essence, instead of disabling a faulty cache block in a set (as is the current practice), it is paired with another faulty cache block in the same set --- the buddy. although both cache blocks are faulty, if the faults of the two blocks do not overlap, then instead of losing two blocks, buddying will yield a functional block from the nonfaulty portions of the two blocks. we found that with buddying, caches can better mitigate the negative impacts of process variations on performance and yield, gracefully downgrading performance as opposed to catastrophic failure. we will describe the details of the buddy cache and give insights as to why it is both more performance and yield resilient to faults.","caches', 'fault recovery', 'memory structures', 'processor architectures'",4,"faulty cache block', 'processor cache', 'associative cache', 'buddy cache', 'important trend', 'important component', 'sram cell', 'functional block', 'process variation', 'fault rate'",4,"faulty cache block', 'sram cell', 'process variation', 'buddy cache', 'important component', 'parametric yield', 'overall performance', 'microarchitectural solution', 'associative cache', 'fault rate'",4,"cache', 'block', 'yield', 'set', 'fault', 'performance', 'buddy', 'variation', 'sram', 'process'",3,"faulty cache block', 'associative cache', 'processor cache', 'buddy cache', 'functional block', 'parametric yield', 'overall performance', 'fault rate', 'sram cell', 'important component'",5,"faulty cache block', 'cache blocks', 'processor caches', 'important trend', 'buddy cache', 'associative caches', 'such caches', 'caches', 'important component', 'microprocessor architectures'",5,"processor caches', 'faulty cache block', 'cache blocks', 'buddy cache', 'important component', 'processor', 'important trend', 'associative caches', 'such caches', 'cache'",5,"today microprocessor architectures', 'associative caches', 'disabling faulty cache', 'buddying caches better', 'microarchitectural solution called', 'propose microarchitectural solution', 'microprocessor architectures increase', 'yield resilient faults', 'tolerate faults sram', 'overall performance yield'",5
"scratchpad memory (spm), a fast on-chip sram managed by software, is widely used in embedded systems. this article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an spm. the novelty of this approach lies in partitioning the spm into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between spm and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. our experimental results using a set of 10 c benchmarks from mediabench and mibench show that our methodology is capable of managing spms efficiently and effectively for large embedded applications. in addition, our spm allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based spm allocator and an ilp-based spm allocator.","graph coloring', 'live range splitting', 'memory allocation', 'memory coloring', 'register coalescing', 'scratchpad memory', 'software-managed cache'",3,"spm allocator', 'scratchpad memory', 'chip memory', 'memory coloring', 'static datum aggregate', 'chip sram', 'data aggregate', 'purpose compiler approach', 'register file', 'potential data transfer statement'",4,"potential data transfer statement', 'static datum aggregate', 'purpose compiler approach', 'spm', 'register file', 'scratchpad memory', 'chip sram', 'memory coloring', 'live range', 'chip memory'",5,"spm', 'register', 'memory', 'aggregate', 'allocator', 'file', 'chip', 'datum', 'pseudo', 'approach'",3,"spm allocator', 'potential data transfer statement', 'static datum aggregate', 'chip memory', 'purpose compiler approach', 'register file', 'memory coloring', 'data aggregate', 'scratchpad memory', 'c benchmark'",5,"chip memory', 'scratchpad memory', 'spm allocator', 'memory coloring', 'spm', 'memory', 'static data aggregates', 'purpose compiler approach', 'chip sram', 'data aggregates'",5,"memory coloring', 'scratchpad memory', 'chip memory', 'static data aggregates', 'spm allocator', 'data aggregates', 'spm', 'memory', 'purpose compiler approach', 'chip sram'",4,"spm allocator ilp', 'compiler approach called', 'algorithm register allocation', 'spm allocator', 'spm pseudo register', 'pseudo register file', 'heuristics based spm', 'based spm allocator', 'aliased registers splitting', 'fast chip sram'",4
"out-of-order speculative processors need a bookkeeping method to recover from incorrect speculation. in recent years, several microarchitectures that employ checkpoints have been proposed, either extending the reorder buffer or entirely replacing it. this work presents an in-dept-study of checkpointing in checkpoint-based microarchitectures, from the desired content of a checkpoint, via implementation trade-offs, and to checkpoint allocation and release policies. a major contribution of the article is a novel adaptive checkpoint allocation policy that outperforms known policies. the adaptive policy controls checkpoint allocation according to dynamic events, such as second-level cache misses and rollback history. it achieves 6.8\% and 2.2\% speedup for the integer and floating point benchmarks, respectively, and does not require a branch confidence estimator. the results show that the proposed adaptive policy achieves most of the potential of an oracle policy whose performance improvement is 9.8\% and 3.9\% for the integer and floating point benchmarks, respectively. we exploit known techniques for saving leakage power by adapting and applying them to checkpoint-based microarchitectures. the proposed applications combine to reduce the leakage power of the register file to about one half of its original value.","checkpoint', 'early register release', 'leakage', 'misprediction', 'out-of-order execution', 'rollback'",3,"novel adaptive checkpoint allocation policy', 'adaptive policy', 'release policy', 'microarchitecture', 'oracle policy', 'order speculative processor', 'recent year', 'bookkeeping method', 'point benchmark', 'reorder buffer'",5,"policy control checkpoint allocation', 'adaptive checkpoint allocation policy', 'order speculative processor', 'level cache miss', 'branch confidence estimator', 'microarchitecture', 'point benchmark', 'recent year', 'reorder buffer', 'incorrect speculation'",5,"checkpoint', 'policy', 'microarchitecture', 'allocation', 'adaptive', 'benchmark', 'integer', 'leakage', 'power', 'speculation'",3,"novel adaptive checkpoint allocation policy', 'adaptive policy', 'oracle policy', 'release policy', 'order speculative processor', 'level cache miss', 'branch confidence estimator', 'leakage power', 'reorder buffer', 'implementation trade'",5,"speculative processors', 'speculative', 'bookkeeping method', 'processors', 'order', 'checkpoint allocation', 'incorrect speculation', 'bookkeeping', 'checkpoints', 'method'",3,"leakage power', 'speculative processors', 'speculative', 'checkpoints', 'bookkeeping method', 'release policies', 'policies', 'processors', 'reorder buffer', 'order'",3,"floating point benchmarks', 'order speculative processors', 'microarchitectures employ checkpoints', 'misses rollback history', 'checkpoint based microarchitectures', 'novel adaptive checkpoint', 'outperforms known policies', 'cache misses rollback', 'adaptive policy', 'cache misses'",4
"as technology has advanced, the application space of very long instruction word (vliw) processors has grown to include a variety of embedded platforms. due to cost and power consumption constraints, many embedded vliw processors contain limited resources, including registers. as a result, a vliw compiler that maximizes instruction level parallelism (ilp) without considering register constraints may generate excessive register spills, leading to reduced overall system performance. to address this issue, this article presents a new spill reduction technique that improves vliw runtime performance by reordering operations prior to register allocation and instruction scheduling. unlike earlier algorithms, our approach explicitly considers both register reduction and data dependency in performing operation reordering. data dependency control limits unexpected schedule length increases during subsequent instruction scheduling. our technique has been evaluated using trimaran, an academic vliw compiler, and evaluated using a set of embedded systems benchmarks. experimental results show that, on average, this technique improves vliw performance by 10\% for vliw processors with 32 registers and 8 functional units compared with previous spill reduction techniques. limited improvement is seen versus prior approaches for vliw processors with 64 registers and 8 functional units.","instruction level parallelism', 'register pressure', 'very long instruction word (vliw) processor'",3,"vliw processor', 'vliw runtime performance', 'vliw performance', 'academic vliw compiler', 'excessive register spill', 'long instruction word', 'register constraint', 'instruction level parallelism', 'subsequent instruction scheduling', 'new spill reduction technique'",5,"spill reduction technique', 'power consumption constraint', 'vliw processor', 'long instruction word', 'vliw compiler', 'instruction scheduling', 'datum dependency', 'level parallelism', 'system performance', 'register spill'",4,"vliw', 'register', 'processor', 'instruction', 'technique', 'performance', 'spill', 'reduction', 'scheduling', 'functional'",3,"excessive register spill', 'vliw runtime performance', 'previous spill reduction technique', 'new spill reduction technique', 'register constraint', 'academic vliw compiler', 'vliw performance', 'vliw processor', 'unexpected schedule length increase', 'instruction level parallelism'",5,"application space', 'long instruction word', 'application', 'vliw processors', 'subsequent instruction scheduling', 'vliw runtime performance', 'academic vliw compiler', 'instruction level parallelism', 'vliw performance', 'vliw compiler'",5,"subsequent instruction scheduling', 'instruction scheduling', 'power consumption constraints', 'earlier algorithms', 'register constraints', 'vliw runtime performance', 'vliw performance', 'vliw processors', 'academic vliw compiler', 'vliw compiler'",5,"instruction level parallelism', 'improves vliw', 'word vliw processors', 'vliw processors 32', 'improves vliw performance', 'vliw performance 10', 'register allocation instruction', 'improves vliw runtime', 'embedded vliw', 'approaches vliw'",5
"register pressure in modern superscalar processors can be reduced by releasing registers early and by copying their contents to cheap back-up storage. this article quantifies the potential benefits of register occupancy reduction and shows that existing hardware-based schemes typically achieve only a small fraction of this potential. this is because they are unable to accurately determine the last use of a register and must wait until the redefining instruction enters the pipeline. on the other hand, compilers have a global view of the program and, using simple dataflow analysis, can determine the last use. this article evaluates the extent to which compiler analysis can aid early releasing, explores the design space, and introduces commit and issue-based early releasing schemes, quantifying their benefits. using simple compiler analysis and microarchitecture changes, we achieve 70\% of the potential register file occupancy reduction. by adding more hardware support, we can increase this to 94\%. our schemes are compared to state-of-the-art approaches for varying register file sizes and are shown to outperform these existing techniques.","compiler', 'energy efficiency', 'low-power design', 'microarchitecture', 'register file'",3,"potential register file occupancy reduction', 'register occupancy reduction', 'register file size', 'early releasing scheme', 'simple compiler analysis', 'potential benefit', 'modern superscalar processor', 'simple dataflow analysis', 'pressure', 'article'",3,"potential register file occupancy reduction', 'register occupancy reduction', 'simple compiler analysis', 'modern superscalar processor', 'simple dataflow analysis', 'early releasing scheme', 'register file size', 'potential benefit', 'article', 'small fraction'",4,"register', 'potential', 'analysis', 'early', 'scheme', 'compiler', 'article', 'simple', 'occupancy', 'reduction'",2,"potential register file occupancy reduction', 'register occupancy reduction', 'register file size', 'early releasing scheme', 'simple compiler analysis', 'simple dataflow analysis', 'modern superscalar processor', 'potential benefit', 'hardware support', 'art approach'",5,"register occupancy reduction', 'register pressure', 'register file sizes', 'modern superscalar processors', 'register', 'potential benefits', 'simple compiler analysis', 'last use', 'simple dataflow analysis', 'compiler analysis'",5,"register occupancy reduction', 'register pressure', 'register file sizes', 'compilers', 'design space', 'modern superscalar processors', 'register', 'more hardware support', 'art approaches', 'hardware'",4,"modern superscalar processors', 'hardware based schemes', 'analysis microarchitecture changes', 'analysis microarchitecture', 'microarchitecture changes achieve', 'register occupancy reduction', 'determine use register', 'simple compiler analysis', 'extent compiler analysis', 'existing hardware based'",5
"in this article, we investigate the intellectual property (ip) mapping problem that maps a given set of ip cores onto the tiles of a mesh-based network-on-chip (noc) architecture such that the power consumption due to intercore communications is minimized. this ip mapping problem is considered under both bandwidth and latency constraints as imposed by the applications and the on-chip network infrastructure. by examining various applications' communication characteristics extracted from their respective communication trace graphs, two distinguishable connectivity templates are realized: the graphs with tightly coupled vertices and those with distributed vertices. these two templates are formally defined in this article, and different mapping heuristics are subsequently developed to map them. in general, tightly coupled vertices are mapped onto tiles that are physically close to each other while the distributed vertices are mapped following a graph partition scheme. experimental results on both random and multimedia benchmarks have confirmed that the proposed template-based mapping algorithm achieves an average of 15\% power savings as compared with moca, a fast greedy-based mapping algorithm. compared with a branch-and-bound--based mapping algorithm, which produces near optimal results but incurs an extremely high computation cost, the proposed algorithm, due to its polynomial runtime complexity, can generate the results of almost the same quality with much less cpu time. as the on-chip network size increases, the superiority of the proposed algorithm becomes more evident.","bandwidth and latency constraints', 'ip mapping', 'low power', 'network-on-chip (noc)'",3,"ip mapping problem', 'mapping algorithm', 'different mapping heuristic', 'article', 'ip core', 'respective communication trace graph', 'chip network infrastructure', 'communication characteristic', 'chip network size increase', 'intercore communication'",4,"distinguishable connectivity template', 'communication trace graph', 'respective communication trace', 'chip network infrastructure', 'ip mapping problem', 'mapping algorithm', 'ip core', 'communication characteristic', 'intercore communication', 'latency constraint'",5,"mapping', 'algorithm', 'vertex', 'ip', 'chip', 'network', 'communication', 'template', 'graph', 'result'",2,"respective communication trace graph', 'ip mapping problem', 'chip network size increase', 'mapping algorithm', 'different mapping heuristic', 'graph partition scheme', 'chip network infrastructure', 'distinguishable connectivity template', 'communication characteristic', 'intercore communication'",5,"ip mapping problem', 'based mapping algorithm', 'mapping algorithm', 'different mapping heuristics', 'mapping problem', 'intellectual property', 'ip cores', 'mapping', 'article', 'distinguishable connectivity templates'",5,"latency constraints', 'based mapping algorithm', 'mapping algorithm', 'ip mapping problem', 'polynomial runtime complexity', 'architecture', 'distinguishable connectivity templates', 'different mapping heuristics', 'mapping problem', '% power savings'",5,"high computation cost', 'compared branch bound', 'ip mapping problem', 'based mapping algorithm', 'mapping algorithm achieves', 'mesh based network', 'cost proposed algorithm', 'graph partition scheme', 'power consumption intercore', 'power savings compared'",5
"previous researches show that a scratchpad memory device consumed less energy than a cache device with the same capacity. in this article, we locate the scratchpad memory (spm) in the top level of the memory hierarchy to reduce the energy consumption. to take the advantage of a spm, we address two issues of utilizing a spm. first, the program's locality should be improved. the second issue is spm management. to tackle these two issues, we present a hardware/software framework for dynamically allocating both instructions and data in spm. the software flow could be divided into three phases: locality improving, locality extraction, and runtime spm management. without modifying the original compiler and the source code, we improve the locality of a program. an optimization algorithm is proposed to extract the spm allocations. at runtime, an spm management program is employed. in hardware, an address translation logic (atl) is proposed to reduce the overhead of spm management.\par the results show that the proposed framework can reduce energy delay product (edp) by 63\%, on average, when compared with the traditional cache architecture. the reduction in edp is contributed by properly allocating both instructions and data in spm. by allocating only instructions in spm, the edps are reduced by 45\%, on average. by allocating only data in spm, the edps are reduced by 14\%, on average.","allocation algorithm', 'memory allocation', 'scratchpad memory'",2,"spm management', 'spm management.par', 'spm allocation', 'scratchpad memory device', 'previous research', 'memory hierarchy', 'energy consumption', 'energy delay product', 'cache device', 'management program'",3,"traditional cache architecture', 'scratchpad memory device', 'energy delay product', 'address translation logic', 'spm', 'cache device', 'previous research', 'energy consumption', 'management program', 'spm allocation'",3,"spm', 'locality', 'memory', 'average', 'datum', 'management', 'instruction', 'energy', 'issue', 'program'",3,"spm management', 'spm management.par', 'spm allocation', 'scratchpad memory device', 'energy delay product', 'locality extraction', 'address translation logic', 'software framework', 'traditional cache architecture', 'cache device'",5,"scratchpad memory device', 'previous researches', 'scratchpad memory', 'spm management program', 'runtime spm management', 'spm management', 'memory hierarchy', 'researches', 'spm allocations', 'scratchpad'",4,"optimization algorithm', 'energy delay product', 'scratchpad memory device', 'traditional cache architecture', 'scratchpad memory', 'less energy', 'energy consumption', 'memory hierarchy', 'previous researches', 'cache device'",4,"properly allocating instructions', 'reduce overhead spm', 'reduce energy delay', 'reduce overhead', 'reduce energy consumption', 'compared traditional cache', 'proposed reduce overhead', 'locality improving locality', 'improving locality', 'allocating instructions'",5
"heterogeneous multicore processors have emerged as an energy- and area-efficient architectural solution to improving performance for domain-specific applications such as those with a plethora of data-level parallelism. these processors typically contain a large number of small, compute-centric cores for acceleration while keeping one or two high-performance ilp cores on the die to guarantee single-thread performance. although a major portion of the transistors are occupied by the acceleration cores, these resources will sit idle when running unparallelized legacy codes or the sequential part of an application. to address this underutilization issue, in this article, we introduce chameleon, a flexible heterogeneous multicore architecture to virtualize these resources for enhancing memory performance when running sequential programs. the chameleon architecture can dynamically virtualize the idle acceleration cores into a last-level cache, a data prefetcher, or a hybrid between these two techniques. in addition, chameleon can operate in an adaptive mode that dynamically configures the acceleration cores between the hybrid mode and the prefetch-only mode by monitoring the effectiveness of the chameleon cache mode. in our evaluation with spec2006 benchmark suite, different levels of performance improvements were achieved in different modes for different applications. in the case of the adaptive mode, chameleon improves the performance of specint06 and specfp06 by 31\% and 15\%, on average. when considering only memory-intensive applications, chameleon improves the system performance by 50\% and 26\% for specint06 and specfp06, respectively.","cache', 'heterogeneous multicore', 'idle core', 'prefetching'",2,"heterogeneous multicore processor', 'flexible heterogeneous multicore architecture', 'performance ilp core', 'thread performance', 'memory performance', 'performance improvement', 'system performance', 'idle acceleration core', 'chameleon cache mode', 'centric core'",5,"heterogeneous multicore', 'acceleration core', 'adaptive mode', 'multicore processor', 'architectural solution', 'efficient architectural', 'performance improvement', 'specific application', 'different level', 'legacy code'",4,"performance', 'chameleon', 'mode', 'core', 'acceleration', 'application', 'level', 'different', 'processor', 'heterogeneous'",3,"performance ilp core', 'chameleon cache mode', 'idle acceleration core', 'memory performance', 'different mode', 'performance improvement', 'thread performance', 'system performance', 'flexible heterogeneous multicore architecture', 'hybrid mode'",5,"heterogeneous multicore processors', 'performance ilp cores', 'chameleon cache mode', 'multicore', 'idle acceleration cores', 'heterogeneous', 'chameleon architecture', 'efficient architectural solution', 'memory performance', 'acceleration cores'",5,"evaluation', 'chameleon architecture', 'energy', 'memory performance', 'performance ilp cores', 'heterogeneous multicore processors', 'chameleon cache mode', 'memory', 'thread performance', 'system performance'",4,"performance specint06 specfp06', 'heterogeneous multicore processors', 'data level parallelism', 'improves performance specint06', 'memory intensive applications', 'improves performance 50', 'performance domain specific', 'improving performance domain', 'heterogeneous multicore architecture', 'prefetcher hybrid techniques'",4
"traditional coherence protocols present a set of difficult trade-offs: the reliance of snoopy protocols on broadcast and ordered interconnects limits their scalability, while directory protocols incur a performance penalty on sharing misses due to indirection. this work introduces patch (predictive/adaptive token-counting hybrid), a coherence protocol that provides the scalability of directory protocols while opportunistically sending direct requests to reduce sharing latency. patch extends a standard directory protocol to track tokens and use token-counting rules for enforcing coherence permissions. token counting allows patch to support direct requests on an unordered interconnect, while a mechanism called {\em token tenure\/} provides broadcast-free forward progress using the directory protocol's per-block point of ordering at the home along with either timeouts at requesters or explicit race notification messages.\par patch makes three main contributions. first, patch introduces token tenure, which provides broadcast-free forward progress for token-counting protocols. second, patch deprioritizes best-effort direct requests to match or exceed the performance of directory protocols without restricting scalability. finally, patch provides greater scalability than directory protocols when using inexact encodings of sharers because only processors holding tokens need to acknowledge requests. overall, patch is a ``one-size-fits-all'' coherence protocol that dynamically adapts to work well for small systems, large systems, and anywhere in between.","adaptive', 'bandwidth-efficiency', 'cache coherence protocol', 'predictive', 'token coherence'",3,"traditional coherence protocol', 'standard directory protocol', 'snoopy protocol', 'coherence permission', 'token counting', 'introduce token tenure', 'adaptive token', 'explicit race notification messages.par patch', 'effort direct request', 'broadcast'",4,"free forward progress', 'explicit race notification messages.\\par', 'directory protocol', 'coherence protocol', 'traditional coherence protocol', 'notification messages.\\par patch', 'introduce token tenure', 'standard directory protocol', 'effort direct request', 'performance penalty'",3,"protocol', 'token', 'patch', 'directory', 'coherence', 'scalability', 'request', 'counting', 'broadcast', 'free'",2,"explicit race notification messages.par patch', 'traditional coherence protocol', 'standard directory protocol', 'token counting', 'introduce token tenure', 'adaptive token', 'snoopy protocol', 'effort direct request', 'great scalability', 'coherence permission'",5,"traditional coherence protocols', 'coherence protocol', 'standard directory protocol', 'directory protocol', 'counting protocols', 'snoopy protocols', 'protocols', 'coherence permissions', 'coherence', 'token counting'",5,"traditional coherence protocols', 'sharing latency', 'counting protocols', 'coherence protocol', 'standard directory protocol', 'directory protocol', 'greater scalability', 'scalability', 'token counting', 'snoopy protocols'",5,"rules enforcing coherence', 'coherence protocol provides', 'protocol provides scalability', 'directory protocols opportunistically', 'provides scalability directory', 'scalability directory protocols', 'greater scalability directory', 'reduce sharing latency', 'protocol dynamically adapts', 'limits scalability directory'",5
"we propose and apply a new simulation paradigm for microarchitectural design evaluation and optimization. this paradigm enables more comprehensive design studies by combining spatial sampling and statistical inference. specifically, this paradigm (i) defines a large, comprehensive design space, (ii) samples points from the space for simulation, and (iii) constructs regression models based on sparse simulations. this approach greatly improves the computational efficiency of microarchitectural simulation and enables new capabilities in design space exploration.\par we illustrate new capabilities in three case studies for a large design space of approximately 260,000 points: (i) pareto frontier, (ii) pipeline depth, and (iii) multiprocessor heterogeneity analyses. in particular, regression models are exhaustively evaluated to identify pareto optimal designs that maximize performance for given power budgets. these models enable pipeline depth studies in which all parameters vary simultaneously with depth, thereby more effectively revealing interactions with nondepth parameters. heterogeneity analysis combines regression-based optimization with clustering heuristics to identify efficient design compromises between similar optimal architectures. these compromises are potential core designs in a heterogeneous multicore architecture. increasing heterogeneity can improve {\em bips\/}$^3$ / {\em w\/} efficiency by as much as 2.4\times , a theoretical upper bound on heterogeneity benefits that neglects contention between shared resources as well as design complexity. collectively these studies demonstrate regression models' ability to expose trends and identify optima in diverse design regions, motivating the application of such models in statistical inference for more effective use of modern simulator infrastructure.","microarchitecture', 'regression', 'simulation', 'statistics'",2,"comprehensive design study', 'large design space', 'design space exploration.par', 'microarchitectural design evaluation', 'pareto optimal design', 'efficient design compromise', 'potential core design', 'design complexity', 'diverse design region', 'new simulation paradigm'",5,"regression model', 'design space', 'statistical inference', 'pipeline depth', 'heterogeneity analysis', 'new capability', 'comprehensive design', 'space exploration.\\par', 'sparse simulation', 'microarchitectural simulation'",5,"design', 'model', 'regression', 'simulation', 'space', 'heterogeneity', 'study', 'paradigm', 'new', 'depth'",3,"comprehensive design study', 'large design space', 'pareto optimal design', 'design space exploration.par', 'efficient design compromise', 'microarchitectural design evaluation', 'potential core design', 'diverse design region', 'design complexity', 'new simulation paradigm'",4,"new simulation paradigm', 'comprehensive design space', 'large design space', 'design space exploration', 'microarchitectural design evaluation', 'efficient design compromises', 'microarchitectural simulation', 'diverse design regions', 'potential core designs', 'optimal designs'",4,"new simulation paradigm', 'microarchitectural simulation', 'clustering heuristics', 'simulation', 'microarchitectural design evaluation', 'design complexity', 'optimization', 'spatial sampling', 'design space exploration', 'heterogeneous multicore architecture'",5,"designs maximize performance', 'microarchitectural simulation enables', 'microarchitectural design evaluation', 'identify pareto optimal', 'improves computational efficiency', 'efficient design compromises', 'design space exploration', 'identify efficient design', 'designs heterogeneous multicore', 'microarchitectural simulation'",5
"memory accesses limit the performance of stream processors. by exploiting the reuse of data held in the stream register file (srf), an on-chip, software controlled storage, the number of memory accesses can be reduced. in current stream compilers, reuse exploitation is only attempted for simple stream references, those whose start and end are known. compiler analysis, from outside of stream processors, does not directly enable the consideration of other more complex stream references. in this article, we propose a transformation to automatically optimize stream programs to exploit the reuse supplied by loop-dependent stream references. the transformation is based on three results: lemmas identifying the reuse supplied by stream references, a new abstract representation called the stream reuse graph (srg) depicting the identified reuse, and the optimization of the srg for our transformation. both the reuse between the whole sequences accessed by stream references and between partial sequences is exploited in the article. in particular, partial reuse and its treatment are quite new and have never, to the best of our knowledge, appeared in scalar and vector processing. at the same time, reusing streams increases the pressure on the srf, and this presents a problem of which reuse should be exploited within limited srf capacity. we extend our analysis to achieve this objective. finally, we implement our techniques based on the streamc/kernelc compiler that has been optimized with the best existing compilation techniques for stream processors. experimental results show a resultant speed-up of 1.14 to 2.54 times using a range of benchmarks.","streamc', 'stream professor', 'stream programming model', 'stream register file', 'stream reuse'",3,"stream reuse graph', 'stream processor', 'simple stream reference', 'complex stream reference', 'dependent stream reference', 'current stream compiler', 'stream register file', 'stream program', 'stream increase', 'partial reuse'",4,"limited srf capacity', 'stream reuse graph', 'stream processor', 'memory access', 'new abstract representation', 'dependent stream reference', 'complex stream reference', 'simple stream reference', 'current stream compiler', 'stream register file'",5,"stream', 'reuse', 'reference', 'processor', 'access', 'compiler', 'srf', 'transformation', 'memory', 'article'",2,"stream reuse graph', 'current stream compiler', 'complex stream reference', 'dependent stream reference', 'simple stream reference', 'stream register file', 'stream processor', 'stream program', 'stream increase', 'partial reuse'",5,"simple stream references', 'complex stream references', 'dependent stream references', 'current stream compilers', 'stream references', 'stream processors', 'stream register file', 'stream programs', 'stream', 'memory'",4,"optimization', 'current stream compilers', 'simple stream references', 'complex stream references', 'dependent stream references', 'memory', 'stream references', 'stream processors', 'stream register file', 'stream programs'",4,"optimize stream programs', 'programs exploit reuse', 'stream programs exploit', 'accesses limit performance', 'exploiting reuse data', 'automatically optimize stream', 'stream reuse', 'performance stream', 'transformation reuse', 'current stream compilers'",5
"in recent years, circuit reliability in modern high-performance processors has become increasingly important. shrinking feature sizes and diminishing supply voltages have made circuits more sensitive to microprocessor supply voltage fluctuations. these fluctuations result from the natural variation of processor activity as workloads execute, but when left unattended, these voltage fluctuations can lead to timing violations or even transistor lifetime issues. in this article, we present a hardware--software collaborative approach to mitigate voltage fluctuations. a checkpoint-recovery mechanism rectifies errors when voltage violates maximum tolerance settings, while a runtime software layer reschedules the program's instruction stream to prevent recurring violations at the same program location. the runtime layer, combined with the proposed code-rescheduling algorithm, removes 60\% of all violations with minimal overhead, thereby significantly improving overall performance. our solution is a radical departure from the ongoing industry-standard approach to circumvent the issue altogether by optimizing for the worst-case voltage flux, which compromises power and performance efficiency severely, especially looking ahead to future technology generations. existing conservative approaches will have severe implications on the ability to deliver efficient microprocessors. the proposed technique reassembles a traditional reliability problem as a runtime performance optimization problem, thus allowing us to design processors for typical case operation by building intelligent algorithms that can prevent recurring violations.","di/dt', 'inductive noise', 'voltage emergencies', 'voltage noise'",2,"supply voltage fluctuation', 'case voltage flux', 'performance processor', 'runtime performance optimization problem', 'overall performance', 'performance efficiency', 'circuit reliability', 'recent year', 'processor activity', 'violation'",4,"standard approach', 'voltage fluctuation', 'ongoing industry', 'radical departure', 'overall performance', 'runtime layer', 'minimal overhead', 'supply voltage', 'program location', 'instruction stream'",5,"voltage', 'fluctuation', 'violation', 'performance', 'runtime', 'processor', 'approach', 'circuit', 'supply', 'layer'",3,"runtime performance optimization problem', 'supply voltage fluctuation', 'case voltage flux', 'performance processor', 'runtime software layer', 'software collaborative approach', 'traditional reliability problem', 'overall performance', 'performance efficiency', 'transistor lifetime issue'",5,"recent years', 'circuit reliability', 'voltage fluctuations', 'supply voltages', 'years', 'case voltage flux', 'traditional reliability problem', 'circuit', 'recent', 'performance processors'",4,"intelligent algorithms', 'power', 'processor activity', 'recent years', 'maximum tolerance settings', 'performance processors', 'circuit reliability', 'overall performance', 'performance efficiency', 'voltage fluctuations'",5,"efficient microprocessors', 'compromises power performance', 'efficient microprocessors proposed', 'technique reassembles traditional', 'overall performance solution', 'microprocessors proposed technique', 'microprocessors proposed', 'runtime performance optimization', 'minimal overhead', 'code rescheduling algorithm'",5
"we show how to test the bipartiteness of an intersection graph of $n$ line segments or simple polygons in the plane, or of an intersection graph of balls in $d$-dimensional euclidean space, in time {$ o(n \log n) $}. more generally, we find subquadratic algorithms for connectivity and bipartiteness testing of intersection graphs of a broad class of geometric objects. our algorithms for these problems return either a bipartition of the input or an odd cycle in its intersection graph. we also consider lower bounds for connectivity and {$k$}-colorability problems of geometric intersection graphs. for unit balls in {$d$} dimensions, connectivity testing has equivalent randomized complexity to construction of euclidean minimum spanning trees, and for line segments in the plane connectivity testing has the same lower bounds as hopcroft's point-line incidence testing problem; therefore, for these problems, connectivity is unlikely to be solved as efficiently as bipartiteness. for line segments or planar disks, testing {$k$}-colorability of intersection graphs for $k$ > 2 is np-complete.","bipartite graph', 'coin graph', 'disks', 'geometric thickness', 'graph coloring', ""hopcroft's problem"", 'intersection graph', 'line segments', 'minimum spanning tree'",3,"geometric intersection graph', 'bipartiteness testing', 'plane connectivity testing', 'line incidence testing problem', 'n$ line segment', 'k$}-colorability problem', 'low bound', 'subquadratic algorithm', 'geometric object', 'd$-dimensional euclidean space'",4,"intersection graph', 'line incidence testing problem', 'd$-dimensional euclidean space', 'plane connectivity testing', 'equivalent randomized complexity', 'line segment', 'geometric intersection graph', 'simple polygon', 'low bound', 'o(n \\log'",5,"intersection', 'graph', 'connectivity', 'line', 'problem', 'testing', 'segment', 'bipartiteness', 'intersection graph', 'plane'",4,"line incidence testing problem', 'plane connectivity testing', 'geometric intersection graph', 'bipartiteness testing', 'n$ line segment', 'k$}-colorability problem', 'd$-dimensional euclidean space', 'equivalent randomized complexity', 'euclidean minimum', 'low bound'",5,"geometric intersection graphs', 'intersection graphs', 'plane connectivity testing', 'bipartiteness testing', 'connectivity testing', 'line segments', 'intersection', 'graphs', 'log n', 'bipartiteness'",3,"plane connectivity testing', 'connectivity testing', 'geometric intersection graphs', 'intersection graphs', 'subquadratic algorithms', 'algorithms', 'bipartiteness testing', 'equivalent randomized complexity', 'trees', 'connectivity'",4,"bounds connectivity colorability', 'geometric intersection graphs', 'connectivity colorability problems', 'testing intersection graphs', 'colorability intersection graphs', 'colorability problems', 'minimum spanning trees', 'graph balls', 'subquadratic algorithms connectivity', 'intersection graph balls'",5
"we present randomized algorithms for online conflict-free coloring (cf in short) of points in the plane, with respect to halfplanes, congruent disks, and nearly-equal axis-parallel rectangles. in all three cases, the coloring algorithms use {$ o(\log n) $} colors, with high probability.\par we also present a deterministic algorithm for online cf coloring of points in the plane with respect to nearly-equal axis-parallel rectangles, using {$ o(\log^3 n) $} colors. this is the first efficient (i.e., using {$ \polylog (n) $} colors) deterministic online cf coloring algorithm for this problem.","coloring', 'conflict free coloring', 'online algorithms'",2,"coloring algorithm', 'randomized algorithm', 'online cf coloring', 'deterministic algorithm', 'deterministic online cf', 'online conflict', 'free coloring', 'equal axis', 'parallel rectangle', 'point'",5,"parallel rectangle', 'equal axis', 'coloring algorithm', 'deterministic online cf', 'online cf coloring', 'high probability.\\par', 'deterministic algorithm', 'free coloring', 'congruent disk', 'online conflict'",5,"color', 'algorithm', 'online', 'equal', 'axis', 'parallel', 'cf', 'rectangle', 'coloring', 'equal axis'",3,"online cf coloring', 'deterministic online cf', 'coloring algorithm', 'deterministic algorithm', 'randomized algorithm', 'free coloring', 'online conflict', 'parallel rectangle', 'high probability.par', 'equal axis'",5,"online cf coloring', 'coloring algorithms', 'deterministic online cf', 'randomized algorithms', 'deterministic algorithm', 'algorithms', 'online conflict', 'free coloring', 'online', 'cf'",4,"coloring algorithms', 'online cf coloring', 'randomized algorithms', 'free coloring', 'deterministic online cf', 'online conflict', 'deterministic algorithm', 'algorithm', 'high probability', 'online'",4,"using polylog colors', 'algorithms use log', 'present randomized algorithms', 'efficient using polylog', 'coloring algorithms use', 'colors efficient using', 'coloring algorithms', 'randomized algorithms online', 'coloring algorithm problem', 'randomized algorithms'",5
"given a set of $n$ elements, each of which is colored one of $c$ colors, we must determine an element of the plurality (most frequently occurring) color by pairwise equal/unequal color comparisons of elements. we focus on the expected number of color comparisons when the $ c^n $ colorings are equally probable. we analyze an obvious algorithm, showing that its expected performance is {$ c^2 + c - 2 / 2 c n - o(c^2) $}, with variance {$ \theta (c^2 n) $}. we present and analyze an algorithm for the case {$ c = 3 $} colors whose average complexity on the {$ 3^n $} equally probable inputs is {$ 7083 / 5425 n + o(\sqrt n) = 1.3056 \ldots {} n + o(\sqrt n) $}, substantially better than the expected complexity {$ 5 / 3 n + o(1) = 1.6666 \ldots {} n + o(1) $} of the obvious algorithm. we describe a similar algorithm for {$ c = 4 $} colors whose average complexity on the {$ 4^n $} equally probable inputs is {$ 761311 / 402850 n + o(\log n) = 1.8898 \ldots {} n + o(\log n) $}, substantially better than the expected complexity {$ 9 / 4 n + o(1) = 2.25 n + o(1) $} of the obvious algorithm.","algorithm analysis', 'majority problem', 'plurality problem'",2,"unequal color comparison', 'c$ color', 'n$ element', 'set', 'obvious algorithm', 'similar algorithm', 'probable input', 'average complexity', 'c^2', 'o(1'",4,"obvious algorithm', 'unequal color comparison', 'average complexity', 'probable input', 'pairwise equal', 'c$ color', 'n$ element', 'similar algorithm', '\\ldot', 'o(\\sqrt'",4,"n', 'color', 'algorithm', 'o(1', 'complexity', 'probable', 'ldot', 'element', 'obvious', 'comparison'",3,"unequal color comparison', 'c$ color', 'obvious algorithm', 'similar algorithm', 'n$ element', 'average complexity', 'probable input', 'pairwise equal', 'ldot', 'c^2'",5,"n o', 'c n', 'sqrt n', 'log n', 'n', 'unequal color comparisons', 'color comparisons', 'colors', 'o', 'average complexity'",3,"average complexity', 'n o', 'complexity', 'c n', 'sqrt n', 'log n', 'unequal color comparisons', 'n', 'color comparisons', 'color'",3,"expected complexity 25', 'expected complexity 6666', 'complexity 25 obvious', '25 obvious algorithm', 'showing expected performance', 'unequal color comparisons', 'expected performance variance', 'average complexity equally', 'colors average complexity', 'expected complexity'",5
"we consider an optimization problem consisting of an undirected graph, with cost and profit functions defined on all vertices. the goal is to find a connected subset of vertices with maximum total profit, whose total cost does not exceed a given budget. the best result known prior to this work guaranteed a $ (2, o(\log n)) $ bicriteria approximation, that is, the solution's profit is at least a fraction of $ 1 / o(\log n) $ of an optimum solution respecting the budget, while its cost is at most twice the given budget. we improve these results and present a bicriteria tradeoff that, given any $ \epsilon \in (0, 1 $, guarantees a $ (1 + \epsilon, o(1 / \epsilon \log n)) $-approximation.","approximation algorithms', 'bicriteria approximation'",2,"maximum total profit', 'total cost', 'profit function', 'optimization problem', 'budget', 'vertex', 'epsilon log n', 'undirected graph', 'good result', 'optimum solution'",2,"maximum total profit', '\\epsilon \\log n', 'profit function', 'good result', 'total cost', 'bicriteria approximation', 'undirected graph', 'optimization problem', 'optimum solution', '\\epsilon \\in'",4,"budget', 'n', 'cost', 'profit', 'epsilon', 'vertex', 'bicriteria', 'total', 'result', 'solution'",3,"maximum total profit', 'epsilon log n', 'total cost', 'profit function', 'epsilon in', 'good result', 'optimum solution', 'bicriteria tradeoff', 'bicriteria approximation', 'optimization problem'",5,"optimization problem', 'undirected graph', 'problem', 'maximum total profit', 'total cost', 'undirected', 'optimization', 'cost', 'log n', 'profit'",4,"optimization problem', 'undirected graph', 'problem', 'maximum total profit', 'total cost', 'undirected', 'optimization', 'cost', 'work', 'log n'",4,"maximum total profit', 'consider optimization problem', 'given epsilon guarantees', 'epsilon log approximation', 'log optimum solution', 'work guaranteed log', 'cost twice given', 'guaranteed log bicriteria', 'graph cost', 'cost profit functions'",5
"we consider the problem of embedding hyperedges of a hypergraph as paths in a cycle such that the maximum congestion, namely the maximum number of paths that use any single edge in a cycle, is minimized.\par the {\em minimum congestion hypergraph embedding in a cycle\/} problem is known to be np-hard and its graph version, the {\em minimum congestion graph embedding in a cycle}, is solvable in polynomial-time. furthermore, for the graph problem, a polynomial-time approximation scheme for the weighted version is known. for the hypergraph model, several approximation algorithms with a ratio of two have been previously published. a recent paper reduced the approximation ratio to 1.5. we present a polynomial-time approximation scheme in this article, settling the debate regarding whether the problem is polynomial-time approximable.","hypergraph embedding', 'minimum congestion', 'np-hard', 'polynomial-time approximation scheme'",3,"graph problem', 'em minimum congestion hypergraph', 'hypergraph model', 'maximum congestion', 'time approximation scheme', 'graph version', 'cycle', 'path', 'maximum number', 'polynomial'",2,"\\em minimum congestion graph', 'time approximation scheme', 'problem', 'single edge', 'maximum number', 'path', 'cycle', 'graph version', 'maximum congestion', 'polynomial'",3,"time', 'polynomial', 'cycle', 'problem', 'approximation', 'congestion', 'graph', 'em', 'hypergraph', 'minimum'",2,"em minimum congestion graph', 'time approximation scheme', 'graph problem', 'approximation ratio', 'graph version', 'approximation algorithm', 'maximum congestion', 'hypergraph model', 'time approximable', 'weighted version'",5,"graph problem', 'time approximation scheme', 'maximum congestion', 'problem', 'hypergraph model', 'cycle', 'graph version', 'hypergraph', 'maximum number', 'several approximation algorithms'",5,"several approximation algorithms', 'hypergraph model', 'graph problem', 'time approximation scheme', 'maximum congestion', 'problem', 'cycle', 'graph version', 'hypergraph', 'maximum number'",5,"problem embedding hyperedges', 'minimum congestion graph', 'minimum congestion hypergraph', 'embedding cycle problem', 'graph problem polynomial', 'known hypergraph model', 'furthermore graph problem', 'graph problem', 'embedding hyperedges hypergraph', 'hypergraph model approximation'",5
"we address the problem of approximating the distance of bounded-degree and general sparse graphs from having some predetermined graph property $p$. that is, we are interested in sublinear algorithms for estimating the fraction of edge modifications (additions or deletions) that must be performed on a graph so that it obtains $p$. this fraction is taken with respect to a given upper bound $m$ on the number of edges. in particular, for graphs with degree bound $d$ over $n$ vertices, $ m = d n $. to perform such an approximation the algorithm may ask for the degree of any vertex of its choice, and may ask for the neighbors of any vertex.\par the problem of estimating the distance to having a property was first explicitly addressed by parnas et al. 2006. in the context of graphs this problem was studied by fischer and newman 2007 in the dense graphs model. in this model the fraction of edge modifications is taken with respect to $ n^2 $, and the algorithm may ask for the existence of an edge between any pair of vertices of its choice. fischer and newman showed that every graph property that has a testing algorithm in this model, with query complexity independent of the size of the graph, also has a distance approximation algorithm with query complexity that is independent of the size of graph.\par in this work we focus on bounded-degree and general sparse graphs, and give algorithms for all properties shown to have efficient testing algorithms by goldreich and ron 2002. specifically, these properties are $k$-edge connectivity, subgraph freeness (for constant-size subgraphs), being an eulerian graph, and cycle freeness. a variant of our subgraph-freeness algorithm approximates the size of a minimum vertex cover of a graph in sublinear time. this approximation improves on a recent result of parnas and ron 2007.","distance approximation', 'graph properties', 'property testing', 'sublinear approximation algorithms'",3,"graph property', 'general sparse graph', 'eulerian graph', 'distance approximation algorithm', 'sublinear algorithm', 'freeness algorithm', 'efficient testing algorithm', 'problem', 'edge modification', 'degree'",3,"general sparse graph', 'parna et al', 'dense graphs model', 'graph property', 'edge modification', 'query complexity independent', 'distance approximation algorithm', 'minimum vertex cover', 'degree', 'efficient testing algorithm'",5,"graph', 'algorithm', 'property', 'edge', 'degree', 'vertex', 'size', 'problem', 'fraction', 'model'",3,"graph property', 'distance approximation algorithm', 'general sparse graph', 'freeness algorithm', 'efficient testing algorithm', 'eulerian graph', 'sublinear algorithm', 'minimum vertex cover', 'subgraph freeness', 'dense graphs model'",5,"general sparse graphs', 'predetermined graph property', 'graph property', 'dense graphs model', 'distance approximation algorithm', 'eulerian graph', 'graphs', 'efficient testing algorithms', 'sublinear algorithms', 'freeness algorithm'",5,"efficient testing algorithms', 'sublinear algorithms', 'algorithms', 'dense graphs model', 'testing algorithm', 'general sparse graphs', 'query complexity', 'graphs', 'distance approximation algorithm', 'edge connectivity'",4,"subgraph freeness algorithm', 'graph sublinear time', 'sublinear time approximation', 'problem approximating distance', 'minimum vertex cover', 'graph cycle freeness', 'constant size subgraphs', 'graphs degree bound', 'subgraph freeness constant', 'vertex par problem'",4
"given a set of leaf-labeled trees with identical leaf sets, the well-known maximum agreement subtree (mast) problem consists in finding a subtree homeomorphically included in all input trees and with the largest number of leaves. mast and its variant called maximum compatible tree (mct) are of particular interest in computational biology. this article presents a linear-time approximation algorithm to solve the complement version of mast, namely identifying the smallest set of leaves to remove from input trees to obtain isomorphic trees. we also present an {$ o(n^2 + k n) $} algorithm to solve the complement version of mct. for both problems, we thus achieve significantly lower running times than previously known algorithms. fast running times are especially important in phylogenetics where large collections of trees are routinely produced by resampling procedures, such as the nonparametric bootstrap or bayesian mcmc methods.","approximation algorithm', 'maximum agreement subtree', 'maximum compatible subtree', 'phylogenetic tree'",3,"identical leaf set', 'maximum compatible tree', 'input tree', 'small set', 'isomorphic tree', 'maximum agreement subtree', 'mast', 'time approximation algorithm', 'low running time', 'complement version'",3,"low running time', 'bayesian mcmc method', 'o(n^2 + k', 'time approximation algorithm', 'maximum compatible tree', 'maximum agreement subtree', 'identical leaf set', 'input tree', 'complement version', 'mast'",5,"tree', 'mast', 'algorithm', 'set', 'time', 'complement', 'leaf', 'leave', 'subtree', 'version'",2,"maximum compatible tree', 'input tree', 'time approximation algorithm', 'isomorphic tree', 'low running time', 'maximum agreement subtree', 'identical leaf set', 'bayesian mcmc method', 'o(n^2 + k', 'small set'",5,"identical leaf sets', 'maximum compatible tree', 'input trees', 'isomorphic trees', 'smallest set', 'leaves', 'maximum agreement subtree', 'trees', 'set', 'mast'",4,"input trees', 'isomorphic trees', 'maximum compatible tree', 'trees', 'algorithms', 'identical leaf sets', 'phylogenetics', 'computational biology', 'smallest set', 'leaves'",4,"lower running times', 'bayesian mcmc', 'obtain isomorphic trees', 'labeled trees', 'leaf labeled trees', 'subtree mast problem', 'maximum compatible tree', 'known algorithms', 'linear time approximation', 'called maximum compatible'",5
"pipelined filter ordering is a central problem in database query optimization. the problem is to determine the optimal order in which to apply a given set of commutative filters (predicates) to a set of elements (the tuples of a relation), so as to find, as efficiently as possible, the tuples that satisfy all of the filters. optimization of pipelined filter ordering has recently received renewed attention in the context of environments such as the web, continuous high-speed data streams, and sensor networks. pipelined filter ordering problems are also studied in areas such as fault detection and machine learning under names such as learning with attribute costs, minimum-sum set cover, and satisfying search. we present algorithms for two natural extensions of the classical pipelined filter ordering problem: (1) a {\em distributional-type\/} problem where the filters run in parallel and the goal is to maximize throughput, and (2) an {\em adversarial-type\/} problem where the goal is to minimize the expected value of {\em multiplicative regret}. we present two related algorithms for solving (1), both running in time {$ o(n^2) $}, which improve on the {$ o(n 3 \log n) $} algorithm of kodialam. we use techniques from our algorithms for (1) to obtain an algorithm for 1.","flow algorithms', 'pipelined filter ordering', 'query optimization', 'selection ordering'",3,"filter ordering problem', 'commutative filter', 'central problem', 'sum set cover', 'database query optimization', 'algorithm', 'optimal order', 'em multiplicative regret', 'em distributional', 'tuple'",4,"filter ordering problem', 'sum set cover', 'database query optimization', '\\em multiplicative regret', 'speed datum stream', 'central problem', 'optimal order', 'commutative filter', 'renewed attention', 'continuous high'",4,"filter', 'problem', 'ordering', 'algorithm', 'set', 'em', 'optimization', 'tuple', 'filter ordering', 'type'",3,"filter ordering problem', 'em multiplicative regret', 'sum set cover', 'central problem', 'commutative filter', 'database query optimization', 'speed datum stream', 'em distributional', 'em adversarial', 'algorithm'",5,"pipelined filter ordering', 'classical pipelined filter', 'commutative filters', 'filters', 'central problem', 'problem', 'pipelined', 'database query optimization', 'ordering', 'sum set cover'",5,"database query optimization', 'optimization', 'related algorithms', 'algorithms', 'pipelined filter ordering', 'web', 'classical pipelined filter', 'search', 'renewed attention', 'commutative filters'",5,"filter ordering problem', 'filter ordering problems', 'goal maximize throughput', 'adversarial type problem', 'query optimization problem', 'solving running time', 'ordering problem em', 'minimize expected value', 'database query optimization', 'ordering problem'",5
"we give a polynomial-time algorithm to find a shortest contractible cycle (i.e., a closed walk without repeated vertices) in a graph embedded in a surface. this answers a question posed by hutchinson. in contrast, we show that finding a shortest contractible cycle through a given vertex is np-hard. we also show that finding a shortest separating cycle in an embedded graph is np-hard. this answers a question posed by mohar and thomassen.","3-path condition', 'forbidden pairs', 'graphs on surfaces', 'topological graph theory'",3,"contractible cycle', 'time algorithm', 'polynomial', 'vertex', 'graph', 'closed walk', 'question', 'np', 'hard', 'hutchinson'",2,"contractible cycle', 'closed walk', 'time algorithm', 'vertex', 'graph', 'question', 'np', 'hutchinson', 'hard', 'contrast'",3,"cycle', 'hard', 'contractible', 'question', 'np', 'vertex', 'graph', 'surface', 'polynomial', 'time'",3,"contractible cycle', 'time algorithm', 'closed walk', 'vertex', 'graph', 'mohar', 'thomassen', 'hutchinson', 'question', 'np'",3,"shortest contractible cycle', 'time algorithm', 'time', 'shortest', 'polynomial', 'cycle', 'algorithm', 'contractible', 'closed walk', 'vertex'",5,"time algorithm', 'shortest contractible cycle', 'time', 'shortest', 'polynomial', 'cycle', 'algorithm', 'contractible', 'closed walk', 'vertex'",5,"algorithm shortest contractible', 'hard finding shortest', 'finding shortest contractible', 'finding shortest', 'shortest contractible cycle', 'graph np hard', 'polynomial time algorithm', 'embedded graph np', 'shortest separating cycle', 'vertex np hard'",5
"a new randomized asynchronous shared-memory data structure is given for implementing an approximate counter that can be incremented once by each of $n$ processes in a model that allows up to $ n - 1 $ crash failures. for any fixed $ \epsilon $, the counter achieves a relative error of $ \delta $ with high probability, at the cost of {$ o(((1 / \delta) \log n)^{o(1 / \epsilon)}) $} register operations per increment and {$ o(n^{4 / 5 + \epsilon }((1 / \delta) \log n)^{o(1 / \epsilon)}) $} register operations per read. the counter combines randomized sampling for estimating large values with an expander for estimating small values. this is the first counter implementation that is sublinear the number of processes and works despite a strong adversary scheduler that can observe internal states of processes.\par an application of the improved counter is an improved protocol for solving randomized shared-memory consensus, which reduces the best previously known individual work complexity from {$ o(n \log n) $} to an optimal {$ o(n) $}, resolving one of the last remaining open problems concerning consensus in this model.","approximate counting', 'consensus', 'distributed computing', 'expanders', 'martingales'",3,"approximate counter', 'improved counter', 'counter implementation', 'memory datum structure', 'new', 'epsilon', 'memory consensus', 'asynchronous', 'log n)^{o(1', 'delta'",4,"individual work complexity', 'memory datum structure', 'strong adversary scheduler', '\\log n)^{o(1', 'counter', 'high probability', '\\epsilon', 'relative error', 'model', 'crash failure'",4,"counter', 'epsilon', 'log', 'delta', 'n)^{o(1', 'memory', 'operation', 'model', 'process', 'o(n'",3,"improved counter', 'counter implementation', 'approximate counter', 'individual work complexity', 'memory datum structure', 'strong adversary scheduler', 'memory consensus', 'small value', 'large value', 'o(n log'",5,"randomized sampling', 'first counter implementation', 'memory data structure', 'approximate counter', 'log n', 'randomized', 'counter', 'n', 'memory consensus', 'new'",5,"randomized sampling', 'individual work complexity', 'memory data structure', 'memory consensus', 'first counter implementation', 'approximate counter', 'log n', 'randomized', 'high probability', 'counter'",5,"implementing approximate counter', 'fixed epsilon counter', 'implementation sublinear number', 'sublinear number processes', 'operations increment epsilon', 'complexity log optimal', 'work complexity log', 'randomized shared memory', 'cost delta log', 'epsilon counter achieves'",5
"we establish the first nontrivial lower bounds on time-space trade-offs for the selection problem. we prove that any comparison-based randomized algorithm for finding the median requires {$ \omega (n \log \log_s n) $} expected time in the ram model (or more generally in the comparison branching program model), if we have {$s$} bits of extra space besides the read-only input array. this bound is tight for all {$ s > \log n $}, and remains true even if the array is given in a random order. our result thus answers a 16-year-old question of munro and raman 1996, and also complements recent lower bounds that are restricted to sequential access, as in the multipass streaming model chakrabarti et al. 2008b.\par we also prove that any comparison-based, deterministic, multipass streaming algorithm for finding the median requires {$ \omega (n \log^*(n / s) + n \log_s n) $} worst-case time (in scanning plus comparisons), if we have {$s$} cells of space. this bound is also tight for all {$ s > \log^2 n $}. we get deterministic lower bounds for i/o-efficient algorithms as well.\par the proofs in this article are self-contained and do not rely on communication complexity techniques.","adversary arguments', 'lower bounds', 'median finding', 'ram', 'randomized algorithms', 'streaming algorithms', 'time--space trade-offs'",4,"nontrivial low bound', 'deterministic low bound', 'recent low bound', 'case time', 'space trade', 'extra space', 'comparison', 'multipass streaming algorithm', 'efficient algorithm', 'multipass streaming model'",4,"multipass streaming algorithm', 'communication complexity technique', 'deterministic low bound', 'nontrivial low bound', 'chakrabarti et al', 'multipass streaming model', 'recent low bound', 'comparison', 'time', 'selection problem'",5,"bound', 'comparison', 'time', 'space', 'model', 'algorithm', 'low', 'omega', 'median', 's$'",3,"deterministic low bound', 'recent low bound', 'nontrivial low bound', 'multipass streaming model', 'multipass streaming algorithm', 'program model', 'extra space', 'space trade', 'chakrabarti et al', 'case time'",5,"deterministic lower bounds', 'recent lower bounds', 'log s n', 'log n', 'lower', 'nontrivial', 'first', 'n', 'bounds', 'multipass streaming model'",4,"efficient algorithms', 'communication complexity techniques', 'multipass streaming model', 'multipass streaming algorithm', 'deterministic lower bounds', 'recent lower bounds', 'ram model', 'log s n', 'randomized algorithm', 'log n'",5,"lower bounds time', 'bounds efficient algorithms', 'space bound tight', 'deterministic lower bounds', 'worst case time', 'lower bounds efficient', 'bounds efficient', 'requires omega log', 'array bound tight', 'bounds time space'",5
"we describe an automata-theoretic approach for the competitive analysis of {\em online algorithms}. our approach is based on {\em weighted automata}, which assign to each input word a cost in {$ r^{\geq 0} $}. by relating the ``unbounded look ahead'' of optimal offline algorithms with nondeterminism, and relating the ``no look ahead'' of online algorithms with determinism, we are able to solve problems about the competitive ratio of online algorithms, and the memory they require, by reducing them to questions about {\em determinization\/} and {\em approximated determinization\/} of weighted automata.","formal verification', 'online algorithms', 'weighted automata'",2,"em online algorithm', 'automata', 'em determinization', 'theoretic approach', 'optimal offline algorithm', 'competitive analysis', 'competitive ratio', 'input word', 'cost', 'r^{geq'",5,"optimal offline algorithm', '\\em online algorithm', 'automata', 'input word', 'competitive analysis', 'unbounded look', 'theoretic approach', 'competitive ratio', 'r^{\\geq', 'cost'",4,"algorithm', 'em', 'online', 'automata', 'approach', 'competitive', 'look', 'determinization', 'theoretic', 'analysis'",2,"em online algorithm', 'optimal offline algorithm', 'em determinization', 'theoretic approach', 'unbounded look', 'input word', 'competitive ratio', 'competitive analysis', 'automata', 'cost'",4,"online algorithms', 'weighted automata', 'theoretic approach', 'automata', 'algorithms', 'competitive analysis', 'approach', 'competitive ratio', 'input word', 'online'",5,"online algorithms', 'algorithms', 'weighted automata', 'automata', 'theoretic approach', 'memory', 'competitive analysis', 'approach', 'competitive ratio', 'nondeterminism'",4,"determinization weighted automata', 'online algorithms determinism', 'weighted automata assign', 'weighted automata', 'algorithms nondeterminism', 'ratio online algorithms', 'offline algorithms nondeterminism', 'optimal offline algorithms', 'online algorithms', 'em weighted automata'",5
"fractional hypertree width is a hypergraph measure similar to tree width and hypertree width. its algorithmic importance comes from the fact that, as shown in previous work, constraint satisfaction problems (csp) and various problems in database theory are polynomial-time solvable if the input contains a bounded-width fractional hypertree decomposition of the hypergraph of the constraints. in this article, we show that for every fixed $ w \geq 1 $, there is a polynomial-time algorithm that, given a hypergraph {$h$} with fractional hypertree width at most {$w$}, computes a fractional hypertree decomposition of width {$ o(w^3) $} for {$h$}. this means that polynomial-time algorithms relying on bounded-width fractional hypertree decompositions no longer need to be given a decomposition explicitly in the input, since an appropriate decomposition can be computed in polynomial time. therefore, if {$h$} is a class of hypergraphs with bounded fractional hypertree width, then a csp restricted to instances whose structure is in {$h$} is polynomial-time solvable. this makes bounded fractional hypertree width the most general known hypergraph property that makes csp, boolean conjunctive queries, and conjunctive query containment polynomial-time solvable.","constraint satisfaction', 'fractional hypertree width', 'treewidth'",2,"width fractional hypertree decomposition', 'fractional hypertree width', 'hypergraph measure similar', 'hypergraph property', 'polynomial time', 'time solvable', 'time algorithm', 'conjunctive query containment polynomial', 'appropriate decomposition', 'constraint satisfaction problem'",5,"width fractional hypertree decomposition', 'fractional hypertree width', 'hypergraph measure similar', 'constraint satisfaction problem', 'time solvable', 'time algorithm', 'csp', 'algorithmic importance', 'previous work', 'database theory'",3,"hypertree', 'fractional', 'width', 'time', 'polynomial', 'hypergraph', 'decomposition', 'solvable', 'csp', 'fractional hypertree'",3,"width fractional hypertree decomposition', 'fractional hypertree width', 'conjunctive query containment polynomial', 'hypergraph measure similar', 'polynomial time', 'constraint satisfaction problem', 'appropriate decomposition', 'hypergraph property', 'boolean conjunctive query', 'time algorithm'",5,"fractional hypertree width', 'fractional hypertree decomposition', 'hypertree width', 'tree width', 'width', 'hypertree', 'fractional', 'polynomial time', 'hypergraph measure', 'time algorithms'",4,"time algorithms', 'fractional hypertree width', 'tree width', 'fractional hypertree decomposition', 'constraints', 'hypertree width', 'hypergraph measure', 'width', 'hypertree', 'fractional'",4,"fractional hypertree decompositions', 'time class hypergraphs', 'hypertree width computes', 'hypergraph property makes', 'hypertree width csp', 'hypertree width hypergraph', 'known hypergraph property', 'algorithm given hypergraph', 'fractional hypertree decomposition', 'hypertree decompositions longer'",4
"we give an {$ o(n \log^2 n) $}-time, linear-space algorithm that, given a directed planar graph with positive and negative arc-lengths, and given a node {$s$}, finds the distances from {$s$} to all nodes.","monge', 'planar graphs', 'replacement paths', 'shortest paths'",3,"o(n log^2', 'space algorithm', 'node', 's$', 'planar graph', '-time', 'linear', 'negative arc', 'positive', 'length'",3,"negative arc', 'planar graph', 'space algorithm', 'o(n \\log^2', 'node', 's$', 'distance', 'linear', 'length', 'positive'",5,"s$', 'node', '-time', 'linear', 'algorithm', 'length', 'o(n', 'log^2', 'space', 'negative'",5,"planar graph', 'o(n log^2', 'space algorithm', 'negative arc', 'node', 'distance', 'positive', 'linear', 'length', 's$'",4,"n', 'log', 'space algorithm', 'time', 'linear', 'planar graph', 'space', 'negative arc', 'algorithm', 'planar'",3,"n', 'space algorithm', 'log', 'time', 'linear', 'distances', 'planar graph', 'space', 'negative arc', 'algorithm'",3,"linear space algorithm', 'node finds distances', 'planar graph', 'directed planar graph', 'space algorithm', 'space algorithm given', 'log time linear', 'linear space', 'algorithm given directed', 'time linear space'",5
"let {$c$} be a class of labeled connected graphs, and let {$ c_n $} be a graph drawn uniformly at random from graphs in {$c$} that contain exactly {$n$} vertices. denote by {$ b(\ell; c_n) $} the number of blocks (i.e., maximal biconnected subgraphs) of {$ c_n $} that contain exactly {$ \ell $} vertices, and let {$ l b(c_n) $} be the number of vertices in a largest block of {$ c_n $}. we show that under certain general assumptions on {$c$}, {$ c_n $} belongs with high probability to one of the following categories:\par (1) {$ l b(c_n) \sim c n $}, for some explicitly given {$ c = c(c) $}, and the second largest block is of order {$ n^\alpha $}, where {$ 1 > \alpha = \alpha (c) $}, or\par (2) {$ l b(c_n) = o(\log n) $}, that is, all blocks contain at most logarithmically many vertices.\par moreover, in both cases we show that the quantity {$ b(\ell; c_n) $} is concentrated for all {$ \ell $} and we determine its expected value. as a corollary we obtain that the class of planar graphs belongs to category {$1$}. in contrast to that, outerplanar and series-parallel graphs belong to category {$1$}.","graphs with constraints', 'planar graphs', 'random structures'",3,"c$', 'connected graph', 'planar graph', 'parallel graph', 'large block', 'class', 'vertex', 'b(c_n', 'sim c', 'number'",4,"large block', 'certain general assumption', 'c$', 'class', 'high probability', 'vertex', 'connected graph', '\\sim c', 'number', 'b(c_n'",4,"graph', 'block', 'c$', 'b(c_n', 'vertex', 'ell', 'large', 'alpha', 'category', 'class'",3,"parallel graph', 'connected graph', 'planar graph', 'certain general assumption', 'large block', 'sim c', 'high probability', 'vertex', 'alpha', 'vertices.par'",5,"connected graphs', 'planar graphs', 'parallel graphs', 'graphs', 'sim c n', 'c n', 'class', 'largest block', 'l b', 'log n'",5,"connected graphs', 'planar graphs', 'parallel graphs', 'graphs', 'sim c n', 'c n', 'high probability', 'class', 'largest block', 'l b'",4,"let c_n graph', 'c_n contain exactly', 'graphs let c_n', 'vertices let c_n', 'let c_n', 'let c_n number', 'c_n number vertices', 'denote ell c_n', 'subgraphs c_n contain', 'biconnected subgraphs c_n'",4
"we present algorithms for finding optimal strategies for discounted, infinite-horizon, determinsitc markov decision processes (dmdps). our fastest algorithm has a worst-case running time of {$ o(m n) $}, improving the recent bound of {$ o(m n^2) $} obtained by andersson and vorbyov 2006. we also present a randomized {$ o(m^{1 / 2} n^2) $}-time algorithm for finding discounted all-pairs shortest paths (dapsp), improving an {$ o(m n^2) $}-time algorithm that can be obtained using ideas of papadimitriou and tsitsiklis 1987.","markov decision processes', 'minimum mean weight cycles', 'shortest paths'",4,"fast algorithm', 'o(m n^2', 'optimal strategy', 'determinsitc markov decision process', 'recent bound', 'infinite', 'horizon', 'bad', 'case', 'time'",2,"determinsitc markov decision process', 'o(m n^2', 'algorithm', 'optimal strategy', 'fast algorithm', 'recent bound', 'pair short path', 'horizon', 'time', 'infinite'",5,"o(m', 'algorithm', 'n^2', 'o(m n^2', 'dmdps', 'infinite', 'horizon', 'determinsitc', 'markov', 'decision'",4,"determinsitc markov decision process', 'fast algorithm', 'pair short path', 'o(m n^2', 'recent bound', 'optimal strategy', 'time', 'andersson', 'case', 'idea'",5,"time algorithm', 'fastest algorithm', 'algorithms', 'm n', 'optimal strategies', 'time', 'n', 'm', 'strategies', 'optimal'",3,"algorithms', 'time algorithm', 'fastest algorithm', 'optimal strategies', 'm n', 'time', 'n', 'm', 'strategies', 'optimal'",2,"randomized time algorithm', 'algorithms finding optimal', 'improving time algorithm', 'time algorithm', 'dmdps fastest algorithm', 'algorithm worst case', 'time algorithm finding', 'running time improving', 'algorithm finding discounted', 'present algorithms finding'",4
"pebbles are placed on some vertices of a directed graph. is it possible to move each pebble along at most one edge of the graph so that in the final configuration no pebble is left on its own? we give an {$ o(m n) $}-time algorithm for solving this problem, which we call the {\em 2-gathering\/} problem, where {$n$} is the number of vertices and {$m$} is the number of edges of the graph. if such a 2-gathering is not possible, the algorithm finds a solution that minimizes the number of solitary pebbles. the 2-gathering problem forms a nontrivial generalization of the nonbipartite matching problem and it is solved by extending the augmenting paths technique used to solve matching problems.","2-gatherings', 'augmenting paths', 'nonbipartite matchings'",2,"solitary pebble', 'graph', 'vertex', 'nonbipartite matching problem', 'possible', 'edge', 'number', 'algorithm', 'final configuration', 'o(m'",3,"pebble', 'graph', 'vertex', 'nonbipartite matching problem', 'possible', 'edge', 'final configuration', 'algorithm', 'nontrivial generalization', 'solitary pebble'",4,"problem', 'pebble', 'graph', 'number', 'edge', 'vertex', 'possible', 'algorithm', 'matching', 'o(m'",3,"nonbipartite matching problem', 'solitary pebble', 'path technique', 'final configuration', 'nontrivial generalization', 'algorithm', 'possible', 'graph', 'vertex', 'edge'",5,"solitary pebbles', 'gathering problem', 'pebbles', 'nonbipartite matching problem', 'matching problems', 'graph', 'number', 'problem', 'vertices', 'gathering'",5,"nonbipartite matching problem', 'solitary pebbles', 'matching problems', 'gathering problem', 'pebble', 'final configuration', 'time algorithm', 'algorithm', 'graph', 'number'",5,"nonbipartite matching problem', 'pebble edge graph', 'pebbles gathering problem', 'algorithm solving problem', 'matching problems', 'matching problem', 'time algorithm solving', 'possible pebble edge', 'matching problem solved', 'algorithm finds solution'",5
"the {\em unsplittable flow problem\/} is one of the most extensively studied optimization problems in the field of networking. an instance of it consists of an edge capacitated graph and a set of connection requests, each of which is associated with source and target vertices, a demand, and a value. the objective is to route a maximum value subset of requests subject to the edge capacities. it is a well known fact that as the capacities of the edges are larger with respect to the maximal demand among the requests, the problem can be approximated better. in particular, it is known that for sufficiently large capacities, the integrality gap of the corresponding integer linear program becomes $ 1 + \epsilon $, which can be matched by an algorithm that utilizes the randomized rounding technique.\par in this article, we focus our attention on the large capacities unsplittable flow problem in a game theoretic setting. in this setting, there are selfish agents, which control some of the requests characteristics, and may be dishonest about them. it is worth noting that in game theoretic settings many standard techniques, such as randomized rounding, violate certain monotonicity properties, which are imperative for truthfulness, and therefore cannot be employed. in light of this state of affairs, we design a monotone deterministic algorithm, which is based on a primal-dual machinery, which attains an approximation ratio of $ e / (e - 1) $, up to a disparity of $ \epsilon $ away. this implies an improvement on the current best truthful mechanism, as well as an improvement on the current best combinatorial algorithm for the problem under consideration. surprisingly, we demonstrate that any algorithm in the family of reasonable iterative path minimizing algorithms, cannot yield a better approximation ratio. consequently, it follows that in order to achieve a monotone ptas, if that exists, one would have to exert different techniques. we also consider the large capacities {\em single-minded multi-unit combinatorial auction problem}. this problem is closely related to the unsplittable flow problem since one can formulate it as a special case of the integer linear program of the unsplittable flow problem. accordingly, we obtain a comparable performance guarantee by refining the algorithm suggested for the unsplittable flow problem.","approximation algorithms', 'combinatorial and multi-unit auctions', 'mechanism design', 'primal-dual method'",3,"large capacity unsplittable flow problem', 'em unsplittable flow', 'optimization problem', 'unit combinatorial auction problem', 'em single', 'edge capacity', 'current good combinatorial algorithm', 'monotone deterministic algorithm', 'connection request', 'request subject'",4,"unsplittable flow problem', 'integer linear program', 'large capacity', '\\em', 'request', 'game theoretic', 'algorithm', 'edge', 'approximation ratio', 'demand'",3,"problem', 'flow', 'capacity', 'unsplittable', 'algorithm', 'request', 'large', 'edge', 'setting', 'value'",2,"large capacity unsplittable flow problem', 'current good combinatorial algorithm', 'unit combinatorial auction problem', 'monotone deterministic algorithm', 'optimization problem', 'edge capacity', 'current good truthful mechanism', 'well approximation ratio', 'em unsplittable flow', 'game theoretic setting'",5,"unsplittable flow problem', 'optimization problems', 'problem', 'flow', 'unsplittable', 'edge capacities', 'large capacities', 'monotone deterministic algorithm', 'connection requests', 'requests characteristics'",4,"optimization problems', 'algorithms', 'unsplittable flow problem', 'attention', 'problem', 'monotone deterministic algorithm', 'comparable performance guarantee', 'flow', 'unsplittable', 'monotone ptas'",4,"unsplittable flow problem', 'demand requests problem', 'auction problem problem', 'combinatorial auction problem', 'achieve monotone ptas', 'auction problem', 'integer linear program', 'em unsplittable flow', 'capacitated graph set', 'monotonicity properties'",5
"we introduce a facility location problem with submodular facility cost functions, and give an {$ o(\log n) $} approximation algorithm for it. then we focus on a special case of submodular costs, called hierarchical facility costs, and give a {$ (4.237 + \epsilon) $}-approximation algorithm using local search. the hierarchical facility costs model multilevel service installation. shmoys et al. 2004 gave a constant factor approximation algorithm for a two-level version of the problem. here we consider a multilevel problem, and give a constant factor approximation algorithm, independent of the number of levels, for the case of identical costs on all facilities.","approximation algorithm', 'facility location', 'local search', 'submodular function'",2,"submodular facility cost function', 'hierarchical facility cost model multilevel service installation', 'facility location problem', 'submodular cost', 'identical cost', 'multilevel problem', 'constant factor approximation algorithm', 'special case', 'level version', 'shmoy et al'",5,"facility cost model multilevel service installation', 'submodular facility cost function', 'hierarchical facility cost model multilevel service', 'facility location problem', 'constant factor approximation algorithm', 'shmoy et al', 'submodular cost', 'special case', 'local search', 'level version'",5,"cost', 'facility', 'algorithm', 'approximation', 'problem', 'hierarchical', 'submodular', 'constant', 'factor', 'case'",3,"hierarchical facility cost model multilevel service installation', 'submodular facility cost function', 'facility location problem', 'constant factor approximation algorithm', 'submodular cost', 'multilevel problem', 'identical cost', 'shmoy et al', 'level version', 'special case'",5,"hierarchical facility costs', 'facility location problem', 'submodular costs', 'identical costs', 'facilities', 'costs', 'approximation algorithm', 'multilevel problem', 'problem', 'multilevel service installation'",4,"hierarchical facility costs', 'facility location problem', 'local search', 'submodular costs', 'approximation algorithm', 'identical costs', 'facilities', 'costs', 'multilevel problem', 'problem'",4,"constant factor approximation', 'submodular facility cost', 'epsilon approximation algorithm', 'case submodular costs', 'consider multilevel problem', 'cost functions log', 'location problem submodular', 'facility location problem', 'approximation algorithm using', 'submodular costs'",5
"scheduling on unrelated machines is one of the most general and classical variants of the task scheduling problem. fractional scheduling is the lp-relaxation of the problem, which is polynomially solvable in the nonstrategic setting, and is a useful tool to design deterministic and randomized approximation algorithms.\par the mechanism design version of the scheduling problem was introduced by nisan and ronen. in this article, we consider the mechanism design version of the fractional variant of this problem. we give lower bounds for any fractional truthful mechanism. our lower bounds also hold for any (randomized) mechanism for the integral case. in the positive direction, we propose a truthful mechanism that achieves approximation 3/2 for 2 machines, matching the lower bound. this is the first new tight bound on the approximation ratio of this problem, after the tight bound of 2, for 2 machines, obtained by nisan and ronen. for $n$ machines, our mechanism achieves an approximation ratio of $ n + 1 / 2 $.\par motivated by the fact that all the known deterministic and randomized mechanisms for the problem assign each task independently from the others, we focus on an interesting subclass of allocation algorithms, the {\em task-independent\/} algorithms. we give a lower bound of $ n + 1 / 2 $, that holds for every (not only monotone) allocation algorithm that takes independent decisions. under this consideration, our truthful independent mechanism is the best that we can hope from this family of algorithms.","scheduling', 'truthful mechanisms', 'unrelated machines'",2,"task scheduling problem', 'fractional scheduling', 'unrelated machine', 'n$ machine', 'fractional truthful mechanism', 'mechanism design version', 'truthful independent mechanism', 'randomized mechanism', 'fractional variant', 'approximation ratio'",3,"mechanism design version', 'fractional truthful mechanism', 'truthful independent mechanism', 'task scheduling problem', 'low bound', 'approximation ratio', 'fractional scheduling', 'useful tool', 'nonstrategic setting', 'unrelated machine'",4,"mechanism', 'problem', 'scheduling', 'algorithm', 'machine', 'approximation', 'design', 'fractional', 'truthful', 'low'",3,"fractional truthful mechanism', 'truthful independent mechanism', 'task scheduling problem', 'mechanism design version', 'randomized mechanism', 'fractional scheduling', 'low bound', 'approximation ratio', 'em task', 'tight bound'",5,"task scheduling problem', 'scheduling problem', 'fractional scheduling', 'fractional truthful mechanism', 'mechanism design version', 'truthful independent mechanism', 'unrelated machines', 'randomized approximation algorithms', 'scheduling', 'randomized mechanisms'",5,"task scheduling problem', 'scheduling problem', 'fractional scheduling', 'scheduling', 'randomized approximation algorithms', 'allocation algorithms', 'algorithms', 'mechanism design version', 'fractional truthful mechanism', 'truthful independent mechanism'",5,"algorithms lower bound', 'problem lower bounds', 'randomized mechanisms problem', 'randomized approximation algorithms', 'new tight bound', 'approximation ratio problem', 'matching lower bound', 'monotone allocation algorithm', 'problem tight bound', 'tight bound machines'",5
"this article studies labeling schemes for the vertex connectivity function on general graphs. we consider the problem of assigning short labels to the nodes of any $n$-node graph is such a way that given the labels of any two nodes $u$ and $v$, one can decide whether $u$ and $v$ are $k$-vertex connected in {$g$}, that is, whether there exist {$k$} vertex disjoint paths connecting {$u$} and {$v$}. this article establishes an upper bound of $ k^2 \log n $ on the number of bits used in a label. the best previous upper bound for the label size of such a labeling scheme is $ 2^k \log n $.","graph algorithms', 'labeling schemes', 'vertex-connectivity'",3,"short label', 'label size', 'article study', 'labeling scheme', 'vertex connectivity function', 'vertex disjoint path', 'general graph', 'node', 'u$', 'good previous upper bound'",2,"good previous upper bound', 'vertex connectivity function', 'vertex disjoint path', 'short label', 'article study', 'general graph', 'scheme', 'node', 'u$', 'k^2 \\log'",5,"label', 'u$', 'v$', 'article', 'graph', 'vertex', 'log', 'scheme', 'upper', 'bound'",4,"good previous upper bound', 'short label', 'label size', 'vertex disjoint path', 'vertex connectivity function', 'general graph', 'article study', 'labeling scheme', 'k^2 log', 'node'",5,"vertex disjoint paths', 'vertex connectivity function', 'article studies', 'node graph', 'short labels', 'label size', 'log n', 'labeling scheme', 'article', 'vertex'",4,"vertex connectivity function', 'general graphs', 'short labels', 'vertex disjoint paths', 'label size', 'article studies', 'labels', 'node graph', 'log n', 'labeling scheme'",4,"disjoint paths connecting', 'bound label size', 'schemes vertex connectivity', 'label size labeling', 'size labeling scheme', 'graphs consider problem', 'vertex disjoint paths', 'disjoint paths', 'labeling schemes vertex', 'given labels nodes'",5
"multiple-interval graphs are a natural generalization of interval graphs where each vertex may have more then one interval associated with it. we initiate the study of optimization problems in multiple-interval graphs by considering three classical problems: minimum vertex cover, minimum dominating set, and maximum clique. we describe applications for each one of these problems, and then proceed to discuss approximation algorithms for them.\par our results can be summarized as follows: let $t$ be the number of intervals associated with each vertex in a given multiple-interval graph. for minimum vertex cover, we give a $ (2 - 1 / t) $-approximation algorithm which also works when a $t$-interval representation of our given graph is absent. following this, we give a $ t^2 $-approximation algorithm for minimum dominating set which adapts well to more general variants of the problem. we then proceed to prove that maximum clique is np-hard already for 3-interval graphs, and provide a $ t^2 - (t + 1) / 2 $-approximation algorithm for general values of $ t \geq 2 $, using bounds proven for the so-called transversal number of $t$-interval families.","$t$-interval graphs', 'approximation algorithms', 'maximum clique', 'minimum dominating set', 'minimum vertex cover', 'multiple-interval graphs'",4,"interval graph', 'multiple', 'minimum vertex cover', 'optimization problem', 'classical problem', 'minimum dominating set', 'natural generalization', 'maximum clique', '-approximation algorithm', 'study'",4,"minimum vertex cover', 'minimum dominating set', 'interval graph', 'natural generalization', 'maximum clique', 'optimization problem', 'classical problem', 'multiple', '-approximation algorithm', 't$-interval representation'",4,"interval', 'graph', 'minimum', 'vertex', 'algorithm', 'problem', 'multiple', '-approximation', 'cover', 'dominating'",3,"minimum vertex cover', 'interval graph', 'minimum dominating set', 'classical problem', 'optimization problem', 't geq', 'approximation algorithm', 'maximum clique', 'general variant', 'general value'",4,"interval graphs', 'interval representation', 'interval families', 'interval', 'minimum vertex cover', 'graphs', 'approximation algorithms', 'optimization problems', 'multiple', 'classical problems'",5,"optimization problems', 'approximation algorithms', 'interval graphs', 'interval representation', 'interval families', 'interval', 'minimum vertex cover', 'graph', 'applications', 'multiple'",4,"problems minimum vertex', 'minimum vertex cover', 'prove maximum clique', 'interval graphs provide', 'hard interval graphs', 'vertex cover approximation', 'multiple interval graphs', 'interval graphs vertex', 'maximum clique applications', 'cover approximation algorithm'",5
"we present an {$ o(\sqrt {{\rm opt}}) $}-approximation algorithm for the maximum leaf spanning arborescence problem, where opt is the number of leaves in an optimal spanning arborescence. the result is based upon an {$ o(1) $}-approximation algorithm for a special class of directed graphs called willows. incorporating the method for willow graphs as a subroutine in a local improvement algorithm gives the bound for general directed graphs.","approximation algorithms', 'arborescence', 'directed graphs', 'maximum leaf'",4,"willow graph', 'local improvement algorithm', 'arborescence problem', 'o(sqrt', 'maximum leaf', 'special class', 'result', 'o(1'",4,"local improvement algorithm', 'arborescence problem', 'willow graph', 'special class', 'maximum leaf', '-approximation algorithm', '\\rm opt', 'optimal', 'result', 'leave'",4,"algorithm', '-approximation', 'graph', 'arborescence', 'opt', 'willow', 'o(sqrt', 'rm', 'problem', 'maximum'",3,"local improvement algorithm', 'willow graph', 'rm opt', '-approximation algorithm', 'arborescence problem', 'maximum leaf', 'special class', 'optimal', 'leave', 'subroutine'",5,"approximation algorithm', 'local improvement algorithm', 'rm', 'approximation', 'algorithm', 'maximum leaf', 'arborescence problem', 'sqrt', 'willow graphs', 'arborescence'",5,"approximation algorithm', 'local improvement algorithm', 'willow graphs', 'graphs', 'rm', 'approximation', 'algorithm', 'maximum leaf', 'arborescence problem', 'sqrt'",5,"algorithm maximum leaf', 'arborescence problem opt', 'spanning arborescence problem', 'arborescence problem', 'local improvement algorithm', 'approximation algorithm', 'approximation algorithm maximum', 'opt approximation algorithm', 'number leaves optimal', 'optimal spanning arborescence'",5
"we study error confinement in distributed applications, which can be viewed as an extreme case of various fault locality notions studied in the past. error confinement means that to the external observer, only nodes that were directly hit by a fault may deviate from their specified correct behavior, and only temporarily. the externally observable behavior of all other nodes must remain impeccable, even though their internal state may be affected. error confinement is impossible if an adversary is allowed to inflict arbitrary transient faults on the system, since the faults might completely wipe out input values. we introduce a new fault-tolerance measure we call {\em agility}, which quantifies the fault tolerance of an algorithm that disseminates information against state corrupting faults.\par we then propose broadcast algorithms that guarantee error confinement with optimal agility to within a constant factor in synchronous networks. these algorithms can serve as building blocks in more general reactive systems. previous results in exploring locality in reactive systems were not error confined, or allowed a wide range of behaviors to be considered correct. our results also include a new technique that can be used to analyze the ``cow path'' problem.","distributed algorithms', 'persistence', 'self-stabilization', 'voting'",2,"error confinement', 'fault locality notion', 'fault tolerance', 'new fault', 'arbitrary transient fault', 'broadcast algorithm', 'correct behavior', 'general reactive system', 'observable behavior', 'state corrupting faults.par'",3,"error confinement', 'general reactive system', 'state corrupting faults.\\par', 'fault locality notion', 'arbitrary transient fault', 'extreme case', 'external observer', 'correct behavior', 'observable behavior', 'internal state'",4,"fault', 'error', 'confinement', 'system', 'behavior', 'algorithm', 'node', 'correct', 'reactive', 'agility'",3,"fault locality notion', 'arbitrary transient fault', 'fault tolerance', 'new fault', 'error confinement', 'general reactive system', 'state corrupting faults.par', 'correct behavior', 'broadcast algorithm', 'internal state'",5,"error confinement', 'error', 'arbitrary transient faults', 'confinement', 'fault tolerance', 'new fault', 'fault', 'extreme case', 'general reactive systems', 'reactive systems'",4,"broadcast algorithms', 'algorithms', 'fault tolerance', 'tolerance measure', 'error confinement', 'correct behavior', 'observable behavior', 'new fault', 'fault', 'error'",3,"propose broadcast algorithms', 'state corrupting faults', 'impeccable internal state', 'externally observable behavior', 'algorithms guarantee error', 'fault tolerance algorithm', 'specified correct behavior', 'locality notions', 'broadcast algorithms guarantee', 'introduce new fault'",5
"publishing data for analysis from a table containing personal records, while maintaining individual privacy, is a problem of increasing importance today. the traditional approach of deidentifying records is to remove identifying fields such as social security number, name, etc. however, recent research has shown that a large fraction of the u.s. population can be identified using nonkey attributes (called quasi-identifiers) such as date of birth, gender, and zip code. the $k$-anonymity model protects privacy via requiring that nonkey attributes that leak information are suppressed or generalized so that, for every record in the modified table, there are at least $k$-1 other records having exactly the same values for quasi-identifiers. we propose a new method for anonymizing data records, where quasi-identifiers of data records are first clustered and then cluster centers are published. to ensure privacy of the data records, we impose the constraint that each cluster must contain no fewer than a prespecified number of data records. this technique is more general since we have a much larger choice for cluster centers than $k$-anonymity. in many cases, it lets us release a lot more information without compromising privacy. we also provide constant factor approximation algorithms to come up with such a clustering. this is the first set of algorithms for the anonymization problem where the performance is independent of the anonymity parameter $k$. we further observe that a few outlier points can significantly increase the cost of anonymization. hence, we extend our algorithms to allow an $ \epsilon $ fraction of points to remain unclustered, that is, deleted from the anonymized publication. thus, by not releasing a small fraction of the database records, we can ensure that the data published for analysis has less distortion and hence is more useful. our approximation algorithms for new clustering objectives are of independent interest and could be applicable in other clustering scenarios as well.","anonymity', 'approximation algorithms', 'clustering', 'privacy'",2,"datum record', 'personal record', 'database record', 'individual privacy', 'modified table', 'analysis', 'quasi', 'cluster center', 'identifier', 'large fraction'",4,"approximation algorithm', 'datum record', 'outlier point', 'anonymity parameter', 'anonymization problem', 'cluster center', 'prespecified number', 'large choice', 'factor approximation', 'modified table'",4,"record', 'datum', 'cluster', 'privacy', 'algorithm', 'identifier', 'quasi', 'fraction', 'table', 'nonkey'",2,"datum record', 'constant factor approximation algorithm', 'personal record', 'database record', 'cluster center', 'individual privacy', 'new clustering objective', 'social security number', 'large fraction', 'anonymization problem'",5,"anonymizing data records', 'data records', 'publishing data', 'personal records', 'other records', 'database records', 'records', 'data', 'individual privacy', 'social security number'",4,"other clustering scenarios', 'new clustering objectives', 'clustering', 'individual privacy', 'privacy', 'approximation algorithms', 'algorithms', 'social security number', 'anonymizing data records', 'anonymity model'",5,"new method anonymizing', 'ensure privacy data', 'anonymizing data records', 'anonymization problem performance', 'increase cost anonymization', 'anonymized publication releasing', 'privacy problem', 'privacy data', 'set algorithms anonymization', 'algorithms allow epsilon'",4
"the combination of the buffer sizes of routers deployed in the internet, and the internet traffic itself, leads routinely to the dropping of packets. motivated by this, we are interested in the problem of maximizing the throughput of protocols that control packet networks. moreover, we are interested in a setting where different packets have different priorities (or weights), thus taking into account quality-of-service considerations.\par we first extend the competitive network throughput (cnt) model introduced by aiello et al. 2003 to the weighted packets case. we analyze the performance of online, local-control protocols by their competitive ratio, in the face of arbitrary traffic, using as a measure the total weight of the packets that arrive to their destinations, rather than being dropped en-route. we prove that on directed acyclic graphs (dags), any greedy protocol is competitive, with competitive ratio independent of the weights of the packets. here we mean by a ``greedy protocol'' a protocol that not only does not leave a resource idle unnecessarily, but also prefers packets with higher weight over those with lower weight. we give two independent upper bounds on the competitive ratio of general greedy protocols on dags. we further give lower bounds that show that our upper bounds cannot be improved (other than constant factors) in the general case. both our upper and lower bounds apply also to the unweighted case, and they improve the results given in aiello et al. 2003 for that case. we thus give tight (up to constant factors) upper and lower bounds for both the unweighted and weighted cases.\par in the course of proving our upper bounds we prove a lemma that gives upper bounds on the delivery times of packets by any greedy protocol on general dags (without buffer size considerations). we believe that this lemma may be of independent interest and may find additional applications.","buffer management', 'competitive analysis', 'competitive network throughput', 'online algorithms'",2,"weighted packet case', 'different packet', 'packet network', 'independent upper bound', 'buffer size consideration', 'general greedy protocol', 'low bound', 'control protocol', 'competitive network throughput', 'low weight'",3,"aiello et al', 'buffer size', 'competitive ratio', 'greedy protocol', 'upper bound', 'internet traffic', 'low bound', 'arbitrary traffic', 'total weight', 'ratio independent'",5,"packet', 'protocol', 'bound', 'weight', 'upper', 'competitive', 'greedy', 'case', 'low', 'dag'",2,"weighted packet case', 'independent upper bound', 'general greedy protocol', 'different packet', 'packet network', 'competitive ratio independent', 'low weight', 'competitive network throughput', 'high weight', 'total weight'",4,"buffer size considerations', 'buffer sizes', 'weighted packets case', 'general greedy protocols', 'buffer', 'internet traffic', 'packet networks', 'different packets', 'greedy protocol', 'independent upper bounds'",5,"buffer size considerations', 'buffer sizes', 'control protocols', 'internet traffic', 'acyclic graphs', 'competitive network throughput', 'internet', 'combination', 'weighted packets case', 'general greedy protocols'",5,"bounds upper bounds', 'dags lower bounds', 'problem maximizing throughput', 'bounds apply unweighted', 'maximizing throughput protocols', 'proving upper bounds', 'independent upper bounds', 'upper bounds prove', 'bounds prove lemma', 'directed acyclic graphs'",3
"in this article we study the problem of approximating the distance of a function {$ f : n^d \rightarrow r $} to monotonicity where {$ n = \{ 1, \ldots, n \} $} and {$r$} is some fully ordered range. namely, we are interested in randomized sublinear algorithms that approximate the hamming distance between a given function and the closest monotone function. we allow both an additive error, parameterized by {$ \delta $}, and a multiplicative error.\par previous work on distance approximation to monotonicity focused on the one-dimensional case and the only explicit extension to higher dimensions was with a multiplicative approximation factor exponential in the dimension {\em d}. building on goldreich et al. 2000 and dodis et al. 1999, in which there are better implicit results for the case {$ n = 2 $}, we describe a reduction from the case of functions over the {$d$}-dimensional hypercube $ n^d $ to the case of functions over the $k$-dimensional hypercube $ n^k $, where $ 1 \leq k \leq d $. the quality of estimation that this reduction provides is linear in $ \lceil d / k \rceil $ and logarithmic in the size of the range {$ |r| $} (if the range is infinite or just very large, then {$ \log |r| $} can be replaced by {$ d \log n $}). using this reduction and a known distance approximation algorithm for the one-dimensional case, we obtain a distance approximation algorithm for functions over the {$d$}-dimensional hypercube, with any range {$r$}, which has a multiplicative approximation factor of {$ o(d \log |r) $}.\par for the case of a binary range, we present algorithms for distance approximation to monotonicity of functions over one dimension, two dimensions, and the {$k$}-dimensional hypercube (for any {$ k \geq 1 $} ). applying these algorithms and the reduction described before, we obtain a variety of distance approximation algorithms for boolean functions over the {$d$}-dimensional hypercube which suggest a trade-off between quality of estimation and efficiency of computation. in particular, the multiplicative error ranges between {$ o(d) $} and {$ o(1) $}.","distance approximation', 'monotonicity', 'property testing', 'sublinear approximation algorithms'",3,"distance approximation algorithm', 'hamming distance', 'close monotone function', 'boolean function', 'multiplicative approximation factor exponential', 'randomized sublinear algorithm', 'binary range', 'd$}-dimensional hypercube', 'dimensional case', 'multiplicative error'",5,"multiplicative approximation factor', 'distance approximation algorithm', 'dimensional case', 'd$}-dimensional hypercube', 'et al', 'function', '\\leq d', 'implicit result', 'k \\rceil', '\\lceil d'",4,"function', 'distance', 'approximation', 'case', 'range', 'algorithm', 'hypercube', 'multiplicative', 'reduction', 'dimension'",3,"distance approximation algorithm', 'leq k leq d', 'multiplicative approximation factor exponential', 'multiplicative error.par previous work', 'close monotone function', 'randomized sublinear algorithm', 'binary range', 'boolean function', 'o(d log |r', 'hamming distance'",5,"distance approximation algorithm', 'distance approximation', 'multiplicative approximation factor', 'distance', 'dimensional case', 'closest monotone function', 'log n', 'dimensional hypercube', 'boolean functions', 'problem'",4,"distance approximation algorithms', 'randomized sublinear algorithms', 'algorithms', 'distance approximation', 'multiplicative approximation factor', 'distance', 'previous work', 'dimensional case', 'closest monotone function', 'log n'",4,"problem approximating distance', 'approximate hamming distance', 'distance approximation algorithm', 'distance approximation algorithms', 'approximating distance function', 'randomized sublinear algorithms', 'obtain distance approximation', 'algorithms distance approximation', 'given function closest', 'work distance approximation'",5
"quickselect with median-of-3 is largely used in practice and its behavior is fairly well understood. however, the following natural adaptive variant, which we call {\em proportion-from-3}, had not been previously analyzed: ``choose as pivot the smallest of the sample if the relative rank of the sought element is below 1/3, the largest if the relative rank is above 2/3, and the median if the relative rank is between 1/3 and 2/3.'' we first analyze the average number of comparisons made when using proportion-from-2 and then for proportion-from-3. we also analyze $ \nu $-find, a generalization of proportion-from-3 with interval breakpoints at $ \nu $ and $ 1 - \nu $. we show that there exists an optimal value of $ \nu $ and we also provide the range of values of $ \nu $ where $ \nu $-find outperforms median-of-3. then, we consider the average total cost of these strategies, which takes into account the cost of both comparisons and exchanges. our results strongly suggest that a suitable implementation of $ \nu $-find could be the method of choice in a practical setting. we also study the behavior of proportion-from-$s$ with $ s > 3 $ and in particular we show that proportion-from-$s$-like strategies are optimal when $ s \rightarrow \infty $.","analytic combinatorics', 'average-case analysis', 'divide-and-conquer', 'find', 'quickselect', 'sampling', 'selection'",3,"outperform, 'quickselect', 'from-3', 'nu', 'relative rank', 'following natural adaptive variant', 'behavior', 'average total cost'",3,"following natural adaptive variant', 'relative rank', 'proportion', 'of-3', 'from-3', 'sought element', 'behavior', '\\nu', 'average total cost', 'median'",5,"nu', 'proportion', 'from-3', 'relative', 'rank', 'median', '-find', 'of-3', 'value', 'average'",2,"-find outperform median', 'following natural adaptive variant', 'average total cost', 'em proportion', 'optimal value', 'from-$s$-like strategy', 'average number', 'interval breakpoint', 'relative rank', 'practical setting'",5,"relative rank', 'median', 'proportion', 'natural adaptive variant', 'average total cost', 'relative', 'rank', 'average number', 'behavior', 'quickselect'",3,"behavior', 'relative rank', 'median', 'proportion', 'strategies', 'natural adaptive variant', 'average total cost', 'relative', 'rank', 'average number'",3,"optimal value nu', 'number comparisons using', 'average total cost', 'results strongly suggest', 'optimal rightarrow infty', 'analyze average number', 'average number comparisons', 'exists optimal value', 'adaptive variant', 'comparisons using proportion'",5
"we consider the following simple algorithm for feedback arc set problem in weighted tournaments: order the vertices by their weighted indegrees. we show that this algorithm has an approximation guarantee of 5 if the weights satisfy {\em probability constraints\/} (for any pair of vertices $u$ and $v$, $ w_{uv} + w_{vu} = 1 $ ). special cases of the feedback arc set problem in such weighted tournaments include the feedback arc set problem in unweighted tournaments and rank aggregation. to complement the upper bound, for any constant $ \epsilon > 0 $, we exhibit an infinite family of (unweighted) tournaments for which the aforesaid algorithm ({\em irrespective\/} of how ties are broken) has an approximation ratio of $ 5 - \epsilon $.","approximation algorithms', ""borda's method"", 'feedback arc set problem', 'rank aggregation', 'tournaments'",3,"feedback arc', 'following simple algorithm', 'weighted tournament', 'aforesaid algorithm', 'weighted indegree', 'problem', 'vertex', 'approximation guarantee', 'em probability', 'approximation ratio'",4,"following simple algorithm', 'feedback arc', 'weighted tournament', 'weighted indegree', '\\em probability', 'approximation guarantee', 'weight satisfy', '+ w_{vu', 'special case', 'upper bound'",4,"tournament', 'arc', 'problem', 'feedback', 'weighted', 'algorithm', 'approximation', 'em', 'vertex', 'epsilon'",3,"weighted tournament', 'following simple algorithm', 'aforesaid algorithm', 'weighted indegree', 'em probability', 'em irrespective', 'feedback arc', 'approximation ratio', 'approximation guarantee', 'weight satisfy'",5,"simple algorithm', 'such weighted tournaments', 'aforesaid algorithm', 'weighted tournaments', 'algorithm', 'unweighted tournaments', 'weighted indegrees', 'feedback', 'tournaments', 'arc'",4,"probability constraints', 'simple algorithm', 'aforesaid algorithm', 'algorithm', 'rank aggregation', 'such weighted tournaments', 'weighted tournaments', 'unweighted tournaments', 'ties', 'weighted indegrees'",4,"problem unweighted tournaments', 'problem weighted tournaments', 'weighted indegrees algorithm', 'family unweighted tournaments', 'indegrees algorithm approximation', 'weighted tournaments', 'unweighted tournaments', 'unweighted tournaments rank', 'vertices weighted indegrees', 'weighted indegrees'",5
"given a rectangular boundary partitioned into rectangles, the minimum-length corridor (mlc-r) problem consists of finding a corridor of least total length. a corridor is a set of connected line segments, each of which must lie along the line segments that form the rectangular boundary and/or the boundary of the rectangles, and must include at least one point from the boundary of every rectangle and from the rectangular boundary. the mlc-r problem is known to be np-hard. we present the first polynomial-time constant ratio approximation algorithm for the mlc-r and mlc$_k$ problems. the mlc$_k$ problem is a generalization of the mlc-r problem where the rectangles are rectilinear $c$-gons, for $ c \leq k $ and $k$ is a constant. we also present the first polynomial-time constant ratio approximation algorithm for the group traveling salesperson problem (gtsp) for a rectangular boundary partitioned into rectilinear $c$-gons as in the mlc$_k$ problem. our algorithms are based on the restriction and relaxation approximation techniques.","approximation algorithms', 'complexity', 'computational geometry', 'corridors', 'restriction and relaxation techniques'",2,"rectangular boundary', 'r problem', 'mlc$_k$ problem', 'rectangle', 'salesperson problem', 'length corridor', 'connected line segment', 'time constant ratio approximation algorithm', 'total length', 'minimum'",3,"rectangular boundary', 'time constant ratio approximation algorithm', 'length corridor', 'connected line segment', 'total length', 'r problem', 'rectangle', 'mlc$_k$ problem', 'minimum', 'polynomial'",3,"boundary', 'problem', 'mlc', 'rectangular', 'rectangle', 'corridor', 'constant', 'mlc$_k$', 'approximation', 'algorithm'",3,"time constant ratio approximation algorithm', 'r problem', 'mlc$_k$ problem', 'salesperson problem', 'connected line segment', 'relaxation approximation technique', 'length corridor', 'rectangular boundary', 'total length', 'rectangle'",5,"rectangular boundary', 'boundary', 'r problem', 'length corridor', 'rectangular', 'salesperson problem', 'connected line segments', 'rectangle', 'least total length', 'mlc'",5,"algorithms', 'rectangular boundary', 'boundary', 'r problem', 'length corridor', 'rectangular', 'relaxation approximation techniques', 'salesperson problem', 'connected line segments', 'rectangles'",5,"traveling salesperson problem', 'rectangles rectilinear gons', 'salesperson problem gtsp', '_k problem algorithms', 'boundary partitioned rectangles', 'partitioned rectilinear gons', 'salesperson problem', 'partitioned rectangles minimum', 'problem rectangles rectilinear', 'corridor mlc problem'",3
"statistical language modeling (lm), which aims to capture the regularities in human natural language and quantify the acceptability of a given word sequence, has long been an interesting yet challenging research topic in the speech and language processing community. it also has been introduced to information retrieval (ir) problems, and provided an effective and theoretically attractive probabilistic framework for building ir systems. in this article, we propose a word topic model (wtm) to explore the co-occurrence relationship between words, as well as the long-span latent topical information, for language modeling in spoken document retrieval and transcription. the document or the search history as a whole is modeled as a composite wtm model for generating a newly observed word. the underlying characteristics and different kinds of model structures are extensively investigated, while the performance of wtm is thoroughly analyzed and verified by comparison with the well-known probabilistic latent semantic analysis (plsa) model as well as the other models. the ir experiments are performed on the tdt chinese collections (tdt-2 and tdt-3), while the large vocabulary continuous speech recognition (lvcsr) experiments are conducted on the mandarin broadcast news collected in taiwan. experimental results seem to indicate that wtm is a promising alternative to the existing models.","adaptation', 'information retrieval', 'language model', 'speech recognition', 'word topic model'",4,"statistical language modeling', 'human natural language', 'language processing community', 'word topic model', 'composite wtm model', 'model structure', 'word sequence', 'information retrieval', 'span latent topical information', 'spoken document retrieval'",4,"language modeling', 'chinese collection', 'tdt chinese', 'semantic analysis', 'ir experiment', 'model structure', 'latent semantic', 'wtm model', 'natural language', 'different kind'",4,"model', 'language', 'word', 'wtm', 'ir', 'modeling', 'long', 'topic', 'information', 'retrieval'",2,"word topic model', 'composite wtm model', 'probabilistic latent semantic analysis', 'large vocabulary continuous speech recognition', 'span latent topical information', 'model structure', 'human natural language', 'language processing community', 'statistical language modeling', 'spoken document retrieval'",5,"statistical language modeling', 'human natural language', 'language modeling', 'language processing community', 'language', 'word topic model', 'modeling', 'composite wtm model', 'latent topical information', 'information retrieval'",5,"statistical language modeling', 'language modeling', 'continuous speech recognition', 'language processing community', 'search history', 'human natural language', 'word topic model', 'chinese collections', 'language', 'composite wtm model'",5,"word topic model', 'experiments conducted mandarin', 'modeling spoken document', 'topic model', 'topic model wtm', 'language modeling spoken', 'propose word topic', 'composite wtm model', 'continuous speech recognition', 'latent semantic analysis'",5
"extractive document summarization automatically selects a number of indicative sentences, passages, or paragraphs from an original document according to a target summarization ratio, and sequences them to form a concise summary. in this article, we present a comparative study of various probabilistic ranking models for spoken document summarization, including supervised classification-based summarizers and unsupervised probabilistic generative summarizers. we also investigate the use of unsupervised summarizers to improve the performance of supervised summarizers when manual labels are not available for training the latter. a novel training data selection approach that leverages the relevance information of spoken sentences to select reliable document-summary pairs derived by the probabilistic generative summarizers is explored for training the classification-based summarizers. encouraging initial results on mandarin chinese broadcast news data are demonstrated.","extractive summarization', 'probabilistic ranking models', 'relevance information', 'spoken document summarization'",4,"extractive document summarization', 'spoken document summarization', 'original document', 'reliable document', 'target summarization ratio', 'unsupervised probabilistic generative summarizer', 'supervised summarizer', 'indicative sentence', 'spoken sentence', 'supervised classification'",5,"mandarin chinese broadcast news datum', 'novel training datum selection approach', 'unsupervised probabilistic generative summarizer', 'extractive document summarization', 'target summarization ratio', 'spoken document summarization', 'indicative sentence', 'original document', 'comparative study', 'concise summary'",5,"summarizer', 'document', 'summarization', 'probabilistic', 'generative', 'classification', 'summary', 'sentence', 'supervised', 'spoken'",2,"unsupervised probabilistic generative summarizer', 'mandarin chinese broadcast news datum', 'spoken document summarization', 'novel training datum selection approach', 'extractive document summarization', 'supervised summarizer', 'original document', 'target summarization ratio', 'reliable document', 'spoken sentence'",5,"extractive document summarization', 'document summarization', 'original document', 'reliable document', 'document', 'target summarization ratio', 'probabilistic generative summarizers', 'summarization', 'indicative sentences', 'supervised summarizers'",4,"extractive document summarization', 'supervised classification', 'document summarization', 'classification', 'target summarization ratio', 'original document', 'reliable document', 'document', 'probabilistic generative summarizers', 'relevance information'",4,"document summarization including', 'spoken document summarization', 'document summarization', 'extractive document summarization', 'summarization including supervised', 'document summarization automatically', 'target summarization', 'use unsupervised summarizers', 'target summarization ratio', 'supervised summarizers manual'",5
"spoken language translation (slt) is the research area that focuses on the translation of speech or text between two spoken languages. phrase-based and syntax-based methods represent the state-of-the-art for statistical machine translation (smt). the phrase-based method specializes in modeling local reorderings and translations of multiword expressions. the syntax-based method is enhanced by using syntactic knowledge, which can better model long word reorderings, discontinuous phrases, and syntactic structure. in this article, we leverage on the strength of these two methods and propose a strategy based on multiple hypotheses generation in a two-stage framework for spoken language translation. the hypotheses are generated in two stages, namely, decoding and regeneration. in the decoding stage, we apply state-of-the-art, phrase-based, and syntax-based methods to generate basic translation hypotheses. then in the regeneration stage, much more hypotheses that cannot be captured by the decoding algorithms are produced from the basic hypotheses. we study three regeneration methods: redecoding, n-gram expansion, and confusion network in the second stage. finally, an additional reranking pass is introduced to select the translation outputs by a linear combination of rescoring models. experimental results on the chinese-to-english iwslt-2006 challenge task of translating the transcription of spontaneous speech show that the proposed mechanism achieves significant improvements over the baseline of about 2.80 bleu-score.","hypotheses generation', 'spoken language translation', 'statistical machine translation'",3,"spoken language translation', 'basic translation hypothesis', 'statistical machine translation', 'translation output', 'regeneration method', 'discontinuous phrase', 'regeneration stage', 'syntax', 'multiple hypothesis generation', 'basic hypothesis'",4,"spoken language translation', 'statistical machine translation', 'long word reordering', 'multiple hypothesis generation', 'basic translation hypothesis', 'method', 'research area', 'phrase', 'syntax', 'multiword expression'",3,"translation', 'method', 'stage', 'phrase', 'hypothesis', 'language', 'syntax', 'spoken', 'regeneration', 'model'",3,"basic translation hypothesis', 'spoken language translation', 'statistical machine translation', 'regeneration method', 'multiple hypothesis generation', 'regeneration stage', 'basic hypothesis', 'translation output', 'stage framework', 'long word reordering'",5,"language translation', 'basic translation hypotheses', 'statistical machine translation', 'translation outputs', 'translation', 'regeneration methods', 'languages', 'basic hypotheses', 'discontinuous phrases', 'research area'",3,"decoding algorithms', 'language translation', 'basic translation hypotheses', 'statistical machine translation', 'translation outputs', 'translation', 'art', 'linear combination', 'model', 'regeneration methods'",5,"combination rescoring models', 'modeling local reorderings', 'phrase based method', 'phrase based syntax', '80 bleu score', 'generate basic translation', 'using syntactic', 'linear combination rescoring', 'rescoring models', 'long word reorderings'",5
"this article presents a new hypothesis alignment method for combining outputs of multiple machine translation (mt) systems. an indirect hidden markov model (ihmm) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. unlike traditional hmms whose parameters are trained via maximum likelihood estimation (mle), the parameters of the ihmm are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. the ihmm-based method significantly outperforms the state-of-the-art, ter-based alignment model in our experiments on nist benchmark datasets. our combined smt system using the proposed method achieved the best chinese-to-english translation result in the constrained training track of the 2008 nist open mt evaluation.","hidden markov model', 'statistical machine translation', 'system combination', 'word alignment'",3,"new hypothesis alignment method', 'alignment model', 'word semantic similarity', 'word surface similarity', 'ihmm', 'article', 'markov model', 'multiple machine translation', 'parameter', 'english translation'",3,"new hypothesis alignment method', 'multiple machine translation', 'word surface similarity', 'maximum likelihood estimation', 'word semantic similarity', 'ihmm', 'combined smt system', 'nist benchmark dataset', 'traditional hmms', 'synonym matching'",4,"ihmm', 'alignment', 'word', 'method', 'hypothesis', 'mt', 'system', 'translation', 'similarity', 'model'",3,"new hypothesis alignment method', 'word surface similarity', 'word semantic similarity', 'nist benchmark dataset', 'multiple machine translation', 'combined smt system', 'alignment model', 'maximum likelihood estimation', 'nist open', 'markov model'",5,"hypothesis alignment', 'alignment model', 'new', 'hypothesis', 'article', 'alignment', 'multiple machine translation', 'word surface similarity', 'proposed method', 'method'",4,"open mt evaluation', 'alignment model', 'hypothesis alignment', 'distortion penalty', 'word surface similarity', 'new', 'semantic similarity', 'synonym matching', 'constrained training track', 'hypothesis'",4,"based alignment model', 'alignment model', 'method significantly outperforms', 'distance based distortion', 'traditional hmms parameters', 'similarity distance', 'hypothesis alignment method', 'significantly outperforms state', 'hidden markov model', 'traditional hmms'",5
"we introduce a bilingually motivated word segmentation approach to languages where word boundaries are not orthographically marked, with application to phrase-based statistical machine translation (pb-smt). our approach is motivated from the insight that pb-smt systems can be improved by optimizing the input representation to reduce the predictive power of translation models. we firstly present an approach to optimize the existing segmentation of both source and target languages for pb-smt and demonstrate the effectiveness of this approach using a chinese--english mt task, that is, to measure the influence of the segmentation on the performance of pb-smt systems. we report a 5.44\% relative increase in bleu score and a consistent increase according to other metrics. we then generalize this method for chinese word segmentation without relying on any segmenters and show that using our segmentation pb-smt can achieve more consistent state-of-the-art performance across two domains. there are two main advantages of our approach. first of all, it is adapted to the specific translation task at hand by taking the corresponding source (target) language into account. second, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains.","alignment', 'bilingually motivated', 'phrase-based statistical machine translation', 'word segmentation'",3,"motivated word segmentation approach', 'chinese word segmentation', 'segmentation pb', 'word boundary', 'smt system', 'target language', 'specific translation task', 'statistical machine translation', 'translation model', 'consistent increase'",5,"motivated word segmentation approach', 'statistical machine translation', 'smt system', 'english mt task', '5.44\\% relative increase', 'pb', 'chinese word segmentation', 'language', 'translation model', 'predictive power'",3,"smt', 'pb', 'approach', 'segmentation', 'word', 'language', 'translation', 'system', 'target', 'domain'",2,"motivated word segmentation approach', 'chinese word segmentation', 'segmentation pb', 'specific translation task', 'statistical machine translation', '5.44% relative increase', 'english mt task', 'target language', 'consistent increase', 'translation model'",4,"chinese word segmentation', 'segmentation pb', 'word boundaries', 'word', 'segmentation', 'approach', 'target languages', 'smt systems', 'statistical machine translation', 'specific translation task'",5,"chinese word segmentation', 'segmentation pb', 'segmentation', 'predictive power', 'other metrics', 'art performance', 'word boundaries', 'word', 'segmented training data', 'performance'",4,"optimize existing segmentation', 'word segmentation approach', 'increase bleu score', 'specific translation task', 'statistical machine translation', 'segmentation pb smt', 'word segmentation relying', 'smt approach motivated', 'motivated word segmentation', 'performance domains main'",5
"the bilingual lexicon is an expensive but critical resource for multilingual applications in natural language processing. this article proposes an integrated framework for building a bilingual lexicon between the chinese and japanese languages. since the language pair chinese--japanese does not include english, which is a central language of the world, few large-scale bilingual resources between chinese and japanese have been constructed. one solution to alleviate this problem is to build a chinese--japanese bilingual lexicon through english as the pivot language. in addition to the pivotal approach, we can make use of the characteristics of chinese and japanese languages that use han characters. we incorporate a translation model obtained from a small chinese--japanese lexicon and use the similarity of the hanzi and kanji characters by using the log-linear model. our experimental results show that the use of the pivotal approach can improve the translation performance over the translation model built from a small chinese--japanese lexicon. the results also demonstrate that the similarity between the hanzi and kanji characters provides a positive effect for translating technical terms.","bilingual lexicon', 'han characters', 'hanzi', 'kanji', 'pivot language', 'statistical machine translation'",2,"japanese bilingual lexicon', 'japanese lexicon', 'scale bilingual resource', 'language pair chinese', 'japanese language', 'natural language processing', 'central language', 'pivot language', 'small chinese', 'critical resource'",4,"japanese bilingual lexicon', 'scale bilingual resource', 'language pair chinese', 'natural language processing', 'japanese language', 'pivotal approach', 'multilingual application', 'translation model', 'critical resource', 'small chinese'",4,"japanese', 'chinese', 'language', 'lexicon', 'bilingual', 'character', 'model', 'translation', 'small', 'resource'",3,"japanese bilingual lexicon', 'language pair chinese', 'japanese language', 'natural language processing', 'japanese lexicon', 'scale bilingual resource', 'central language', 'pivot language', 'small chinese', 'translation model'",5,"japanese bilingual lexicon', 'bilingual lexicon', 'japanese lexicon', 'japanese languages', 'bilingual resources', 'natural language processing', 'lexicon', 'bilingual', 'japanese', 'central language'",4,"japanese bilingual lexicon', 'bilingual lexicon', 'japanese lexicon', 'japanese languages', 'bilingual resources', 'multilingual applications', 'natural language processing', 'translation model', 'lexicon', 'bilingual'",5,"improve translation performance', 'bilingual resources chinese', 'building bilingual lexicon', 'lexicon chinese japanese', 'language pair chinese', 'kanji characters using', 'resources chinese japanese', 'use han characters', 'chinese japanese bilingual', 'han characters incorporate'",5
"dependency parsing has become increasingly popular for a surge of interest lately for applications such as machine translation and question answering. currently, several supervised learning methods can be used for training high-performance dependency parsers if sufficient labeled data are available.\par however, currently used statistical dependency parsers provide poor results for words separated by long distances. in order to solve this problem, this article presents an effective dependency parsing approach of incorporating short dependency information from unlabeled data. the unlabeled data is automatically parsed by using a deterministic dependency parser, which exhibits a relatively high performance for short dependencies between words. we then train another parser that uses the information on short dependency relations extracted from the output of the first parser. the proposed approach achieves an unlabeled attachment score of 86.52\%, an absolute 1.24\% improvement over the baseline system on the chinese treebank data set. the results indicate that the proposed approach improves the parsing performance for longer distance words.","chinese dependency parsing', 'semi-supervised learning', 'unlabeled data'",2,"performance dependency parser', 'statistical dependency parser', 'deterministic dependency parser', 'short dependency information', 'dependency parsing', 'short dependency relation', 'effective dependency', 'unlabeled datum', 'chinese treebank datum', 'high performance'",4,"long distance word', 'chinese treebank datum', 'unlabeled attachment score', 'deterministic dependency parser', 'short dependency relation', 'statistical dependency parser', 'short dependency information', 'performance dependency parser', 'supervised learning method', 'unlabeled datum'",4,"dependency', 'parser', 'datum', 'performance', 'word', 'unlabeled', 'short', 'approach', 'distance', 'high'",3,"performance dependency parser', 'deterministic dependency parser', 'statistical dependency parser', 'short dependency information', 'short dependency relation', 'dependency parsing', 'effective dependency', 'unlabeled datum', 'long distance word', 'chinese treebank datum'",5,"performance dependency parsers', 'statistical dependency parsers', 'deterministic dependency parser', 'short dependency information', 'short dependency relations', 'short dependencies', 'effective dependency', 'dependency', 'first parser', 'machine translation'",5,"performance dependency parsers', 'statistical dependency parsers', 'deterministic dependency parser', 'short dependency information', 'short dependency relations', 'short dependencies', 'effective dependency', 'dependency', 'high performance', 'chinese treebank data'",5,"improves parsing performance', 'effective dependency parsing', 'performance dependency parsers', 'dependency parsing approach', 'dependency parsers provide', 'statistical dependency parsers', 'parsers provide poor', 'parsing performance', 'deterministic dependency parser', 'parsing approach incorporating'",4
"in some thai documents, a single text line of a printed document page may contain words of both thai and roman scripts. for the optical character recognition (ocr) of such a document page it is better to identify, at first, thai and roman script portions and then to use individual ocr systems of the respective scripts on these identified portions. in this article, an svm-based method is proposed for identification of word-wise printed roman and thai scripts from a single line of a document page. here, at first, the document is segmented into lines and then lines are segmented into character groups (words). in the proposed scheme, we identify the script of a character group combining different character features obtained from structural shape, profile behavior, component overlapping information, topological properties, and water reservoir concept, etc. based on the experiment on 10,000 data (words) we obtained 99.62\% script identification accuracy from the proposed scheme.","multi-script ocr', 'script identification', 'svm', 'thai script'",2,"thai document', 'thai script', 'document page', 'roman script portion', 'single text line', 'single line', 'respective script', '99.62% script identification accuracy', 'optical character recognition', 'word'",2,"99.62\\% script identification accuracy', 'individual ocr system', 'water reservoir concept', 'different character feature', 'document page', 'roman script portion', 'optical character recognition', 'single text line', 'word', 'thai'",3,"script', 'document', 'thai', 'word', 'line', 'character', 'page', 'roman', 'single', 'scheme'",3,"99.62% script identification accuracy', 'roman script portion', 'thai script', 'different character feature', 'single text line', 'optical character recognition', 'thai document', 'respective script', 'character group', 'document page'",5,"thai documents', 'thai scripts', 'roman script portions', 'document page', 'single text line', 'roman scripts', 'single line', 'document', 'respective scripts', 'thai'",4,"optical character recognition', 'thai documents', 'thai scripts', 'roman script portions', 'document page', 'single text line', 'roman scripts', 'profile behavior', 'single line', 'document'",5,"use individual ocr', 'optical character recognition', 'segmented character groups', 'individual ocr systems', 'single line document', 'character recognition', 'character features obtained', 'different character features', 'identify script character', 'document segmented lines'",5
"web search clustering is a solution to reorganize search results (also called ``snippets'') in a more convenient way for browsing. there are three key requirements for such post-retrieval clustering systems: (1) the clustering algorithm should group similar documents together; (2) clusters should be labeled with descriptive phrases; and (3) the clustering system should provide high-quality clustering without downloading the whole web page.\par this article introduces a novel framework for clustering web search results in vietnamese which targets the three above issues. the main motivation is that by enriching short snippets with hidden topics from huge resources of documents on the internet, it is able to cluster and label such snippets effectively in a topic-oriented manner without concerning whole web pages. our approach is based on recent successful topic analysis models, such as probabilistic-latent semantic analysis, or latent dirichlet allocation. the underlying idea of the framework is that we collect a very large external data collection called ``universal dataset,'' and then build a clustering system on both the original snippets and a rich set of hidden topics discovered from the universal data collection. this can be seen as a richer representation of snippets to be clustered. we carry out careful evaluation of our method and show that our method can yield impressive clustering quality.","cluster labeling', 'collocation', 'hidden topics analysis', 'hierarchical agglomerative clustering', 'latent dirichlet allocation', 'vietnamese', 'web search clustering'",3,"web search clustering', 'web search result', 'web page.par', 'impressive clustering quality', 'quality clustering', 'short snippet', 'original snippet', 'system', 'recent successful topic analysis model', 'hidden topic'",3,"recent successful topic', 'web search clustering', 'web search result', 'datum collection', 'snippet', 'key requirement', 'convenient way', 'system', 'huge resource', 'similar document'",3,"cluster', 'snippet', 'web', 'topic', 'search', 'system', 'clustering', 'result', 'document', 'quality'",2,"recent successful topic analysis model', 'web search clustering', 'large external datum collection', 'web search result', 'short snippet', 'original snippet', 'latent semantic analysis', 'universal datum collection', 'impressive clustering quality', 'hidden topic'",5,"clustering web search', 'web search clustering', 'whole web page', 'clustering system', 'impressive clustering quality', 'quality clustering', 'clustering algorithm', 'clustering', 'search results', 'web'",4,"web search clustering', 'clustering web search', 'clustering algorithm', 'clustering system', 'impressive clustering quality', 'quality clustering', 'careful evaluation', 'whole web pages', 'search results', 'clustering'",4,"clustering original snippets', 'novel framework clustering', 'clustering provide high', 'retrieval clustering systems', 'reorganize search results', 'clustering web search', 'framework clustering web', 'cluster label snippets', 'label snippets', 'topic analysis models'",5
"the arabic language presents researchers and developers of natural language processing (nlp) applications for arabic text and speech with serious challenges. the purpose of this article is to describe some of these challenges and to present some solutions that would guide current and future practitioners in the field of arabic natural language processing (anlp). we begin with general features of the arabic language in sections 1, 2, and 3 and then we move to more specific properties of the language in the rest of the article. in section 1 of this article we highlight the significance of the arabic language today and describe its general properties. section 2 presents the feature of arabic diglossia showing how the sociolinguistic aspects of the arabic language differ from other languages. the stability of arabic diglossia and its implications for anlp applications are discussed and ways to deal with this problematic property are proposed. section 3 deals with the properties of the arabic script and the explosion of ambiguity that results from the absence of short vowel representations and overt case markers in contemporary arabic texts. we present in section 4 specific features of the arabic language such as the nonconcatenative property of arabic morphology, arabic as an agglutinative language, arabic as a pro-drop language, and the challenge these properties pose to anlp. we also present solutions that have already been adopted by some pioneering researchers in the field. in section 5 we point out to the lack of formal and explicit grammars of modern standard arabic which impedes the progress of more advanced anlp systems. in section 6 we draw our conclusion.","arabic dialects', 'arabic script', 'modern standard arabic'",2,"arabic natural language processing', 'arabic language today', 'contemporary arabic text', 'arabic diglossia', 'modern standard arabic', 'arabic script', 'arabic morphology', 'agglutinative language', 'drop language', 'section'",3,"arabic language', 'arabic natural language processing', 'arabic language today', 'arabic text', 'contemporary arabic text', 'overt case marker', 'future practitioner', 'arabic diglossia', 'section', 'short vowel representation'",4,"arabic', 'language', 'section', 'property', 'anlp', 'article', 'challenge', 'feature', 'processing', 'natural'",3,"arabic natural language processing', 'arabic language today', 'contemporary arabic text', 'modern standard arabic', 'arabic diglossia', 'drop language', 'agglutinative language', 'arabic script', 'arabic morphology', 'specific property'",5,"arabic language today', 'arabic language', 'contemporary arabic texts', 'natural language processing', 'arabic text', 'arabic diglossia', 'arabic morphology', 'arabic script', 'arabic', 'other languages'",4,"arabic morphology', 'arabic language today', 'arabic language', 'contemporary arabic texts', 'natural language processing', 'arabic text', 'arabic diglossia', 'arabic script', 'arabic', 'other languages'",5,"showing sociolinguistic aspects', 'sociolinguistic aspects arabic', 'natural language processing', 'aspects arabic language', 'nlp applications arabic', 'arabic language sections', 'sociolinguistic aspects', 'processing nlp applications', 'arabic language today', 'arabic language presents'",5
"a design for an arabic-to-english translation system is presented. the core of the system implements a standard phrase-based statistical machine translation architecture, but it is extended by incorporating a local discriminative phrase selection model to address the semantic ambiguity of arabic. local classifiers are trained using linguistic information and context to translate a phrase, and this significantly increases the accuracy in phrase selection with respect to the most frequent translation traditionally considered. these classifiers are integrated into the translation system so that the global task gets benefits from the discriminative learning. as a result, we obtain significant improvements in the full translation task at the lexical, syntactic, and semantic levels as measured by an heterogeneous set of automatic evaluation metrics.","arabic', 'discriminative learning', 'english', 'statistical machine translation'",2,"english translation system', 'statistical machine translation architecture', 'local discriminative phrase selection model', 'translation task', 'frequent translation', 'standard phrase', 'arabic', 'design', 'local classifier', 'discriminative learning'",3,"local discriminative phrase selection model', 'statistical machine translation architecture', 'english translation system', 'standard phrase', 'linguistic information', 'local classifier', 'semantic ambiguity', 'frequent translation', 'arabic', 'global task'",4,"translation', 'phrase', 'system', 'arabic', 'classifier', 'local', 'selection', 'discriminative', 'semantic', 'task'",2,"local discriminative phrase selection model', 'statistical machine translation architecture', 'english translation system', 'translation task', 'frequent translation', 'standard phrase', 'automatic evaluation metric', 'local classifier', 'global task', 'semantic level'",4,"english translation system', 'translation system', 'full translation task', 'frequent translation', 'arabic', 'translation', 'phrase selection', 'local classifiers', 'standard phrase', 'system'",3,"automatic evaluation metrics', 'english translation system', 'design', 'translation system', 'full translation task', 'context', 'frequent translation', 'arabic', 'discriminative learning', 'translation'",4,"syntactic semantic levels', 'semantic ambiguity arabic', 'phrase selection model', 'classifiers trained using', 'local classifiers', 'semantic levels', 'frequent translation traditionally', 'incorporating local discriminative', 'local discriminative phrase', 'automatic evaluation'",5
"the arabic language has a very rich/complex morphology. each arabic word is composed of zero or more {\em prefixes}, one {\em stem\/} and zero or more {\em suffixes}. consequently, the arabic data is sparse compared to other languages such as english, and it is necessary to conduct word segmentation before any natural language processing task. therefore, the word-segmentation step is worth a deeper study since it is a preprocessing step which shall have a significant impact on all the steps coming afterward. in this article, we present an arabic mention detection system that has very competitive results in the recent automatic content extraction (ace) evaluation campaign. we investigate the impact of different segmentation schemes on arabic mention detection systems and we show how these systems may benefit from more than one segmentation scheme. we report the performance of several mention detection models using different kinds of possible and known segmentation schemes for arabic text: punctuation separation, arabic treebank, and morphological and character-level segmentations. we show that the combination of competitive segmentation styles leads to a better performance. results indicate a statistically significant improvement when arabic treebank and morphological segmentations are combined.","arabic information extraction', 'arabic mention detection', 'arabic segmentation'",2,"arabic language', 'arabic word', 'arabic mention detection system', 'arabic datum', 'arabic treebank', 'arabic text', 'word segmentation', 'natural language processing task', 'segmentation step', 'different segmentation scheme'",3,"arabic mention detection system', 'natural language processing task', 'automatic content extraction', 'different segmentation scheme', 'recent automatic content', 'mention detection model', 'significant impact', 'arabic datum', 'preprocessing step', '\\em prefix'",5,"arabic', 'segmentation', 'em', 'word', 'mention', 'detection', 'scheme', 'language', 'system', 'step'",2,"arabic mention detection system', 'different segmentation scheme', 'competitive segmentation style', 'arabic language', 'segmentation step', 'word segmentation', 'arabic word', 'arabic treebank', 'level segmentation', 'morphological segmentation'",5,"arabic language', 'arabic word', 'arabic treebank', 'arabic data', 'arabic text', 'arabic', 'different segmentation schemes', 'word segmentation', 'known segmentation schemes', 'segmentation scheme'",4,"evaluation campaign', 'different segmentation schemes', 'word segmentation', 'known segmentation schemes', 'segmentation scheme', 'segmentation step', 'competitive segmentation styles', 'arabic language', 'complex morphology', 'arabic word'",5,"known segmentation schemes', 'character level segmentations', 'treebank morphological segmentations', 'morphological segmentations', 'em prefixes em', 'em prefixes', 'automatic content extraction', 'competitive segmentation', 'competitive segmentation styles', 'detection models using'",4
"in the last two decades, significant effort has been put into annotating linguistic resources in several languages. despite this valiant effort, there are still many languages left that have only small amounts of such resources. the goal of this article is to present and investigate a method of propagating information (specifically mention detection) from a resource-rich language into a relatively resource-poor language such as arabic. part of the investigation is to quantify the contribution of propagating information in different conditions based on the availability of resources in the target language. experiments on the language pair arabic-english show that one can achieve relatively decent performance by propagating information from a language with richer resources such as english into arabic alone (no resources or models in the source language arabic). furthermore, results show that propagated features from english do help improve the arabic system performance even when used in conjunction with all feature types built from the source language. experiments also show that using propagated features in conjunction with lexically derived features only (as can be obtained directly from a mention annotated corpus) brings the system performance at the one obtained in the target language by using feature derived from many linguistic resources, therefore improving the system when such resources are not available. in addition to arabic-english language pair, we investigate the effectiveness of our approach on other language pairs such as chinese--english and spanish--english.","arabic information extraction', 'arabic mention detection'",2,"language pair arabic', 'source language arabic', 'english language pair', 'rich language', 'target language', 'poor language', 'linguistic resource', 'rich resource', 'arabic system performance', 'valiant effort'",3,"linguistic resource', 'english language pair', 'arabic system performance', 'target language', 'source language arabic', 'language pair arabic', 'valiant effort', 'small amount', 'poor language', 'rich language'",3,"language', 'resource', 'arabic', 'english', 'feature', 'information', 'pair', 'system', 'performance', 'effort'",2,"language pair arabic', 'english language pair', 'source language arabic', 'rich language', 'arabic system performance', 'target language', 'poor language', 'rich resource', 'linguistic resource', 'feature type'",4,"english language pair', 'many linguistic resources', 'significant effort', 'many languages', 'other language pairs', 'language pair', 'such resources', 'linguistic resources', 'target language', 'decades'",5,"english language pair', 'many linguistic resources', 'arabic system performance', 'significant effort', 'many languages', 'other language pairs', 'language pair', 'such resources', 'linguistic resources', 'target language'",5,"improve arabic performance', 'english spanish english', 'chinese english spanish', 'language richer resources', 'obtained target language', 'language experiments using', 'english spanish', 'lexically derived features', 'target language using', 'language experiments language'",5
"the arabic language presents a number of challenges for speech recognition, arising in part from the significant differences in the spoken and written forms, in particular the conventional form of texts being non-vowelized. being a highly inflected language, the arabic language has a very large lexical variety and typically with several possible (generally semantically linked) vowelizations for each written form. this article summarizes research carried out over the last few years on speech-to-text transcription of broadcast data in arabic. the initial research was oriented toward processing of broadcast news data in modern standard arabic, and has since been extended to address a larger variety of broadcast data, which as a consequence results in the need to also be able to handle dialectal speech. while standard techniques in speech recognition have been shown to apply well to the arabic language, taking into account language specificities help to significantly improve system performance.","arabic language processing', 'automatic speech recognition', 'mophological decomposition', 'speech processing', 'speech-to-text transcription'",3,"arabic language', 'modern standard arabic', 'inflected language', 'account language specificity', 'speech recognition', 'dialectal speech', 'broadcast news datum', 'broadcast datum', 'conventional form', 'large lexical variety'",4,"arabic language', 'speech recognition', 'account language specificity', 'modern standard arabic', 'significant difference', 'conventional form', 'large lexical variety', 'broadcast news datum', 'inflected language', 'broadcast datum'",4,"arabic', 'language', 'speech', 'form', 'broadcast', 'datum', 'large', 'text', 'recognition', 'variety'",3,"arabic language', 'account language specificity', 'modern standard arabic', 'large lexical variety', 'speech recognition', 'broadcast news datum', 'dialectal speech', 'inflected language', 'large variety', 'broadcast datum'",5,"arabic language', 'account language specificities', 'inflected language', 'language', 'arabic', 'speech recognition', 'dialectal speech', 'broadcast news data', 'speech', 'broadcast data'",4,"speech recognition', 'arabic language', 'account language specificities', 'broadcast news data', 'inflected language', 'broadcast data', 'language', 'arabic', 'system performance', 'dialectal speech'",4,"handle dialectal speech', 'standard techniques speech', 'speech recognition', 'dialectal speech standard', 'significantly improve performance', 'techniques speech recognition', 'speech standard techniques', 'arabic extended address', 'number challenges speech', 'dialectal speech'",5
"thabet 2005 applied cluster analysis to the qur'an in the hope of generating a classification of the (suras) that is useful for understanding of its thematic structure. the result was positive, but variation in (sura) length was a problem because clustering of the shorter was found to be unreliable. the present discussion addresses this problem in four parts. the first part summarizes thabet's work. the second part argues that unreliable clustering of the shorter is a consequence of poor estimation of lexical population probabilities in those. the third part proposes a solution to the problem based on calculation of a minimum length threshold using concepts from statistical sampling theory followed by selection of and lexical variables based on that threshold. the fourth part applies the proposed solution to a reanalysis of the qur'an.","arabic natural language processing', 'cluster analysis', 'document length normalization', 'lexical probability estimation', ""qur'an sampling""",4,"thabet', 'problem', 'cluster analysis', 'minimum length threshold', 'unreliable clustering', 'lexical population probability', 'short', ""qur'an"", 'lexical variable', 'result'",4,"statistical sampling theory', 'minimum length threshold', 'lexical population probability', 'thabet', ""qur'an"", 'thematic structure', 'cluster analysis', 'present discussion', 'poor estimation', 'unreliable clustering'",5,"problem', 'cluster', 'thabet', 'short', ""qur'an"", 'unreliable', 'length', 'solution', 'lexical', 'threshold'",4,"minimum length threshold', 'statistical sampling theory', 'part', 'lexical population probability', 'lexical variable', 'cluster analysis', 'unreliable clustering', 'problem', 'present discussion', 'thematic structure'",5,"cluster analysis', 'second part', 'first part', 'third part', 'fourth part', 'part', 'thabet', 'cluster', 'qur', 'analysis'",4,"unreliable clustering', 'clustering', 'statistical sampling theory', 'classification', 'cluster analysis', 'second part', 'first part', 'work', 'third part', 'fourth part'",4,"problem based calculation', 'unreliable clustering', 'applied cluster analysis', 'cluster analysis qur', 'generating classification', 'variation sura length', '2005 applied cluster', 'proposes solution problem', 'analysis qur hope', 'understanding thematic structure'",5
"the world wide web has been considered one of the important sources for information. using search engines to retrieve web pages can gather lots of information, including foreign information. however, to be better understood by local readers, proper names in a foreign language, such as english, are often transliterated to a local language such as chinese. due to different translators and the lack of translation standard, translating foreign proper nouns may result in different transliterations and pose a notorious headache. in particular, it may cause incomplete search results. using one transliteration as a query keyword will fail to retrieve the web pages which use a different word as the transliteration. consequently, important information may be missed. we present a framework for mining synonymous transliterations as many as possible from the web for a given transliteration. the results can be used to construct a database of synonymous transliterations which can be utilized for query expansion so as to alleviate the incomplete search problem. experimental results show that the proposed framework can effectively retrieve the set of snippets which may contain synonymous transliterations and then extract the target terms. most of the extracted synonymous transliterations have higher rank of similarity to the input transliteration compared to other noise terms.","chinese transliteration', 'cross-lingual information retrieval', 'synonymous transliteration', 'text mining', 'web mining'",3,"world wide web', 'web page', 'foreign information', 'different transliteration', 'important information', 'synonymous transliteration', 'input transliteration', 'incomplete search result', 'foreign proper noun', 'foreign language'",5,"incomplete search problem', 'incomplete search result', 'world wide web', 'foreign proper noun', 'web page', 'synonymous transliteration', 'foreign language', 'foreign information', 'proper name', 'translation standard'",4,"transliteration', 'information', 'web', 'result', 'synonymous', 'foreign', 'search', 'different', 'language', 'term'",4,"different transliteration', 'synonymous transliteration', 'incomplete search result', 'input transliteration', 'foreign proper noun', 'foreign information', 'world wide web', 'incomplete search problem', 'important information', 'foreign language'",5,"wide web', 'web pages', 'different transliterations', 'synonymous transliterations', 'web', 'important information', 'foreign information', 'wide', 'input transliteration', 'incomplete search results'",4,"wide web', 'web pages', 'web', 'incomplete search results', 'incomplete search problem', 'search engines', 'other noise terms', 'different transliterations', 'synonymous transliterations', 'important information'",4,"using transliteration query', 'different word transliteration', 'transliteration query keyword', 'transliterations extract target', 'transliteration query', 'extract target terms', 'results using transliteration', 'web given transliteration', 'synonymous transliterations extract', 'extracted synonymous transliterations'",5
"this article presents a pipeline framework for identifying soundbite and its speaker name from mandarin broadcast news transcripts. both of the two modules, soundbite segment detection and soundbite speaker name recognition, are based on a supervised classification approach using multiple linguistic features. we systematically evaluated performance for each module as well as the entire system, and investigated the effect of using speech recognition (asr) output and automatic sentence segmentation. we found that both of the two components impact the pipeline system, with more degradation in the entire system performance due to automatic speaker name recognition errors than soundbite segment detection. in addition, our experimental results show that using asr output degrades the system performance significantly, and that using automatic sentence segmentation greatly impacts soundbite detection, but has much less effect on speaker name recognition.","automatic speech recognition', 'sentence segmentation', 'soundbite detection', 'speaker name recognition'",4,"soundbite speaker', 'soundbite segment detection', 'soundbite detection', 'pipeline system', 'automatic speaker', 'pipeline framework', 'entire system performance', 'speech recognition', 'recognition error', 'article'",3,"automatic speaker name recognition error', 'soundbite speaker name recognition', 'soundbite segment detection', 'mandarin broadcast news transcript', 'automatic sentence segmentation', 'multiple linguistic feature', 'supervised classification approach', 'entire system performance', 'pipeline framework', 'speech recognition'",5,"soundbite', 'recognition', 'speaker', 'system', 'detection', 'automatic', 'performance', 'segment', 'segmentation', 'entire'",4,"soundbite segment detection', 'soundbite speaker', 'entire system performance', 'soundbite detection', 'mandarin broadcast news transcript', 'automatic sentence segmentation', 'automatic speaker', 'pipeline system', 'recognition error', 'speech recognition'",5,"pipeline system', 'speaker name recognition', 'soundbite segment detection', 'pipeline framework', 'impacts soundbite detection', 'entire system performance', 'speaker name', 'pipeline', 'system performance', 'entire system'",5,"automatic sentence segmentation', 'speaker name recognition', 'supervised classification approach', 'speech recognition', 'entire system performance', 'system performance', 'pipeline system', 'soundbite segment detection', 'pipeline framework', 'impacts soundbite detection'",5,"using speech recognition', 'automatic speaker recognition', 'automatic sentence segmentation', 'soundbite speaker recognition', 'evaluated performance module', 'speech recognition', 'evaluated performance', 'sentence segmentation', 'identifying soundbite speaker', 'soundbite segment detection'",5
,,2.85,,3.71,,4.2,,2.95,,4.77,,4.41,,4.4,,4.61