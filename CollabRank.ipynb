{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "from KeyEval import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textrank (no docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PKE(\"singlerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 22)\n",
      "2019    33\n",
      "Name: year, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:43<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.013756163622473784, 0.010514033616800415, 0.02121212121212121, 0.17224910293059145] [0.022815776052984127, 0.02463959651459651, 0.021875, 0.19686227545602536]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_scores, all_scores_adjust = eval_file(\"Datasets/DataFiles/bib_tug_dataset_full.parquet\", model,model_param = [\"\"] ,year1=2019, year2=2019, types=[\"compsci\"] , log=True)\n",
    "print(all_scores, all_scores_adjust )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                                | 1/33 [00:00<00:05,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 22)\n",
      "2019    33\n",
      "Name: year, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:04<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08293932691552149, 0.06150149364240514, 0.13333333333333333, 0.32405777877451225] [0.13848805603590483, 0.15122949966699964, 0.1375, 0.5371264730639731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_scores, all_scores_adjust = eval_file(\"Datasets/DataFiles/bib_tug_dataset_full.parquet\", Textacy(\"scake\"),model_param = [\"\"] ,year1=2019, year2=2019, types=[\"compsci\"] , log=True)\n",
    "print(all_scores, all_scores_adjust )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CollabRank using clustering (kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 22)\n",
      "2009    19\n",
      "1992    16\n",
      "1993    16\n",
      "2010    16\n",
      "2008    14\n",
      "1991    13\n",
      "1994    12\n",
      "1995    10\n",
      "2007     9\n",
      "1990     9\n",
      "1978     8\n",
      "2006     7\n",
      "1975     7\n",
      "1985     7\n",
      "1989     7\n",
      "2005     6\n",
      "1988     6\n",
      "1983     5\n",
      "1986     5\n",
      "1980     4\n",
      "1999     4\n",
      "1976     3\n",
      "1984     3\n",
      "1997     3\n",
      "2013     3\n",
      "2011     3\n",
      "1996     3\n",
      "1973     3\n",
      "2003     3\n",
      "2004     3\n",
      "2015     2\n",
      "2012     2\n",
      "1987     2\n",
      "1971     2\n",
      "1981     2\n",
      "1974     1\n",
      "1977     1\n",
      "1972     1\n",
      "1970     1\n",
      "2018     1\n",
      "1979     1\n",
      "1982     1\n",
      "2016     1\n",
      "2000     1\n",
      "2001     1\n",
      "2002     1\n",
      "2014     1\n",
      "1969     1\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "keywords_gold, abstracts = get_key_abs(\"Datasets/DataFiles/bib_tug_dataset_full.parquet\",count = 250, rand =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs =abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=5, n_init=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 2, 4, 1, 4, 0, 3, 2, 0, 2, 1, 3, 2, 0, 4, 0, 4,\n",
       "       2, 0, 3, 3, 4, 3, 3, 3, 1, 4, 0, 2, 4, 2, 1, 3, 4, 4, 4, 1, 2, 2,\n",
       "       3, 0, 0, 4, 1, 0, 0, 3, 0, 0, 3, 3, 0, 0, 2, 0, 1, 3, 4, 2, 4, 1,\n",
       "       4, 2, 4, 2, 1, 2, 3, 2, 1, 4, 1, 3, 2, 4, 4, 0, 1, 0, 4, 0, 0, 4,\n",
       "       2, 2, 0, 2, 4, 2, 1, 2, 3, 4, 3, 0, 2, 4, 2, 0, 4, 2, 2, 4, 4, 2,\n",
       "       3, 2, 2, 0, 0, 4, 0, 3, 4, 0, 3, 0, 3, 2, 4, 2, 0, 3, 1, 3, 4, 0,\n",
       "       4, 4, 2, 0, 4, 0, 4, 3, 1, 1, 4, 0, 3, 4, 2, 4, 4, 2, 2, 4, 1, 2,\n",
       "       0, 2, 0, 3, 1, 3, 3, 2, 3, 4, 0, 4, 4, 1, 3, 3, 4, 1, 3, 0, 0, 2,\n",
       "       2, 2, 0, 2, 2, 2, 1, 2, 2, 4, 2, 4, 1, 0, 2, 1, 4, 2, 4, 4, 4, 3,\n",
       "       3, 2, 4, 3, 2, 1, 4, 2, 0, 0, 2, 2, 1, 1, 0, 2, 2, 2, 2, 0, 0, 4,\n",
       "       2, 1, 3, 1, 2, 0, 4, 4, 0, 1, 0, 1, 0, 0, 3, 2, 4, 4, 4, 1, 0, 2,\n",
       "       2, 4, 3, 3, 2, 4, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 22)\n",
      "2019    33\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "keywords_gold, abstracts = get_key_abs(\"Datasets/DataFiles/bib_tug_dataset_full.parquet\",year1 = 2019, year2 = 2019, types=[\"compsci\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_documents = []\n",
    "for d in new_documents:\n",
    "    d = new_documents[0]\n",
    "    X = vectorizer.transform([d])\n",
    "    min_d = (100,0)\n",
    "    for center in range(k):\n",
    "        dist = distance.cosine(X[0].toarray(), np.array(model.cluster_centers_[center]))\n",
    "        if dist < min_d[0]:\n",
    "            min_d=(dist, center)\n",
    "    collab_docs = []\n",
    "    for i in range(len(model.labels_)):\n",
    "        label = model.labels_[i]\n",
    "        if label == min_d[1]:\n",
    "            collab_docs.append((docs[i],min_d[0]))\n",
    "    collab_documents.append(collab_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "all_scores_adjust = []\n",
    "T0 = 0.0\n",
    "T1 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▌                                                                                | 1/33 [01:29<47:50, 89.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████                                                                              | 2/33 [02:59<46:20, 89.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▌                                                                           | 3/33 [04:21<43:41, 87.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████                                                                         | 4/33 [05:51<42:34, 88.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▌                                                                      | 5/33 [07:20<41:18, 88.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████████████                                                                    | 6/33 [08:43<39:03, 86.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▌                                                                 | 7/33 [10:17<38:34, 89.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|████████████████████                                                               | 8/33 [11:57<38:26, 92.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▋                                                            | 9/33 [13:38<37:56, 94.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▊                                                         | 10/33 [15:07<35:41, 93.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▎                                                      | 11/33 [16:36<33:41, 91.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████▊                                                    | 12/33 [18:07<32:06, 91.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████████████████████▎                                                 | 13/33 [19:37<30:20, 91.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▊                                               | 14/33 [21:11<29:05, 91.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                            | 15/33 [22:40<27:22, 91.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████▊                                          | 16/33 [24:11<25:46, 90.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▏                                       | 17/33 [25:32<23:30, 88.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|████████████████████████████████████████████▋                                     | 18/33 [26:57<21:48, 87.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▏                                  | 19/33 [28:28<20:35, 88.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|█████████████████████████████████████████████████▋                                | 20/33 [29:57<19:08, 88.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▏                             | 21/33 [31:27<17:47, 88.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████▋                           | 22/33 [32:54<16:11, 88.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▏                        | 23/33 [34:17<14:29, 86.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████▋                      | 24/33 [35:36<12:41, 84.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████████████████████████████████████                    | 25/33 [37:12<11:42, 87.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▌                 | 26/33 [38:34<10:03, 86.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████               | 27/33 [39:54<08:25, 84.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▌            | 28/33 [41:13<06:52, 82.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████████          | 29/33 [42:34<05:29, 82.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 30/33 [43:54<04:04, 81.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████     | 31/33 [45:13<02:41, 80.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████████▌  | 32/33 [46:32<01:20, 80.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a new performance model for dynamic locking is proposed. it is based on a flow diagram and uses only the steady state average values of the variables. it is general enough to handle nonuniform access, shared locks, static locking, multiple transaction classes, and transactions of indeterminate length. the analysis is restricted to the case in which all conflicts are resolved by restarts. the model shows that data contention can cause the throughput to thrash, and gives a limit on the workload that will prevent this. it also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.\n",
      "describes three ada packages which support the teaching of some recent and important ideas in numerical computation. the first is designed to enable students to write and use applications programs demonstrating the behavior of floating point arithmetic with different precisions and roundings. it also underpins the other two packages which provide simple computational tools for interval arithmetic and for accurate arithmetic.\n",
      "a number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in long-running programs on production systems -- the same bugs that are the most difficult to find using traditional techniques. in this paper we propose the artemis$^1$ is the greek goddess of the hunt and wild animals. our framework guides the hunt for wild bugs. compiler-based instrumentation framework that complements many pre-existing runtime monitoring techniques. the artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. artemis also facilitates system-load aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. our experiments show that artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that artemis can effectively guide a monitoring tool to the buggy regions of a program. our experimental results show that artemis applied to a hardware-based pc-invariance monitoring scheme and a value-based invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bug-detecting capabilities.\n",
      "computing machines which directly execute the statements of a high level language have been proposed in the past. this report describes the actual implementation of such a machine: it is a computer whose ``machine language'' is apl. the machine is fully operational and correctly executes almost all of the apl operations on scalars, vectors, and arrays. the machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors.\n",
      "g. knott has presented algorithms for computing a bijection between the set of binary trees on $n$ nodes and an initial segment of the positive integers. d. rotem and y. l. varol presented a more complicated algorithm that computes a different bijection, claiming that their algorithm is more efficient and has advantages if a sequence of several consecutive trees is required. a modification of knott's algorithm that is simpler than knott's and as efficient as rotem and varol's is presented. also given is a new linear-time algorithm for transforming a tree into its successor in the natural ordering of binary trees.\n",
      "this article intended to help readers compare the three products, senac, intercall and irena. each of the three is in the numerical algorithms group (fortran 77). senac is a package developed by the mathematical software project at the university of waikato. senac includes a symbolic manipulation interpreter (sencore), together with high level interfaces to the nag fortran library (numlink) and nag graphics library (graflink). the system evolved from an interface, naglink, between the nag library and the symbolic manipulation language macsyma. the sensore interpreter is written in common lisp and belongs to the macsyma/maple/reduce family of languages. it is easy to load user-written files written in common lisp or in senac. intercall is a mathematica package that enables its user to interactively call external code, for example from an external numeric library. it allows various default settings to be defined for any arguments that need to be passed to any routine in that code, and initial default settings for all the routines in the imsl and nag libraries are included with intercall in the form of a small defaults-database file. irena --- an interface from reduce to nag --- runs under the reduce computer algebra system and provides an interactive front end to the nag library. it has been designed to make the use of the nag library considerably simpler, in a number of ways.\n",
      "this article presents the design and implementation of a trusted sensor node that provides internet-grade security at low system cost. we describe trustedfleck, which uses a commodity trusted platform module (tpm) chip to extend the capabilities of a standard wireless sensor node to provide security services such as {\\em message integrity, confidentiality, authenticity}, and {\\em system integrity\\/} based on rsa public-key and xtea-based symmetric-key cryptography. in addition trustedfleck provides secure storage of private keys and provides platform configuration registers (pcrs) to store system configurations and detect code tampering. we analyze system performance using metrics that are important for wsn applications such as computation time, memory size, energy consumption and cost. our results show that trustedfleck significantly outperforms previous approaches (e.g., tinyecc) in terms of these metrics while providing stronger security levels. finally, we describe a number of examples, built on trustedfleck, of symmetric key management, secure rpc, secure software update, and {\\em remote attestation}.\n",
      "regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. one approach is tamper detection via cryptographic hashing. this article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. we present four successively more sophisticated forensic analysis algorithms: the monochromatic, rgby, tiled bitmap, and a3d algorithms, and characterize their ``forensic cost'' under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. a lower bound on forensic cost is derived, with rgby and a3d being shown optimal for a large number of corruptions. we also provide validated cost formul{\\ae} for these algorithms and recommendations for the circumstances in which each algorithm is indicated.\n",
      "a method for accurately determining whether two given line segments intersect is presented. this method uses the standard floating-point arithmetic that conforms to ieee 754 standard. if three or four ending points of the two given line segments are on a same vertical or horizontal line, the intersection testing result is obtained directly. otherwise, the ending points and their connections are mapped onto a $3 \\times 3$ grid, and the intersection testing falls into one of the five testing classes. the intersection testing method is based on our method for floating-point dot product summation, whose error bound is 1 {\\em ulp}. our method does not have the limitation in the method of gavrilova and rokne (2000) that the product of two floating-point numbers is calculated by a twice higher precision floating-point arithmetic than that of the multipliers. furthermore, this method requires less than one-fifth of the running time used by the method of gavrilova and rokne (2000), and our new method for calculating the sign of a sum of $n$ floating-point numbers requires less than one-fifteenth of the running time used by essa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. we synthesize the complementary advantages of these two mechanisms in a novel module system design we call mixml.\\par a mixml module is like an ml structure in which some of the components are specified but not defined. in other words, it unifies the ml structure and signature languages into one. mixml seamlessly integrates hierarchical composition, translucent mlstyle data abstraction, and mixin-style recursive linking. moreover, the design of mixml is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ml module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.\n",
      "the authors describe a fortran to ada converter that uses a standard intermediate tree representation of the program being converted. points of some general interest include overall comments on high level language conversion and the selection of an intermediate representation as well as the way this system was influenced by that of the interactive improvement system for which it is ultimately to provide components. finally, the existing system and the conversions applied to some selected fortran constructs are described in detail.\n",
      "this paper presents a generalization of an old programming technique; using it, one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size. radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (i/o) format. applications to cobol, mix, and hexadecimal sums are discussed.\n",
      "the authors give a randomized incremental algorithm for the construction of planar voronoi diagrams and delaunay triangulations. the algorithm is more `on-line' than earlier similar methods, takes expected time $ o(n / \\log n) $ and space $ o(n) $, and is eminently practical to implement. the analysis of the algorithm is also interesting in its own right and can serve as a model for many similar questions in both two and three dimensions. finally they demonstrate how this approach for constructing voronoi diagrams obviates the need for building a separate point-location structure for nearest-neighbor queries.\n",
      "shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation. an elegant model for shallow binding in lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. rerooting is completely general and reversible, and is optional in the sense that a lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked on every context change. since rerooting leaves ${\\rm assoc}[v, a]$ invariant, for all variables $v$ and all environments $a$, the programmer can have access to a rerooting primitive, shallow [], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.\n",
      "in this tutorial, selected topics of cryptology and of computational complexity theory are presented. we give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. a function is one-way if it is easy to compute, but hard to invert. we discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. we also consider interactive proof systems and present some interesting zero-knowledge protocols. in a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.\n",
      "successful programming languages change as they age. they tend to become more complex, and eventually some features become outdated or are rarely used. programming tools for these languages become more complex as well, since they have to support archaic features. old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. these problems can be solved by refactoring tools that can transform programs to use the modern form. we show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, fortran and java, and showing that each change corresponds to an automatable refactoring.\n",
      "applications of high-speed fiber optic transmission systems continue to grow. special lightwave test instrumentation is required to test these high-speed systems. often the systems and their components, such as laser diode transmitters, need to be characterized in both frequency and time domains. the hp 11982a amplified lightwave converter combines a high-speed pin photodetector with a low-noise preamplifier to provide a general-purpose instrumentation front end for lightwave frequency-domain and time-domain measurements on optical signals over the 1200-nm-to-1600-nm wavelength range. it can be used with spectrum analyzers, oscilloscopes, bit error rate testers, and network analyzers.\n",
      "a novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. the representation, which has practical storage requirements, provides a rapid means of searching large files for data associated with geometric position as well as with attribute value. an application of this representation to handling terrain information illustrates its utility. the algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point. pertinent to mapping.\n",
      "this paper draws a parallel between document preparation and the traditional processes of compilation and link editing for computer programs. a block-based document model is described which allows for separate compilation of various portions of a document. these portions are brought together and merged by a linker program, called {\\tt dlink}, whose pilot implementation is based on {\\tt ditroff} and on its underlying intermediate code. in the light of experiences with {\\tt dlink} the requirements for a universal `object-module language' for documents are discussed. these requirements often resemble the characteristics of the intermediate codes used by programming-language compilers but with interesting extra constraints which arise from the way documents are `executed'.\n",
      "structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. as such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. this fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. in this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. this result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the paper provides the rationale for the architecture and design of a color image display and processing system called hacienda. the system was heavily influenced by one of its most important intended applications, the processing of landsat data, including that to be provided by landsat d. also considered in the paper are the trade-offs involved in making the system suitable for a broader range of image processing work without unduly adding to cost or complexity.\n",
      "many of the ada 9x mappings will have a dramatic impact on current implementations of ada run-time executives. this impact is particularly severe for the case of run-time systems that are implemented with protected kernels, often required in real-time military embedded systems. further, we believe that run-time systems that provide these new 9x facilities will be required to use techniques that are less efficient than current run-time systems at providing ada 83 tasking services. consequently, existing ada systems that meet performance requirements may require modification to tread water. this report considers the proposed 9x changes which affect the implementation of real-time ada run-time systems, addresses why these features impact the run-time, and, where possible, suggests optimizations to minimize the performance impact on properly implemented applications.\n",
      "java, a language designed for internet development, is an object-oriented, multithreaded, portable, dynamic language that's similar to c, yet simpler than c++.\n",
      "distributed computations are concurrent programs in which processes communicate by message passing. such programs typically execute on network architectures such as networks of workstations or distributed memory parallel machines (i.e., multicomputers such as hypercubes). several paradigms --- examples or models --- for process interaction in distributed computations are described. these include networks of filters, clients, and servers, heartbeat algorithms, probe/echo algorithms, broadcast algorithms, token-passing algorithms, decentralized servers, and bags of tasks. these paradigms are appliable to numerous practical problems. they are illustrated by solving problems, including parallel sorting, file servers, computing the topology of a network, distributed termination detection, replicated databases, and parallel adaptive quadrature. solutions to all problems are derived in a step-wise fashion from a general specification of the problem to a concrete solution. the derivations illustrate techniques for developing distributed algorithms.\n",
      "the field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. this is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. we give a survey of the nowadays most important metaheuristics from a conceptual point of view. we outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. two very important concepts in metaheuristics are intensification and diversification. these are the two forces that largely determine the behavior of a metaheuristic. they are in some way contrary but also complementary to each other. we introduce a framework, that we call the i\\&d frame, in order to put different intensification and diversification components into relation with each other. outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.\n",
      "we present a semantics-driven approach for stroke-based painterly rendering, based on recent image parsing techniques [tu et al. 2005; tu and zhu 2006] in computer vision. image parsing integrates segmentation for regions, sketching for curves, and recognition for object categories. in an interactive manner, we decompose an input image into a hierarchy of its constituent components in a parse tree representation with occlusion relations among the nodes in the tree. to paint the image, we build a brush dictionary containing a large set (760) of brush examples of four shape/appearance categories, which are collected from professional artists, then we select appropriate brushes from the dictionary and place them on the canvas guided by the image semantics included in the parse tree, with each image component and layer painted in various styles. during this process, the scene and object categories also determine the color blending and shading strategies for inhomogeneous synthesis of image details. compared with previous methods, this approach benefits from richer meaningful image semantic information, which leads to better simulation of painting techniques of artists using the high-quality brush dictionary. we have tested our approach on a large number (hundreds) of images and it produced satisfactory painterly effects.\n",
      "a mathematical formalism is described through which a program is given a symbolic representation and, with the application of several basic formulas, may be transformed into a equivalent representation giving rise to a reorganized program. examples are given in which programs are simplified (e.g., code is reduced) or reorganized into a structured form. in effect a mathematics is described that applies to programs in much the same manner as boolean algebra applies to switching circuits.\n",
      "recently, a quick and simple way of creating very efficient distinguishers for cryptographic primitives such as block ciphers or hash functions, was presented and proved useful by the authors. in this paper, this cryptanalytic attack (named genetic cryptanalysis after its use of genetic algorithms) is shown to be successful when applied over reduced-round versions of the block cipher xtea. efficient distinguishers for xtea are proposed for up to 4 cycles (8 rounds). additionally, a variant of this genetic attack is also introduced, and their results over the block cipher tea presented, which are the most powerful published to date.\n",
      "identifies the major requirements and design for storage controls and describes how these requirements have been met over time. it also describes the interplay of the three critical components of a subsystem: hardware technology, micro-code and software.\n",
      "in the past, programming systems have provided only a single general purpose implementation for an abstract type. thus the programs produced using abstract types were often inefficient in space or time. a system for automatically choosing efficient implementations for abstract types from a library of implementations for abstract types from a library of implementations is discussed. this process is discussed in detail for an example program. general issues in data structure selection are also reviewed.\n",
      "an efficient procedure for detecting approximate circles and approximately circular arcs of varying gray levels in an edge-enhanced digitized picture is described.\n",
      "once dismissed as figments of pilots' imaginations, strange flashes appearing above thunderstorms have been confirmed as entirely new forms of lightning. known as sprites, elves, blue jets and gamma-ray events, these high-altitude phenomena arise through a physics all their own.\n",
      "abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. we present an abstract interpretation-based method, called {\\em abstract debugging\\/}, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. we show how {\\em invariant assertions\\/} and {\\em intermittent assertions\\/}, such as termination, can be used to formally debug programs. finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the {\\em syntox\\/} system that enables the abstract debugging of the {\\em pascal\\/} language by the determination of the range of the scalar variables of programs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the utilization of storage is studied in a two-level memory hierarchy. the first storage level, which is the fast store, is divided into a number of storage areas. when an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas. if this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available. given that n entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store. this expression gives a measure of how effectively the first-level store is being used. by means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store.\n",
      "java is a programming language loosely related to c++. java originated in a project to produce a software development environment for small distributed embedded systems. programs needed to be small, fast, ``safe'' and portable. these needs led to a design that is rather different from standard practice. in particular, the form of compiled programs is machine independent bytecodes. but we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. this lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.\n",
      "with the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. we argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. we propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. the first abstraction, flash, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. operand descriptions are registered for a particular operation a priori by the library implementor. a runtime system, supermatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. but not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. we show how our recently proposed lu factorization with incremental pivoting and a closely related algorithm-by-blocks for the qr factorization, both originally designed for out-of-core computation, overcome this difficulty. anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.\n",
      "we study protected nodes in various classes of random rooted trees by putting them in the general context of fringe subtrees introduced by aldous (1991). several types of random trees are considered: simply generated trees (or conditioned galton--watson trees), which includes several cases treated separately by other authors, binary search trees and random recursive trees. this gives unified and simple proofs of several earlier results, as well as new results.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $ a = \\pm 2^q \\pm 2^r $ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n",
      "with constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. the fpga community has only recently started focusing on the effects of variations. in this work we present a statistical analysis to compare the effects of variations on designs mapped to fpgas and asics. we also present cad and architecture techniques to mitigate the impact of variations. first we present a variation-aware router that optimizes statistical criticality. we then propose a modification to the clock network to deliver programmable skews to different flip-flops. finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12\\% improvement in timing yield. when the desired timing yield is set to 99\\%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10\\% over a purely deterministic approach.\n",
      "in an effort to capture the dynamics of c++ class design evolution, the article illustrates the process by which object-oriented techniques and c++ idioms were incrementally applied to solve a relatively small, yet surprisingly subtle problem. this problem arose during the development of a family of concurrent distributed applications that execute efficiently on uniprocessor and multiprocessor platforms. this article focuses on the steps involved in generalizing from existing code by using templates and overloading to transparently parameterize synchronization mechanisms into a concurrent application. some of the infrastructure code is based on components in the freely available adaptive service executive (asx) framework.\n",
      "adapt is an architecture-independent language based on the split-and-merge model. it is specialized for efficient, parallel computation of image-processing algorithms.\n",
      "cedar is the name for both a language and an environment in use in the computer science laboratory at xerox parc since 1980. the cedar language is a superset of mesa, the major additions being garbage collection and runtime types. neither the language nor the environment was originally intended to be portable, and for many years ran only on d-machines at parc and a few other locations in xerox. we recently re-implemented the language to make it portable across many different architectures. we present a brief description of the cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the unix operating system, and some measures of the performance of our `portable cedar'.\n",
      "the emergence of aspect-oriented programming (aop) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. what we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. the main contribution of this work is a new form of information-hiding interface for aop that we call the crosscut programming interface, or xpi. xpis abstract crosscutting behaviors and make these abstractions explicit. xpis can be used, albeit with limited enforcement of interface rules, with existing aop languages, such as aspectj. to evaluate our notion of xpis, we have applied our xpi-based design methodology to a medium-sized network overlay application called hypercast. a qualitative and quantitative analysis of existing ao design methods and xpi-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.\n",
      "using their protected mode program loader, bill and lynne create a minimal 80386 protected mode standalone c programming environment for operating systems kernel development\n",
      "this paper begins with a brief overview of the concept of a page description language in general, and postscript in particular. the remainder of the paper discusses the technical and commercial issues that we believe to be critical factors that a user of a page description language must evaluate in choosing a particular language such as postscript. throughout the paper we will use concepts from the postscript language and its implementation in the products that embody it to illustrate key points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a method of automatic generation of functional vectors for sequential circuits. these vectors can be used for design verification, manufacturing testing, or power estimation. a high-level description of the circuit in vhdl or c is assumed available. our method automatically transforms the high-level description of a circuit in vhdl or c into an extended finite state machine (efsm) model that is used to generate functional vectors. the efsm model is a generalization of the traditional state machine model. it is a compact representation of models with local data variables and preserves many nice properties of a traditional state machine model. the theoretical background of the efsm model is addressed in this article. our method guarantees that the generated vectors cover every statement in the high-level description at least once. experimental results show that a set of comprehensive functional vectors for sequential circuits with more than a hundred flip-flops can be generated automatically in a few minutes of cpu time using our prototype system.\n",
      "comparing one language to another usually is like comparing coconuts to kumquats. to make comparisons easier, we implemented a double-ended linked-list class in c++, then in smalltalk, eiffel, sather, objective-c, parasol, beta, turbo pascal, c+@, liana, ada, and, yes, even drool.\n",
      "recommended prerequisites: familiarity with object oriented programming principles in a language such as ada, java, or c++ is recommended. previous experience with c or ada 83 is strongly recommended. this tutorial will briefly describe the object-oriented (oo) features provided by ada 95, and then discuss in detail how some of these features are implemented 'under the hood.' the bulk of the tutorial will show, via code examples and discussion, how dynamic dispatching is implemented in ada 95, issues associated with the class tag, and the effort required to add a sibling class and a child class to an existing class hierarchy. we will briefly discuss several ada 2005 features, and the value these features add over an ada 95 implementation of a program. we will conclude with a brief discussion of the 'overloaded vocabulary' issues encountered by java and c++ programmers as they transition to an ada 95 or ada 2005 project 'on the fly.'\n",
      "building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. thinking of a project with a high integrity kernel written in ada, using a set of low level libraries and drivers written in c or c++, with a graphical interface done in java and unit tests driven by python is not thinking of science fiction anymore. it's actual concrete and day-to-day work. unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.\\par in this tutorial, we'll first study how to interface directly ada with native languages, such as c or c++. we'll then have a deep look at communications with languages running on virtual machines, such as java, python and the .net framework. finally, we'll see how ada can be interfaced with an arbitrary language using a middleware solution, such as soap or corba we?ll see how the communication can be manually done using low level features and apis, and how a substantial part of this process can be automated using high level binding generators.\n",
      "two ibm enterprise system/3090 model 600j computer systems, each with six processors capable of executing vector and scalar instructions, have been connected into a cluster for parallel execution of single fortran programs. the clustering is achieved through a combination of software and hardware. when enabled for parallel execution and allowed to use all twelve processors in the cluster, fortran programs have run as much as 11.7 times faster than when run on a single processor. the combined hardware and software technology is called ibm clustered fortran. it was achieved by modifying existing technology quickly to provide new capabilities. the paper discusses the modifications and the motivations behind them. it summarizes the performance of several applications executed with clustered fortran. finally, it describes how clustering has been used to improve performance in novel ways.\n",
      "this paper explains our efforts to add ada to microsoft's family of .net languages. there are several advantages to weaving ada into the common language environment provided by the .net environment. this paper explains our approach and current progress on the research. we provide the means to extract ada specification files from microsoft intermediate language (msil) code and compile ada programs into msil.\n",
      "ted and dennis review b-tree concepts, then summarize their investigation into a simpler, more efficient approach to managing b-trees.\n",
      "the traditional approach to computer language design has been to design one language at a time. this approach has led to languages which are syntacticly complex and incompatible with one another. the proposed approach develops a language base, utilizing primitive language forms, upon which new languages may be built for all purposes. the resulting languages may possess diverse vocabulary and characteristics, but they are unified by the grammar of the language base upon which they are built, and they can be translated by one general processor. these languages can possess all the functions and facilities of today's languages and they can be readily extended to accommodate new requirements as they arise. a partial implementation of a general processor for this approach has been completed and is described.\n",
      "the hp e5200a broadband service analyzer contains three intel i960 microprocessors, each running several threads of execution. a layer of software called object transport provides an interface for streaming elemental types between threads of execution on the same processor, between processors on the same host and between hosts on the same network. another software layer, called remote object communication, makes object services available to remote clients. each of the threads managed object is specified in a language-independent managed object definition (mod) file which specifies and documents the public interface to a managed object.\n",
      "the hp e1414 pressure scanning vxibus analog-to-digital converter completes hp's vxibus offering for jet engine and wind tunnel test applications by providing the ability to make pressure measurements. our systems approach to providing all of the desired throughput features and all of the required mixed measurements was based on the c-size vxibus platform and the partnership with pressure systems incorporated (psi). this approach combines the new hp e1413 and e1414 vxibus modules with psi pressure scanners and calibrators, other vxibus modules, and compiled scpi (standard commands for programmable instruments) programs. this combination provides the performance and measurement versatility required to satisfy these demanding applications.\n",
      "two versions of an experimental bipolar dynamic memory cell are described. the memory cell consists of one p-channel mosfet and a bipolar npn transistor with extensive node sharing. the mosfet device controls the charge injection into the floating base of the npn transistor, and the bipolar device provides amplification for the stored charge during read operation. for memories, this cell offers performance associated with bipolar technology and chip density comparable to mosfet memories.\n",
      "the working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms. although the words ``working set'' are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program working set behavior. it is the purpose of this paper to present empirical data from actual program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work.\n",
      "linear congruential random-number generators with mersenne prime modulus and multipliers of the form $a = \\pm 2^q \\pm 2^r$ have been proposed recently. their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. this note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a methodology for benchmarking dedicated, interactive systems has been developed at the mitre corporation. this methodology uses a synthetic program model of the application which runs on the proposed hardware/operating system configurations and is driven by a statistically derived load. system performance is measured by analyzing the synthetic transaction response times. the methodology yields assurances to a buyer that the benchmarked system has at least an a priori defined amount of computer power available for applications-oriented software. this paper examines the methodology and the problems that were encountered and solutions which have been used in calibrating a benchmark model for a specific application. the benchmark was designed to model a large interactive information processing application on a procurement requiring loosely-coupled (no shared memory) multicomputer systems. the model consists of a set of interacting synthetic program cells, each composed of several abstractly defined components. the model is maintained in a very high level language that is automatically translated into a standard high order language (typically fortran or cobol) for delivery to the competing vendors. these delivered model cells contain automatically generated size and time filler code that ``calibrate'' the cells to consume the appropriate cpu time and memory space as defined by the abstract size units after accounting for each vendor's hardware and proposed system design.\n",
      "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes. these protocols are run in a system that consists of $n$ processes, $t$ of which are faulty. the protocols are conducted in synchronous rounds of message exchange. it is shown that, in the absence of eavesdropping, without using cryptography, for any $\\epsilon > 0$ and $t = n/(3 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. if cryptographic methods are allowed, then, for $\\epsilon > 0$ and $t = n / (2 + \\epsilon)$, there is a randomized protocol with $o(\\log n)$ expected number of rounds. this is an improvement on the lower bound of $t + 1$ rounds required for deterministic protocols, and on a previous result of $t / \\log n$ expected number of rounds for randomized noncryptographic protocols.\n",
      "a data abstraction can be naturally specified using algebraic axioms. the virtue of these axioms is that they permit a representation-independent formal specification of a data type. an example is given which shows how to employ algebraic axioms at successive levels of implementation. first, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. this implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.\n",
      "a circuit synthesis program for high-performance small-signal amplifiers, called ampdes is described. the synthesis can be done with minimal interaction from the user: only the essential information must be given, and the program will produce a spice net list and a {\\tex} design report. for a typical design, this takes 30 minutes on a mini-computer. unlike other synthesis programs, it searches in an extremely large set of possible circuits (over 1,000,000 configurations), and is not restricted to one device technology. to ensure a high rate of success in the designs, it uses mathematically sound search rules instead of heuristic ones, and accurate approximations of the circuit behavior. these features require considerable computational effort, and therefore it is necessary to use a refined search strategy to reduce the time for a design run. this strategy is also described. (13 refs.)\n",
      "a process for preventing defects has been gaining momentum in the ibm corporation as a way to improve quality and increase productivity. the communications programming laboratory has been implementing the process for the past six years and has realized a 54 percent reduction in errors. the paper documents experiences at the ibm myers corners laboratory mvs interactive programming area in putting the defect prevention process theories into practice. the paper begins with the proposal to adopt the defect prevention process at the myers corners laboratory in poughkeepsie, new york, and experiences thus far. other organizations can benefit from these experiences by understanding how the defect prevention process can be adapted to best meet the needs of any organization.\n",
      "using a receiver makes it right (rmr) data conversion technique in pvm significantly improves the message-passing performance in heterogeneous environments. the improvements are due to two factors: (1). rmr reduces the need for conversions in a heterogeneous environment; (2). at most each message is converted, only once compared to twice for xdr used in public version of pvm, and our conversion routines are streamlined and are several times faster than the xdr routines. the drawback to rmr is the potential need for a large number of conversion routines. we demonstrate that only a small number of routines are required because many vendors use the ieee standard for data representation. given this fact, rmr may emerge as a promising technique in distributed computing.\n",
      "this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. experience has shown that most software engineers find standard temporal logics difficult to understand and use. the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. to illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. the article also describes the tool set and the implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [48:05<00:00, 87.43s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(keywords_gold))):\n",
    "\n",
    "    t3 = time.time()\n",
    "    model = CollabRank(collab_documents[i])\n",
    "    predicted_keywords = model.get_keywords(abstracts[i])\n",
    "    #open(\"C:/Users/dallal/Documents/GitHub/keyphrase/temp/\"+str(i)+\".txt\", \"w\").write(abstracts[i])\n",
    "\n",
    "    t4 = time.time()\n",
    "    try:\n",
    "        scores = get_all_scores(keywords_gold[i], predicted_keywords[0], abstracts[i], adjust=True)\n",
    "\n",
    "    except:\n",
    "        scores = [[0.0, 0.0, 0.0,0.0] , [0.0, 0.0, 0.0,0.0]]\n",
    "\n",
    "    t5 = time.time()\n",
    "    all_scores.append(scores[0])\n",
    "    if scores[1]:\n",
    "        all_scores_adjust.append(scores[1])\n",
    "    T0 += (t4 - t3)\n",
    "    T1 += (t5 - t4)\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_scores_adjust = pd.DataFrame(all_scores_adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = list(all_scores.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09443882496953493,\n",
       " 0.06921998161957657,\n",
       " 0.15454545454545457,\n",
       " 0.28055005354691265]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Bib-CollabRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 22)\n",
      "2017    32\n",
      "2018    22\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keywords_gold_bib, abstracts_bib = get_key_abs(\"Datasets/DataFiles/bib_tug_dataset_full.parquet\",year1 = 2017, year2 = 2018, types=[\"compsci\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_documents_bib=[]\n",
    "for i in range(len(abstracts_bib)):\n",
    "    collab_documents_bib.append((abstracts_bib[i],1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollabRank(collab_documents_bib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 22)\n",
      "2019    33\n",
      "Name: year, dtype: int64\n",
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▌                                                                                | 1/33 [00:45<24:23, 45.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████                                                                              | 2/33 [01:33<23:53, 46.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▌                                                                           | 3/33 [02:33<25:11, 50.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████                                                                         | 4/33 [03:53<28:45, 59.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▌                                                                      | 5/33 [04:57<28:20, 60.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████████████                                                                    | 6/33 [05:55<26:55, 59.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▌                                                                 | 7/33 [06:55<26:00, 60.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|████████████████████                                                               | 8/33 [08:03<25:58, 62.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▋                                                            | 9/33 [09:02<24:30, 61.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▊                                                         | 10/33 [10:07<23:57, 62.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▎                                                      | 11/33 [11:18<23:49, 64.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████▊                                                    | 12/33 [12:28<23:16, 66.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████████████████████▎                                                 | 13/33 [13:28<21:30, 64.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▊                                               | 14/33 [14:24<19:37, 61.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                            | 15/33 [15:14<17:32, 58.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████▊                                          | 16/33 [16:09<16:13, 57.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▏                                       | 17/33 [17:06<15:14, 57.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|████████████████████████████████████████████▋                                     | 18/33 [17:53<13:35, 54.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▏                                  | 19/33 [18:48<12:43, 54.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|█████████████████████████████████████████████████▋                                | 20/33 [19:44<11:52, 54.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▏                             | 21/33 [20:47<11:27, 57.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████▋                           | 22/33 [22:08<11:49, 64.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▏                        | 23/33 [23:26<11:23, 68.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████▋                      | 24/33 [24:41<10:33, 70.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████████████████████████████████████                    | 25/33 [25:56<09:35, 71.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▌                 | 26/33 [27:16<08:40, 74.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████               | 27/33 [28:29<07:22, 73.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▌            | 28/33 [29:35<05:57, 71.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████████          | 29/33 [30:41<04:39, 69.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 30/33 [31:47<03:26, 68.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████     | 31/33 [32:54<02:16, 68.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████████▌  | 32/33 [34:03<01:08, 68.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in deep sub-micron region, spin transfer torque ram (stt-ram) shows read-disturbance error (rde) which presents a crucial reliability challenge. we present shield, a technique to mitigate rde in stt-ram last level caches (llcs). shield uses data compression to reduce cache-write traffic and restore requirement. also, shield keeps two copies of data blocks compressed to less than half the block size and since several llc blocks are only accessed once, this approach avoids several restore operations. shield consumes smaller energy than two previous rde-mitigation techniques, namely high-current restore required read (hcrr, also called restore-after-read) and low-current long latency read (lcll) and even an ideal rde-free stt-ram cache.\n",
      "current memory technology has hit a wall trying to scale to meet the increasing demands of modern client and datacenter systems. data compression is a promising solution to this problem. several compressed memory systems have been proposed in the past years [1], [2], [3], [4]. unfortunately, a reasonable methodology to evaluate these systems is missing. in this paper, we identify the challenges for evaluating main memory compression. we propose an effective methodology to evaluate a compressed memory system by proposing mechanisms to: (i) incorporate correct virtual address translation, (ii) choose a region in the application that is representative of the compression ratio, in addition to regular metrics like ipc and cache hit rates, and (iii) choose a representative region for multi-core workloads, bringing down the correlation error from 12.8 to 3.8 percent.\n",
      "the following topics are dealt with: multiprecision arithmetic; computer arithmetic; dsp; floating-point error analysis; reproducible arithmetic; fpga; matrix computations; and cryptography.\n",
      "ordinary differential equations are ubiquitous in scientific computing. solving exactly these equations is usually not possible, except for special cases, hence the use of numerical schemes to get a discretized solution. we are interested in such numerical integration methods, for instance euler's method or the runge-kutta methods. as they are implemented using floating-point arithmetic, round-off errors occur. in order to guarantee their accuracy, we aim at providing bounds on the round-off errors of explicit one-step numerical integration methods. our methodology is to apply a fine-grained analysis to these numerical algorithms. our originality is that our floating-point analysis takes advantage of the linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved.\n",
      "we give a systematic overview of techniques to compute arithmetic modulo $ 2^x p^y \\pm 1 $ and propose improvements. this is useful for computations in the supersingular isogeny diffie-hellman (sidh) key-exchange protocol which is one of the more recent contenders in the post-quantum public-key arena. one of the main computational bottlenecks in this cryptographic key-exchange protocol is computing modular arithmetic in a finite field defined by a prime of this special shape. recent implementations already use this special prime shape to speed up the cryptographic implementations but it remains unclear if the choices made are optimal or if one can do better. our overview shows that in the sidh setting, where arithmetic over a quadratic extension field is required, the approaches based on montgomery multiplication are to be preferred. based on our results, we give selection criteria for such moduli and the outcome of our search reveals that there exist moduli which result in even faster implementations.\n",
      "the implementation of the fused multiply and add (fma) operation has been extensively studied in the literature on standard and large precisions. we suggest re- visiting those studies for 16-bit precision. we introduce a variation of the mixed precision fma targeted for applications processing low precision inputs (such as machine learning). we also introduce several versions of a fixed point based floating- point fma which performs an exact accumulation of binary16 numbers. we study the implementation and area footprint of those operators in comparison with standard fmas.\n",
      "i will introduce a semi-formalism to allow us to conceptually reason about the differences between customised arithmetic design, as one might see in fpga-based compute, and general purpose arithmetic, as one might find in microprocessor design. this framework will, i hope, expose to the reader the reason that we should be thinking carefully about appropriate data representations when designing custom hardware for compute, as well as clearly showing the link between these decisions and algorithmic ones. i will then provide a concrete example from the literature on matrix computation where some careful algorithmic tweaking results in the ability to use fixed-point arithmetic and, hence, far higher performance than would otherwise be achieved.\n",
      "the paper establishes several simple, but useful relationships between ulp (unit in the last place) errors and the corresponding relative errors. these can be used when converting between the two types of errors, ensuring that the least amount of information is lost in the process. the properties presented here were already useful in ieee conformance proofs for iterative division and square root algorithms, and should be so again to numerical analysts both in 'hand' proofs of error bounds for floating-point computations, and in automated tools which carry out, or support deriving such proofs. in most cases, the properties shown herein establish tighter bounds than found in the literature. they also provide 'conversion' rules of finer granularity, at floating-point value level instead of binade level, and take into account the special conditions which occur at binade ends. for this reason, the paper includes a small, but non-negligible element of novelty.\n",
      "this paper introduces a new method for computing matrix exponential based on truncated neumann series. the efficiency of the method is based on smart factorizations for evaluation of several neumann series that can be done in parallel and divided across different processors with low communication overhead. a physical realization on fpga is provided for proof-of-concept. the method is verified to be advantageous over the usual horner's rule approach for polynomial evaluation. the hardware verification shows a reduction of 62\\% in time required for processing for series approximations with 9 terms. software verification demonstrates a 30\\% reduction in time compared to horner's rule and the trade-offs between using a higher precision approach is illustrated.\n",
      "emerging embedded applications lack of a specific standard when they require floating-point arithmetic. in this situation they use the ieee-754 standard or ad hoc variations of it. however, this standard was not designed for this purpose. this paper aims to open a debate to define a new extension of the standard to cover embedded applications. in this work, we only focus on the impact of not performing normalization. we show how eliminating the condition of normalized numbers, implementation costs can be dramatically reduced, at the expense of a moderate loss of accuracy. several architectures to implement addition and multiplication for non-normalized numbers are proposed and analyzed. we show that a combined architecture (adder-multiplier) can halve the area and power consumption of its counterpart ieee-754 architecture. this saving comes at the cost of reducing an average of about 10 dbs the signal-to-noise ratio for the tested algorithms. we think these results should encourage researchers to perform further investigation in this issue.\n",
      "matrix multiplication is ubiquitous in scientific computing. from computational science to machine learning, a large and diverse set of applications rely on the performance of general matrix-matrix multiplication (gemm) subroutines. the intel math kernel library(r) provides highly optimized gemm subroutines that take full advantage of the available parallelism and vectorization in both intel xeon and intel xeon phi(tm) processors. in this paper we discuss the optimization of gemm subroutines for the intel xeon phitm x200 (code-named knights landing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximate matrix inversion based on neumann series has seen a recent increased interest motivated by massive mimo systems. there, the matrices are in many cases diagonally dominant, and, hence, a reasonable approximation can be obtained within a few iterations of a neumann series. in this work, we clarify that the complexity of exact methods are about the same as when three terms are used for the neumann series, so in this case, the complexity is not lower as often claimed. the second common argument for neumann series approximation, higher parallelism, is indeed correct. however, in most current practical use cases, such a high degree of parallelism is not required to obtain a low latency realization. hence, we conclude that a careful evaluation, based on accuracy and latency requirements must be performed and that exact matrix inversion is in fact viable in many more cases than the current literature claims.\n",
      "lifting-based complex multiplications and rotations are integer invertible, i.e., an integer input value is mapped to the same integer output value when rotating forward and backward. this is an important aspect for lossless transform based source coding, but since the structure only require three real-valued multiplications and three real-valued additions it is also a potentially attractive way to perform complex multiplications when the coefficient has unity magnitude. in this work, we consider two aspects of these structures. first, we show that both the magnitude and angular error is dependent on the angle of input value and derive both exact and approximated expressions for these. second, we discuss how to design such structures without the typical separation into three subsequent matrix multiplications. it is shown that the proposed design method allows many more values which are integer invertible, but can not be separated into three subsequent matrix multiplications with fixed-point values. the results show good correspondence between the error approximations and the actual error as well as a significantly increased design space.\n",
      "there is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. for an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. on the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. we discuss a number of topics involving multiprecision arithmetic, including: the need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). how to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. for solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). how a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
      "we study the accuracy of classical algorithms for evaluating expressions of the form $ (a^2 + b^2) $ and $ c (a^2 + b^2) $ in radix-2, precision-$p$ floating-point arithmetic, assuming that the elementary arithmetic operations $ \\pm $, $ \\times $, $/$, $ \\sqrt $ are rounded to nearest, and assuming an unbounded exponent range. classical analyses show that the relative error is bounded by $ 2 u + o(u^2)$ for $ (a^2 + b^2)$, and by $ 3 u + o(u^2)$ for $ c / (a^2 + b^2)$, where $ u = 2^{-p}$ is the unit roundoff. recently, it was observed that for $ (a^2 + b^2)$ the $ o(u^2)$ term is in fact not needed [1]. we show here that it is not needed either for $ c / (a^2 + b^2)$. furthermore, we show that these error bounds are asymptotically optimal. finally, we show that both the bounds and their asymptotic optimality remain valid when an fma instruction is used to evaluate $ a^2 + b^2$.\n",
      "semidefinite programming (sdp) is widely used in optimization problems with many applications, however, certain sdp instances are ill-posed and need more precision than the standard double-precision available. moreover, these problems are large-scale and could benefit from parallelization on specialized architectures such as gpus. in this article, we implement and evaluate the performance of a floating-point expansion-based arithmetic library (campary) in the context of such numerically highly accurate sdp solvers. we plugged-in campary with the state-of-the-art sdpa solver for both cpu and gpu-tuned implementations. we compare and contrast both the numerical accuracy and performance of sdpa-gmp, -qd and -dd, which employ other multiple-precision arithmetic libraries against sdpa-campary. we show that campary is a very good trade-off for accuracy and speed when solving ill-conditioned sdp problems.\n",
      "we study the implementation of a hardware accelerator that computes a dot product of ieee-754 floating-point numbers exactly. the accelerator uses a wide (640 or 4288 bits for single or double-precision respectively) fixed-point representation into which intermediate floating-point products are accumulated. we designed the accelerator as a generator in chisel, which can synthesize various configurations of the accelerator that make different area-performance trade-offs. we integrated eight different configurations into an soc comprised of risc-v in-order scalar core, split l1 instruction and data caches, and unified l2 cache. in a tsmc 45 nm technology, the accelerator area ranges from 0.05 mm2 to 0.32 mm2, and all configurations could be clocked at frequencies in excess of 900mhz. the accelerator successfully saturates the soc's memory system, achieving the same per-element efficiency (1 cycle-per-element) as intel mkl running on an x86 machine with a similar cache configuration.\n",
      "this work presents a resource optimal approach for the design of large multipliers for fpgas. these are composed of smaller multipliers which can be dsp blocks or logic-based multipliers. a previously proposed multiplier tiling methodology is used to describe feasible solutions of the problem. the problem is then formulated as an integer linear programming (ilp) problem which can be solved by standard ilp solvers. it can be used to minimize the total implementation cost or to trade the lut cost against the dsp cost. it is demonstrated that although the problem is np-complete, optimal solutions can be found for most practical multiplier sizes up to 64x64. synthesis experiments on relevant multiplier sizes show slice reductions of up to 47.5\\% compared to state-of-the-art heuristic approaches.\n",
      "we present a new organization of the qr decomposition (qrd), which is optimized for implementation on parallel arithmetic structures, such as found in current fpgas. data dependencies are hidden in the pipeline depths of the datapaths, allowing implementations to approach 100\\% sustained to peak throughput. the algorithm presented here is based on the modified gram-schmidt (mgs) method, and is designed for floating point (fp) arithmetic, with a combination of separate dot product and multiply-add datapaths. in this short paper, we concentrate on the description of the algorithm and architecture, rather than the implementation, of the qrd.\n",
      "we describe algorithms used to optimize the gnu mpfr library when the operands fit into one or two words. on modern processors, this gives a speedup for a correctly rounded addition, subtraction, multiplication, division or square root in the standard binary64 format (resp. binary128) between 1.8 and 3.5 (resp. between 1.6 and 3.2). we also introduce a new faithful rounding mode, which enables even faster computations. those optimizations will be available in version 4 of mpfr.\n",
      "the leak resistant arithmetic in rns was introduced in 2004 to randomize rsa modular exponentiation. this randomization is meant to protect implementations on embedded device from side channel analysis. we propose in this paper a faster version of the approach of bajard et al. in the case of right-to-left square-and-multiply exponentiation. we show that this saves roughly 30\\% of the computation when the randomization is done at each loop iteration. we also show that the level of randomization of the proposed approach is better than the one of bajard et al. after a few number of loop iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of large integers is a fundamental operation for public key cryptography. in contemporary public key cryptography, the sizes of integers are typically from more than one hundred bits to even several thousands of bits. because these sizes exceed the bit widths of all general-purpose processors, these multiplications must be performed with a multiprecision multiplication algorithm which splits the operation into multiple partial products and accumulation steps. to ensure efficiency, multiprecision multiplication algorithms must be designed with special care and optimized for the instruction sets of specific processors. consequently, developing efficient multiprecision multiplication algorithms and optimizing them for specific platforms has been an active research topic. in this paper, we optimize multiprecision multiplication and squaring specifically for the 64-bit armv8 processors which are widely used, for example, in modern smart phones and tablets. we combine the subtractive karatsuba algorithm, operand-scanning techniques (for multiplication) and sliding-block-doubling methods (for squaring) to accelerate the performance of the 256-bit multiprecision multiplication and squaring by 7.6\\% and 7.0\\% compared to the openssl implementations. we focus particularly on the multiprecision multiplications that are required in elliptic curve cryptography. our implementation supports general elliptic curves of various sizes and all source codes are available in public domain.\n",
      "this paper introduces a new datatype that allows reproducible accumulation of floating-point (fp) numbers in a programmer-selectable range. the new datatype has a larger significand and a smaller range than existing fp formats and has much better arithmetic and computational properties. in particular, it is associative, parallelizable, reproducible and correct. for the modest ranges that will accommodate most problems, it is also much faster: 3 to 12 times faster on a single 256-bit simd implementation. the paper also describes a new instruction and associated datapath that support the proposed datatype, and discusses how a recently published software algorithm for reproducible fp summation could be implemented using the proposed approach.\n",
      "matrix operations are common and expensive computations in a variety of applications. they occur frequently in high-performance computing, graphics, graph processing, and machine learning applications. this paper discusses how to map a variety of important matrix computations, including sparse matrix-vector multiplication (spmv), sparse triangle solve (spts), graph processing, and dense matrix-matrix multiplication, to gpus. since many emerging systems will use heterogeneous architectures (e.g. cpus and gpus) to attain the desired performance targets under strict power constraints, this paper discusses implications and future research for matrix processing with heterogeneous designs. conclusions common to the matrix operations discussed in this paper are: (1) future algorithms should be written to ensure that the essential computations fit into local memory, which may require direct programmer management. (2) algorithms are needed that expose high levels of parallelism. (3) while the scale of computation is often sufficient to support algorithms with superior asymptotic order, additional considerations, such as memory capacity and bandwidth, must also be carefully managed. (4) libraries should be used to provide portable performance.\n",
      "itoh and tsujii proposed a fast algorithm for computing multiplicative inverses (inversions) over gf(2m) using normal bases by iterating single multiplications and cyclic shifts. recently, the itoh--tsujii algorithm (ita) has been modified to use two digit-level single multiplications. the improvements of the modified itoh--tsujii and its variant algorithms are based on reducing the computational latency at the expense of more area requirements. in this paper, we propose a new inversion architecture based on the classical it algorithm (or improved one) utilizing a novel interleaved computations of two single multiplications and squarings at the digit-level. the new inverter outperforms previous modified itoh--tsujii algorithms (such as the ternary itoh--tsujii and optimal 3-chain algorithms) in terms of its lower latency, higher throughput, and improved hardware efficiency. the efficiency of the proposed field inverter is demonstrated by comparisons based on application specific integrated circuits (asic) implementations results using the standard 65nm cmos technology libraries.\n",
      "the met office hadley centre for climate science and services, based at the met office's exeter hq, provides world-class guidance on the science of climate change and is the primary focus in the uk for climate science. the hadley centre makes significant contributions to scientific literature and to a variety of climate science reports, including the international panel on climate change (ipcc). in october 2014 the government confirmed its investment of \\pounds 97 million in a new high performance computing facility for the met office. the new cray xc40 located at exeter science park is the largest hpc system in the world dedicated to weather and climate research. i will first give an overview of how climate numerical experiments are organised worldwide through the coupled model intercomparison project (cmip) under the auspices of the world climate research program (wcrp). the numerical results of these simulation campaigns are submitted to intense scrutiny by the scientific community and policy makers. numerical reproducibility is therefore of paramount importance. i will explain the parameters of what numerical reproducibility means to our community and how we aim to achieve it. i will present the use of different types of floating point arithmetic in the models. two examples are the use double--double precision for reproducible global sums and research on single precision algorithms for computational efficiency. finally, i will look at some of the challenges to maintain numerical reproducibility in the exascale era.\n",
      "floating point error is a notable drawback of embedded systems implementation. computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. this problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. in this paper, we propose and compare two new methods based on bernstein expansions and sparse krivine--stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. we release two related software package fpbern and fpkristen, and compare them with state of the art tools. we show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.\n",
      "astc is an efficient and flexible texture compression format but it is relatively costly to implement in hardware. by outputting multiple texels from a single encoded astc block, we will show an performance per area improvement of 25\\%.\n",
      "we give an overview on optimal circuits to implement linear permutations on fpgas using only ram banks and switches. linear means that the permutation maps linearly the bit representation of the indices, as it is the case with most permutations arising in digital signal processing algorithms including those in fast fourier transforms, viterbi decoders, and sorting networks. additionally, we assume that the data to be permuted is streamed, i.e., input in chunks over several cycles. the circuits are obtained from a suitable factorization of the bit matrix representing the permutation and achieve the minimal number of switches possible.\n",
      "current general purpose libraries for multiple precision floating-point arithmetic such as mpfr suffer from a large performance penalty with respect to hard-wired instructions. the performance gap tends to become even larger with the advent of wider simd arithmetic in both cpus and gpus. in this paper, we present efficient algorithms for multiple precision floating-point arithmetic that are suitable for implementations on simd processors. a.c.m. subject classification: g.1.0 computer-arithmetic a.m.s. subject classification: 65y04, 65t50, 68w30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users of financial and e-commerce services demand a high degree of reliability and at the same time an increasing demand of speed of processing. on the other hand, soft errors are becoming more significant due to the higher densities and reduced cmos integration technology sizes. among the basic arithmetic operations, addition/subtraction is the most demanded. although in the past, binary implementations were only considered, today decimal implementations are becoming important. in this context, we introduce a modular design for fast error checking of binary and decimal (bcd) addition/subtraction operations that avoids the whole replication of the arithmetic units. unlike other error checkers based on parity prediction or residue checking, this is a separable design that lies completely off of the critical path of the protected adder without incurring in important penalties in area or performance.\n",
      "reliable implementation of digital filters in finite precision is based on accurate error analysis. however, a small error in the time domain does not guarantee that the implemented filter verifies the initial band specifications in the frequency domain. we propose a novel certified algorithm for the verification of a filter's transfer function, or of an existing finite-precision implementation. we show that this problem boils down to the verification of bounds on a rational function, and further to the positivity of a polynomial. our algorithm has reasonable runtime efficiency to be used as a criterion in large implementation space explorations. we ensure that there are no false positives but false negative answers may occur. for negative answers we give a tight bound on the margin of acceptable specifications. we demonstrate application of our algorithm to the comparison of various finite-precision implementations of filters already fully designed.\n",
      "the design space exploration for fast and power efficient adders is of increasing interest for microprocessors and graphic and digital signal processors. recently, several methods have been proposed to explore the design space of adders, where well known designs appear as possible instances. these methods are based on the identification of parameters that lead to different hardware structures. in this work, we go a step further by exploring the mathematical foundation behind the trees for carry computation. we propose an algorithm that allows to obtain any adder topology based on design decisions. the method is based on finding representations of integers in a given number system. this leads to an adder model that allows the design of any adder structure in a compact a formal way. the proposed formal model might be useful for a formal design description of adders and it can be incorporated to cad tools.\n",
      "we present ry{\\=u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. ry{\\=u} is simpler and approximately three times faster than the previously fastest implementation.\n",
      "the intel avx-512 architecture adds new capabilities such as masked execution, floating-point exception suppression and static rounding modes, as well as a small set of new instructions for mathematical library support. these new features allow for better compliance with floating-point or language standards (e.g. no spurious floating-point exceptions, and faster or more accurate code for directed rounding modes), as well as simpler, smaller footprint implementations that eliminate branches and special case paths. performance is also improved, in particular for vector mathematical functions (which benefit from easier processing in the main path, and fast access to small lookup tables). in this paper, we describe the relevant new features and their possible applications to floating-point computation. the code examples include a few compact implementation sequences for some common vector mathematical functions.\n",
      "some modern processors include decimal floating-point units, with a conforming implementation of the ieee-754 2008 standard. unfortunately, many algorithms from the computer arithmetic literature are not correct anymore when computations are done in radix 10. this is in particular the case for the computation of the average of two floating-point numbers. several radix-2 algorithms are available, including one that provides the correct rounding, but none hold in radix 10. this paper presents a new radix-10 algorithm that computes the correctly-rounded average. to guarantee a higher level of confidence, we also provide a coq formal proof of our theorems, that takes gradual underflow into account. note that our formal proof was generalized to ensure this algorithm is correct when computations are done with any even radix.\n",
      "we present an automatic method for the evaluation of functions via polynomial or rational approximations and its hardware implementation, on fpgas. these approximations are evaluated using ercegovac's iterative e-method adapted for fpga implementation. the polynomial and rational function coefficients are optimized such that they satisfy the constraints of the e-method. we present several examples of practical interest; in each case a resource-efficient approximation is proposed and comparisons are made with alternative approaches.\n",
      "the following topics are dealt with: floating point arithmetic; digital arithmetic; ieee standards; field programmable gate arrays; learning (artificial intelligence); cryptography; parallel processing; table lookup; multiplying circuits; mathematics computing.\n",
      "veritracer automatically instruments a code and traces the accuracy of floating-point variables over time. veritracer enriches the visual traces with contextual information such as the call site path in which a value was modified. contextual information is important to understand how the floating-point errors propagate in complex codes. veritracer is implemented as an llvm compiler tool on top of verificarlo. we demonstrate how veritracer can detect accuracy loss and quantify the impact of using a compensated algorithm on abinit, an industrial hpc application for ab initio quantum computation.\n",
      "when dealing with floating-point numbers, there are several sources of error which can drastically reduce the numerical quality of computed results. one of those error sources is the loss of significance or cancellation, which occurs during for example, the subtraction of two nearly equal numbers. in this article, we propose a representation format named floating-point adaptive noise reduction (fp-anr). this format embeds cancellation information directly into the floating-point representation format thanks to a dedicated pattern. with this format, insignificant trailing bits lost during cancellation are removed from every manipulated floating-point number. the immediate consequence is that it increases the numerical confidence of computed values. the proposed representation format corresponds to a simple and efficient implementation of significance arithmetic based and compatible with the ieee standard 754 standard.\n",
      "distributed storage systems utilize erasure codes to reduce their storage costs while efficiently handling failures. many of these codes (e.g., reed-solomon (rs) codes) rely on galois field (gf) arithmetic, which is considered to be fast when the field characteristic is 2. nevertheless, some developments in the field of erasure codes offer new efficient techniques that require mostly xor operations, and are thus faster than gf operations. recently, intel announced [1] that its future architecture (codename ice lake) will introduce new set of instructions called galois field new instruction (gf-ni). these instructions allow software flows to perform vector and matrix multiplications over gf ($ 2^8$) on the wide registers that are available on the avx512 architectures. in this paper, we explain the functionality of these instructions, and demonstrate their usage for some fast computations in gf($ 2^8$). we also use the intel intelligent storage acceleration library (isa-l) in order to estimate potential future improvement for erasure codes that are based on rs codes. our results predict $ 1.4 \\times $ speedup for vectorized multiplication, and $ 1.83 \\times $ speedup for the actual encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient selection (qs) is a key step in the classic $ o(n^2) $ multiple precision division algorithm. on processors with fast hardware division, it is a trivial problem, but on gpus, division is quite slow. in this paper we investigate the effectiveness of brent and zimmermann's variant as well as our own novel variant of barrett's algorithm. our new approach is shown to be suitable for low radix (single precision) qs. three highly optimized implementations, two of the brent and zimmerman variant and one based on our new approach, have been developed and we show that each is many times faster than using the division operation built in to the compiler. in addition, our variant is on average 22\\% faster than the other two implementations. we also sketch proofs of correctness for all of the implementations and our new algorithm.\n",
      "the ieee 754-2008 standard governs floating-point arithmetic in all types of computer systems. the standard provides for two radices, 2 and 10. it specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. in contrast, the standard does provide for mixing formats of one radix in one operation. in order to enhance the standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix fused-multiply-and-add (fma). our algorithm takes any combination of ieee754 binary64 and decimal64 numbers in argument and provides a result in ieee754 binary64 and decimal64, rounded according to any for the five ieee754 rounding modes. our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. we compare our implementation to a basic mixed-radix fma implementation based on the gmp multiple precision library.\n",
      "we review several ways to split a floating-point number, that is, to decompose it into the exact sum of two floating-point numbers of smaller precision. all the methods considered here involve only a few ieee floating-point operations, with rounding to nearest and including possibly the fused multiply -add (fma). applications range from the implementation of integer functions such as round and floor to the computation of suitable scaling factors aimed, for example, at avoiding spurious underflows and overflows when implementing functions such as the hypotenuse.\n",
      "many studies focus on implementing processing-in memory (pim) on the logic die of the hybrid memory cube (hmc) architecture. the multiply-accumulate (mac) operation is heavily used in digital signal processing (dsp) systems. in this paper, a novel pim architecture called hmc-mac that implements the mac operation in the hmc is proposed. the vault controllers of the conventional hmc are working independently to maximize the parallelism, and hmc-mac is based on the conventional hmc without modifying the architecture much. therefore, a large number of mac operations can be processed in parallel. in hmc-mac, the mac operation can be carried out simultaneously with as much as 128 kb data. the correctness on hmc-mac is verified by simulations, and its performance is better than the conventional cpu-based mac operation when the mac operation is consecutively executed at least six times\n",
      "this work presents an extension of karatsuba's method to efficiently use rectangular multipliers as a base for larger multipliers. the rectangular multipliers that motivate this work are the embedded $ 18 \\times 25$-bit signed multipliers found in the dsp blocks of recent xilinx fpgas: the traditional karatsuba approach must under-use them as square $ 18 \\times 18 $ ones. this work shows that rectangular multipliers can be efficiently exploited in a modified karatsuba method if their input word sizes have a large greatest common divider. in the xilinx fpg a case, this can be obtained by using the embedded multipliers as $ 16 \\times 24 $ unsigned and as $ 17 \\times 25$ signed ones. the obtained architectures are implemented with due detail to architectural features such as the pre-adders and post-adders available in xilinx dsp blocks. they are synthesized and compared with traditional karatsuba, but also with (non-karatsuba) state-of-the-art tiling techniques that make use of the full rectangular multipliers. the proposed technique improves resource consumption and performance for multipliers of numbers larger than 64 bits.\n",
      "arithmetic based applications are one of the most common use cases for modern fpgas. currently, machine learning is emerging as the fastest growth area for fpg as, renewing an interest in low precision multiplication. there is now a new focus on multiplication in the soft fabric --- very high-density systems, consisting of many thousands of operations, are the current norm. in this paper we introduce multiplier regularization, which restructures common multiplier algorithms into smaller, and more efficient architectures. the multiplier structure is parameterizable, and results are given for a continuous range of input sizes, although the algorithm is most efficient for small input precisions. the multiplier is particularly effective for typical machine learning inferencing uses, and the presented cores can be used for dot products required for these applications. although the examples presented here are optimized for intel stratix 10 devices, the concept of regularized arithmetic structures are applicable to generic fpga lut architectures. results are compared to intel megafunction ip as well as contrasted with normalized representations of recently published results for xilinx devices. we report a 10\\% to 35\\% smaller area, and a more significant latency reduction, in the range of 25\\% to 50\\%, for typical inferencing use cases.\n",
      "we recently proposed the first hardware architecture enabling the iterative solution of systems of linear equations to accuracies limited only by the amount of available memory. this technique, named architect, achieves exact numeric computation by using online arithmetic to allow the refinement of results from earlier iterations over time, eschewing rounding error. architect has a key drawback, however: often, many more digits than strictly necessary are generated, with this problem exacerbating the more accurate a solution is sought. in this paper, we infer the locations of these superfluous digits within stationary iterative calculations by exploiting online arithmetic's digit dependencies and using forward error analysis. we demonstrate that their lack of computation is guaranteed not to affect the ability to reach a solution of any accuracy. versus architect, our illustrative hardware implementation achieves a geometric mean $ 20.1 \\times $ speedup in the solution of a set of representative linear systems through the avoidance of redundant digit calculation. for the computation of high-precision results, we also obtain an up-to $ 22.4 \\times $ memory requirement reduction over the same baseline. finally, we demonstrate that solvers implemented following our proposals can show superiority over conventional arithmetic implementations by virtue of their runtime-tunable precisions.\n",
      "this paper proposes a new design of an approximate hybrid divider (axhd), which combines the restoring array and the logarithmic dividers to achieve an excellent tradeoff between accuracy and hardware performance. exact restoring divider cells (exdcrs) are used to generate the msbs of the quotient for attaining a high accuracy; the other quotient digits are processed by a logarithmic divider as inexact scheme to improve figures of merit such as power consumption, area and delay. the proposed axhd is evaluated and analyzed using error and hardware metrics. the proposed design is also compared with the exact restoring divider (exdr) and previous approximate restoring dividers (axdrs). the results show that the proposed design achieves very good performance in terms of accuracy and hardware; case studies for image processing also show the validity of the proposed designs.\n",
      "in this work, we address the design of an on-chip accelerator for machine learning and other computation-demanding applications with a tunable floating-point (tfp) precision. the precision can be chosen for a single operation by selecting a specific number of bits for significand and exponent in the floating-point representation. by tuning the precision of a given algorithm to the minimum precision achieving an acceptable target error, we can make the computation more power efficient. we focus on floating-point multiplication, which is the most power demanding arithmetic operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning has been undergoing rapid growth in recent years thanks to its state-of-the-art performance across a wide range of real-world applications. traditionally neural networks were trained in ieee-754 binary64 or binary32 format, a common practice in general scientific computing. however, the unique computational requirements of deep neural network training workloads allow for much more efficient and inexpensive alternatives, unleashing a new wave of numerical innovations powering specialized computing hardware. we previously presented flexpoint, a blocked fixed-point data type combined with a novel predictive exponent management algorithm designed to support training of deep networks without modifications, aiming at a seamless replacement of the binary32 widely in practice today. we showed that flexpoint with 16-bit mantissa and 5-bit shared exponent (flex16+s) achieved numerical parity to binary32 in training a number of convolutional neural networks. in the current paper we review the continuing trend of predictive numerics enhancing deep neural network training in specialized computing devices such as the intel nervana neural network processor.\n",
      "algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twosum and twoproduct. the current draft of the ieee 754 standard specifies these operations under the names augmentedaddition and augmentedmultiplication. these operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. standardizing the operations provides a hardware acceleration target that can provide at least a 33\\% speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. this paper provides history and motivation for standardizing these operations. we also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.\n",
      "the next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today's systems. improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. by leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. here, we present delta --- a data-enabled multi-threaded architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. we consider a manycore tiled-chip architecture where network-on-chip (noc) routers are extended to support our execution model. the proposed extension is analysed, while simulation results confirm that delta can manage a large number of simultaneous threads, relying on a simple hardware structure.\n",
      "python is popular among scientific communities that value its simplicity and power, especially as it comes along with numeric libraries such as numpy, scipy, dask, and numba. as cpu core counts keep increasing, these modules can make use of many cores via multi-threading for efficient multi-core parallelism. however, threads can interfere with each other leading to overhead and inefficiency if used together in a single application on machines with a large number of cores. this performance loss can be prevented if all multi-threaded modules are coordinated. this paper continues the work started in amala16 by introducing more approaches to coordination for both multi-threading and multi-processing cases. in particular, we investigate the use of static settings, limiting the number of simultaneously active openmp parallel regions, and optional parallelism with intel threading building blocks (intel tbb). we will show how these approaches help to unlock additional performance for numeric applications on multi-core systems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [35:18<00:00, 64.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12735060150535676, 0.09347078227613226, 0.2090909090909091, 0.2990358498295214] [0.21596661895269967, 0.23684752747252744, 0.215625, 0.4930468851171977]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_scores, all_scores_adjust = eval_file(\"Datasets/DataFiles/bib_tug_dataset_full.parquet\", model,model_param = [\"weights20172018\"] ,year1=2019, year2=2019, types=[\"compsci\"] ,bib_weights={}, log=True)\n",
    "print(all_scores, all_scores_adjust )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12735060150535676, 0.09347078227613226, 0.2090909090909091, 0.2990358498295214] [0.21596661895269967, 0.23684752747252744, 0.215625, 0.4930468851171977]\n"
     ]
    }
   ],
   "source": [
    "print(all_scores, all_scores_adjust )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
