variable latent semantic indexing latent semantic indexing is a classical method to produce optimal low-rank approximations of a term-document matrix . however , in the context of a particular query distribution , the approximation thus produced need not be optimal . we propose vlsi , a new query-dependent ( or `` variable '' ) low-rank approximation that minimizes approximation error for any specified query distribution . with this tool , it is possible to tailor the lsi technique to particular settings , often resulting in vastly improved approximations at much lower dimensionality . we validate this method via a series of experiments on classical corpora , showing that vlsi typically performs similarly to lsi with an order of magnitude fewer dimensions .

['linear algebra', 'lsi', 'matrix approximation', 'sparse, structured, and very large systems', 'svd', 'vlsi']

vlsi NOUN dobj propose
lsi NOUN compound technique
vlsi NOUN nsubj performs
lsi VERB pobj to

entity discovery and assignment for opinion mining applications opinion mining became an important topic of study in recent years due to its wide range of applications . there are also many companies offering opinion mining services . one problem that has not been studied so far is the assignment of entities that have been talked about in each sentence . let us use forum discussions about products as an example to make the problem concrete . in a typical discussion post , the author may give opinions on multiple products and also compare them . the issue is how to detect what products have been talked about in each sentence . if the sentence contains the product names , they need to be identified . we call this problem entity discovery . if the product names are not explicitly mentioned in the sentence but are implied due to the use of pronouns and language conventions , we need to infer the products . we call this problem entity assignment . these problems are important because without knowing what products each sentence talks about the opinion mined from the sentence is of little use . in this paper , we study these problems and propose two effective methods to solve the problems . entity discovery is based on pattern discovery and entity assignment is based on mining of comparative sentences . experimental results using a large number of forum posts demonstrate the effectiveness of the technique . our system has also been successfully tested in a commercial setting .

['entity discovery', 'sentiment analysis']


stable feature selection via dense feature groups many feature selection algorithms have been proposed in the past focusing on improving classification accuracy . in this work , we point out the importance of stable feature selection for knowledge discovery from high-dimensional data , and identify two causes of instability of feature selection algorithms : selection of a minimum subset without redundant features and small sample size . we propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results . the framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection . an efficient algorithm drags ( dense relevant attribute group selector ) is developed under this framework . we also introduce a general measure for assessing the stability of feature selection algorithms . our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out , and the drags algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability .

['classification', 'feature selection', 'high-dimensional data', 'kernel density estimation', 'learning', 'stability']

classification NOUN compound accuracy
stability NOUN conj generalization
stability NOUN dobj assessing
classification NOUN compound accuracy
stability NOUN conj accuracy

consensus group stable feature selection stability is an important yet under-addressed issue in feature selection from high-dimensional and small sample data . in this paper , we show that stability of feature selection has a strong dependency on sample size . we propose a novel framework for stable feature selection which first identifies consensus feature groups from subsampling of training samples , and then performs feature selection by treating each consensus feature group as a single entity . experiments on both synthetic and real-world data sets show that an algorithm developed under this framework is effective at alleviating the problem of small sample size and leads to more stable feature selection results and comparable or better generalization performance than state-of-the-art feature selection algorithms . synthetic data sets and algorithm source code are available at http:\/\/www.cs.binghamton.edu\/~lyu\/kdd09\/ .

['ensemble', 'feature selection', 'high-dimensional data', 'small sample', 'stability']

stability NOUN nsubj is
stability NOUN nsubj has

coa : finding novel patents through text analysis in recent years , the number of patents filed by the business enterprises in the technology industry are growing rapidly , thus providing unprecedented opportunities for knowledge discovery in patent data . one important task in this regard is to employ data mining techniques to rank patents in terms of their potential to earn money through licensing . availability of such ranking can substantially reduce enterprise ip ( intellectual property ) management costs . unfortunately , the existing software systems in the ip domain do not address this task directly . through our research , we build a patent ranking software , named coa ( claim originality analysis ) that rates a patent based on its value by measuring the recency and the impact of the important phrases that appear in the `` claims '' section of a patent . experiments show that coa produces meaningful ranking when comparing it with other indirect patent evaluation metrics -- citation count , patent status , and attorney 's rating . in reallife settings , this tool was used by beta-testers in the ibm ip department . lawyers found it very useful in patent rating , specifically , in highlighting potentially valuable patents in a patent cluster . in this article , we describe the ranking techniques and system architecture of coa . we also present the results that validate its effectiveness .

['document ranking', 'information retrieval', 'patent processing', 'patent visualization']


constructing comprehensive summaries of large event sequences event sequences capture system and user activity over time . prior research on sequence mining has mostly focused on discovering local patterns . though interesting , these patterns reveal local associations and fail to give a comprehensive summary of the entire event sequence . moreover , the number of patterns discovered can be large . in this paper , we take an alternative approach and build short summaries that describe the entire sequence , while revealing local associations among events . we formally define the summarization problem as an optimization problem that balances between shortness of the summary and accuracy of the data description . we show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms . we also explore more efficient greedy alternatives and demonstrate that they work well on large datasets . experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results , and reveal interesting local structures in the data .

['dynamic programming', 'event sequences', 'log mining', 'minimum description length', 'summarization']

summarization NOUN compound problem

learning optimal ranking with tensor factorization for tag recommendation tag recommendation is the task of predicting a personalized list of tags for a user given an item . this is important for many websites with tagging capabilities like last . fm or delicious . in this paper , we propose a method for tag recommendation based on tensor factorization ( tf ) . in contrast to other tf methods like higher order singular value decomposition ( hosvd ) , our method rtf ( ` ranking with tensor factorization ' ) directly optimizes the factorization model for the best personalized ranking . rtf handles missing values and learns from pairwise ranking constraints . our optimization criterion for tf is motivated by a detailed analysis of the problem and of interpretation schemes for the observed data in tagging systems . in all , rtf directly optimizes for the actual problem using a correct interpretation of the data . we provide a gradient descent algorithm to solve our optimization problem . we also provide an improved learning and prediction method with runtime complexity analysis for rtf . the prediction runtime of rtf is independent of the number of observations and only depends on the factorization dimensions . besides the theoretical analysis , we empirically show that our method outperforms other state-of-the-art tag recommendation methods like folkrank , pagerank and hosvd both in quality and prediction runtime .

['ranking', 'tag recommendation', 'tensor factorization']

ranking VERB acl rtf
ranking NOUN pobj for
ranking VERB amod constraints

augmenting the generalized hough transform to enable the mining of petroglyphs rock art is an archaeological term for human-made markings on stone . it is believed that there are millions of petroglyphs in north america alone , and the study of this valued cultural resource has implications even beyond anthropology and history . surprisingly , although image processing , information retrieval and data mining have had large impacts on many human endeavors , they have had essentially zero impact on the study of rock art . in this work we identify the reasons for this , and introduce a novel distance measure and algorithms which allow efficient and effective data mining of large collections of rock art .

['cultural artifacts', 'image processing', 'similarity search']


learning nonstationary models of normal network traffic for detecting novel attacks traditional intrusion detection systems ( ids ) detect attacks by comparing current behavior to signatures of known attacks . one main drawback is the inability of detecting new attacks which do not have known signatures . in this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic . behavior that deviates from the learned normal model signals possible novel attacks . our ids is unique in two respects . first , it is nonstationary , modeling probabilities based on the time since the last event rather than on average rate . this prevents alarm floods . second , the ids learns protocol vocabularies ( at the data link through application layers ) in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software . on the 1999 darpa ids evaluation data set ( 9 ) , we detect 70 of 180 attacks ( with 100 false alarms ) , about evenly divided between user behavioral anomalies ( ip addresses and ports , as modeled by most other systems ) and protocol anomalies . because our methods are unconventional there is a significant non-overlap of our ids with the original darpa participants , which implies that they could be combined to increase coverage .

['security and protection', 'unauthorized access']


bypass rates : reducing query abandonment using negative inferences we introduce a new approach to analyzing click logs by examining both the documents that are clicked and those that are bypassed-documents returned higher in the ordering of the search results but skipped by the user . this approach complements the popular click-through rate analysis , and helps to draw negative inferences in the click logs . we formulate a natural objective that finds sets of results that are unlikely to be collectively bypassed by a typical user . this is closely related to the problem of reducing query abandonment . we analyze a greedy approach to optimizing this objective , and establish theoretical guarantees of its performance . we evaluate our approach on a large set of queries , and demonstrate that it compares favorably to the maximal marginal relevance approach on a number of metrics including mean average precision and mean reciprocal rank .

['bypass rates', 'query abandonment', 'random walk', 'relevance', 'similarity search']

relevance NOUN compound approach

social influence analysis in large-scale networks in large social networks , nodes ( users , entities ) are influenced by others for various reasons . for example , the colleagues have strong influence on one 's work , while the friends have strong influence on one 's daily life . how to differentiate the social influences from different angles ( topics ) ? how to quantify the strength of those social influences ? how to estimate the model on real large networks ? to address these fundamental questions , we propose topical affinity propagation ( tap ) to model the topic-level social influence on large networks . in particular , tap can take results of any topic modeling and the existing network structure to perform topic-level influence propagation . with the help of the influence analysis , we present several important applications on real data sets such as 1 ) what are the representative nodes on a given topic ? 2 ) how to identify the social influences of neighboring nodes on a particular node ? to scale to real large networks , tap is designed with efficient distributed learning algorithms that is implemented and tested under the map-reduce framework . we further present the common characteristics of distributed learning algorithms for map-reduce . finally , we demonstrate the effectiveness and efficiency of tap on real large data sets .

['database applications', 'information search and retrieval', 'large-scale network', 'social influence analysis', 'social networks', 'topical analysis propagation']


effective label acquisition for collective classification information diffusion , viral marketing , and collective classification all attempt to model and exploit the relationships in a network to make inferences about the labels of nodes . a variety of techniques have been introduced and methods that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the nodes in a network . however , in part because of the correlation between node labels that the techniques exploit , it is easy to find cases in which , once a misclassification is made , incorrect information propagates throughout the network . this problem can be mitigated if the system is allowed to judiciously acquire the labels for a small number of nodes . unfortunately , under relatively general assumptions , determining the optimal set of labels to acquire is intractable . here we propose an acquisition method that learns the cases when a given collective classification algorithm makes mistakes , and suggests acquisitions to correct those mistakes . we empirically show on both real and synthetic datasets that this method significantly outperforms a greedy approximate inference approach , a viral marketing approach , and approaches based on network structural measures such as node degree and network clustering . in addition to significantly improving accuracy with just a small amount of labeled data , our method is tractable on large networks .

['active inference', 'collective classification', 'label acquisition']


rotation invariant distance measures for trajectories for the discovery of similar patterns in 1d time-series , it is very typical to perform a normalization of the data ( for example a transformation so that the data follow a zero mean and unit standard deviation ) . such transformations can reveal latent patterns and are very commonly used in datamining applications . however , when dealing with multidimensional time-series , which appear naturally in applications such as video-tracking , motion-capture etc , similar motion patterns can also be expressed at different orientations . it is therefore imperative to provide support for additional transformations , such as rotation . in this work , we transform the positional information of moving data , into a space that is translation , scale and rotation invariant . our distance measure in the new space is able to detect elastic matches and can be efficiently lower bounded , thus being computationally tractable . the proposed methods are easy to implement , fast to compute and can have many applications for real world problems , in areas such as handwriting recognition and posture estimation in motion-capture data . finally , we empirically demonstrate the accuracy and the efficiency of the technique , using real and synthetic handwriting data .

['rotation invariance', 'time warping', 'trajectories']

trajectories NOUN pobj for

co-evolution of social and affiliation networks in our work , we address the problem of modeling social network generation which explains both link and group formation . recent studies on social network evolution propose generative models which capture the statistical properties of real-world networks related only to node-to-node link formation . we propose a novel model which captures the co-evolution of social and affiliation networks . we provide surprising insights into group formation based on observations in several real-world networks , showing that users often join groups for reasons other than their friends . our experiments show that the model is able to capture both the newly observed and previously studied network properties . this work is the first to propose a generative model which captures the statistical properties of these complex networks . the proposed model facilitates controlled experiments which study the effect of actors ' behavior on the evolution of affiliation networks , and it allows the generation of realistic synthetic datasets .

['affiliation network', 'evolution', 'graph generator', 'groups', 'social network']

evolution NOUN pobj on
evolution NOUN dobj captures
groups NOUN dobj join
evolution NOUN pobj on

efficient anomaly monitoring over moving object trajectory streams lately there exist increasing demands for online abnormality monitoring over trajectory streams , which are obtained from moving object tracking devices . this problem is challenging due to the requirement of high speed data processing within limited space cost . in this paper , we present a novel framework for monitoring anomalies over continuous trajectory streams . first , we illustrate the importance of distance-based anomaly monitoring over moving object trajectories . then , we utilize the local continuity characteristics of trajectories to build local clusters upon trajectory streams and monitor anomalies via efficient pruning strategies . finally , we propose a piecewise metric index structure to reschedule the joining order of local clusters to further reduce the time cost . our extensive experiments demonstrate the effectiveness and efficiency of our methods .

['outlier detection', 'similarity search', 'temporal data']


active learning with direct query construction active learning may hold the key for solving the data scarcity problem in supervised learning , i.e. , the lack of labeled data . indeed , labeling data is a costly process , yet an active learner may request labels of only selected instances , thus reducing labeling work dramatically . most previous works of active learning are , however , pool-based ; that is , a pool of unlabeled examples is given and the learner can only select examples from the pool to query for their labels . this type of active learning has several weaknesses . in this paper we propose novel active learning algorithms that construct examples directly to query for labels . we study both a specific active learner based on the decision tree algorithm , and a general active learner that can work with any base learning algorithm . as there is no restriction on what examples to be queried , our methods are shown to often query fewer examples to reduce the predictive error quickly . this casts doubt on the usefulness of the pool in pool-based active learning . nevertheless , our methods can be easily adapted to work with a given pool of unlabeled examples .

['active learning', 'classification', 'supervised learning']


customer lifetime value modeling and its use for customer retention planning we present and discuss the important business problem of estimating the effect of retention efforts on the lifetime value of a customer in the telecommunications industry . we discuss the components of this problem , in particular customer value and length of service ( or tenure ) modeling , and present a novel segment-based approach , motivated by the segment-level view marketing analysts usually employ . we then describe how we build on this approach to estimate the effects of retention on lifetime value . our solution has been successfully implemented in amdocs ' business insight ( bi ) platform , and we illustrate its usefulness in real-world scenarios .

['churn modeling', 'incentive allocation', 'length of service', 'lifetime value', 'number-theoretic computations', 'retention campaign']


sustainable operation and management of data center chillers using temporal data mining motivation : data centers are a critical component of modern it infrastructure but are also among the worst environmental offenders through their increasing energy usage and the resulting large carbon footprints . efficient management of data centers , including power management , networking , and cooling infrastructure , is hence crucial to sustainability . in the absence of a ` first-principles ' approach to manage these complex components and their interactions , data-driven approaches have become attractive and tenable . results : we present a temporal data mining solution to model and optimize performance of data center chillers , a key component of the cooling infrastructure . it helps bridge raw , numeric , time-series information from sensor streams toward higher level characterizations of chiller behavior , suitable for a data center engineer . to aid in this transduction , temporal data streams are first encoded into a symbolic representation , next run-length encoded segments are mined to form frequent motifs in time series , and finally these metrics are evaluated by their contributions to sustainability . a key innovation in our application is the ability to intersperse `` do n't care '' transitions ( e.g. , transients ) in continuous-valued time series data , an advantage we inherit by the application of frequent episode mining to symbolized representations of numeric time series . our approach provides both qualitative and quantitative characterizations of the sensor streams to the data center engineer , to aid him in tuning chiller operating characteristics . this system is currently being prototyped for a data center managed by hp and experimental results from this application reveal the promise of our approach .

['chillers', 'clustering', 'data centers', 'frequent episodes', 'motifs', 'sustainability']

chillers NOUN pobj of
sustainability NOUN pobj to
chillers NOUN pobj of
motifs NOUN dobj form
sustainability NOUN pobj to

structured entity identification and document categorization : two tasks with one joint model traditionally , research in identifying structured entities in documents has proceeded independently of document categorization research . in this paper , we observe that these two tasks have much to gain from each other . apart from direct references to entities in a database , such as names of person entities , documents often also contain words that are correlated with discriminative entity attributes , such age-group and income-level of persons . this happens naturally in many enterprise domains such as crm , banking , etc. . then , entity identification , which is typically vulnerable against noise and incompleteness in direct references to entities in documents , can benefit from document categorization with respect to such attributes . in return , entity identification enables documents to be categorized according to different label-sets arising from entity attributes without requiring any supervision . in this paper , we propose a probabilistic generative model for joint entity identification and document categorization . we show how the parameters of the model can be estimated using an em algorithm in an unsupervised fashion . using extensive experiments over real and semi-synthetic data , we demonstrate that the two tasks can benefit immensely from each other when performed jointly using the proposed model .

['document categorization', 'entity identification', 'probabilistic generative model']


learning , indexing , and diagnosing network faults modern communication networks generate massive volume of operational event data , e.g. , alarm , alert , and metrics , which can be used by a network management system ( nms ) to diagnose potential faults . in this work , we introduce a new class of indexable fault signatures that encode temporal evolution of events generated by a network fault as well as topological relationships among the nodes where these events occur . we present an efficient learning algorithm to extract such fault signatures from noisy historical event data , and with the help of novel space-time indexing structures , we show how to perform efficient , online signature matching . we provide results from extensive experimental studies to explore the efficacy of our approach and point out potential applications of such signatures for many different types of networks including social and information networks .

['fault signature', 'network topology', 'online diagnosis']


bgp-lens : patterns and anomalies in internet routing updates the border gateway protocol ( bgp ) is one of the fundamental computer communication protocols . monitoring and mining bgp update messages can directly reveal the health and stability of internet routing . here we make two contributions : firstly we find patterns in bgp updates , like self-similarity , power-law and lognormal marginals ; secondly using these patterns , we find anomalies . specifically , we develop bgp-lens , an automated bgp updates analysis tool , that has three desirable properties : ( a ) it is effective , able to identify phenomena that would otherwise go unnoticed , such as a peculiar ` clothesline ' behavior or prolonged ` spikes ' that last as long as 8 hours ; ( b ) it is scalable , using algorithms are all linear on the number of time-ticks ; and ( c ) it is admin-friendly , giving useful leads for phenomenon of interest . we showcase the capabilities of bgp-lens by identifying surprising phenomena verified by syadmins , over a massive trace of bgp updates spanning 2 years , from the publicly available site datapository.net .

['anomalies', 'bgp monitoring', 'patterns', 'self-similarity']

patterns NOUN nsubj updates
patterns NOUN dobj using
anomalies NOUN dobj find

a family of dissimilarity measures between nodes generalizing both the shortest-path and the commute-time distances this work introduces a new family of link-based dissimilarity measures between nodes of a weighted directed graph . this measure , called the randomized shortest-path ( rsp ) dissimilarity , depends on a parameter î¸ and has the interesting property of reducing , on one end , to the standard shortest-path distance when î¸ is large and , on the other end , to the commute-time ( or resistance ) distance when î¸ is small ( near zero ) . intuitively , it corresponds to the expected cost incurred by a random walker in order to reach a destination node from a starting node while maintaining a constant entropy ( related to î¸ ) spread in the graph . the parameter î¸ is therefore biasing gradually the simple random walk on the graph towards the shortest-path policy . by adopting a statistical physics approach and computing a sum over all the possible paths ( discrete path integral ) , it is shown that the rsp dissimilarity from every node to a particular node of interest can be computed efficiently by solving two linear systems of n equations , where n is the number of nodes . on the other hand , the dissimilarity between every couple of nodes is obtained by inverting an n x n matrix . the proposed measure can be used for various graph mining tasks such as computing betweenness centrality , finding dense communities , etc , as shown in the experimental section .

['biased random walk', 'commute-time distance', 'graph mining', 'kernel on a graph', 'resistance distance', 'shortest path']


cross domain distribution adaptation via kernel mapping when labeled examples are limited and difficult to obtain , transfer learning employs knowledge from a source domain to improve learning accuracy in the target domain . however , the assumption made by existing approaches , that the marginal and conditional probabilities are directly related between source and target domains , has limited applicability in either the original space or its linear transformations . to solve this problem , we propose an adaptive kernel approach that maps the marginal distribution of target-domain and source-domain data into a common kernel space , and utilize a sample selection strategy to draw conditional probabilities between the two domains closer . we formally show that under the kernel-mapping space , the difference in distributions between the two domains is bounded ; and the prediction error of the proposed approach can also be bounded . experimental results demonstrate that the proposed method outperforms both traditional inductive classifiers and the state-of-the-art boosting-based transfer algorithms on most domains , including text categorization and web page ratings . in particular , it can achieve around 10 % higher accuracy than other approaches for the text categorization problem . the source code and datasets are available from the authors .

['domain transfer', 'ensemble', 'generalization bound', 'kernel']

kernel NOUN compound mapping
kernel NOUN compound approach
kernel NOUN compound space
kernel NOUN compound mapping

heterogeneous source consensus learning via decision propagation and negotiation nowadays , enormous amounts of data are continuously generated not only in massive scale , but also from different , sometimes conflicting , views . therefore , it is important to consolidate different concepts for intelligent decision making . for example , to predict the research areas of some people , the best results are usually achieved by combining and consolidating predictions obtained from the publication network , co-authorship network and the textual content of their publications . multiple supervised and unsupervised hypotheses can be drawn from these information sources , and negotiating their differences and consolidating decisions usually yields a much more accurate model due to the diversity and heterogeneity of these models . in this paper , we address the problem of `` consensus learning '' among competing hypotheses , which either rely on outside knowledge ( supervised learning ) or internal structure ( unsupervised clustering ) . we argue that consensus learning is an np-hard problem and thus propose to solve it by an efficient heuristic method . we construct a belief graph to first propagate predictions from supervised models to the unsupervised , and then negotiate and reach consensus among them . their final decision is further consolidated by calculating each model 's weight based on its degree of consistency with other models . experiments are conducted on 20 newsgroups data , cora research papers , dblp author-conference network , and yahoo ! movies datasets , and the results show that the proposed method improves the classification accuracy and the clustering quality measure ( nmi ) over the best base model by up to 10 % . furthermore , it runs in time proportional to the number of instances , which is very efficient for large scale data sets .

['classification', 'consensus', 'ensemble', 'heterogeneous sources']

consensus NOUN compound learning
consensus NOUN compound learning
consensus NOUN dobj reach
classification NOUN compound accuracy

an association analysis approach to biclustering the discovery of biclusters , which denote groups of items that show coherent values across a subset of all the transactions in a data set , is an important type of analysis performed on real-valued data sets in various domains , such as biology . several algorithms have been proposed to find different types of biclusters in such data sets . however , these algorithms are unable to search the space of all possible biclusters exhaustively . pattern mining algorithms in association analysis also essentially produce biclusters as their result , since the patterns consist of items that are supported by a subset of all the transactions . however , a major limitation of the numerous techniques developed in association analysis is that they are only able to analyze data sets with binary and\/or categorical variables , and their application to real-valued data sets often involves some lossy transformation such as discretization or binarization of the attributes . in this paper , we propose a novel association analysis framework for exhaustively and efficiently mining `` range support '' patterns from such a data set . on one hand , this framework reduces the loss of information incurred by the binarization - and discretization-based approaches , and on the other , it enables the exhaustive discovery of coherent biclusters . we compared the performance of our framework with two standard biclustering algorithms through the evaluation of the similarity of the cellular functions of the genes constituting the patterns\/biclusters derived by these algorithms from microarray data . these experiments show that the real-valued patterns discovered by our framework are better enriched by small biologically interesting functional classes . also , through specific examples , we demonstrate the ability of the rap framework to discover functionally enriched patterns that are not found by the commonly used biclustering algorithm isa . the source code and data sets used in this paper , as well as the supplementary material , are available at http:\/\/www.cs.umn.edu\/vk\/gaurav\/rap .

['association analysis', 'biclustering', 'functional modules', 'microarray data', 'range support', 'real-valued data']

association analysis NOUN compound approach
biclustering VERB pcomp to
biclustering NOUN compound algorithms
biclustering NOUN amod isa

discrimination-aware data mining in the context of civil rights law , discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority , without regard to individual merit . rules extracted from databases by data mining techniques , such as classification or association rules , when used for decision tasks such as benefit or credit approval , can be discriminatory in the above sense . in this paper , the notion of discriminatory classification rules is introduced and studied . providing a guarantee of non-discrimination is shown to be a non trivial task . a naive approach , like taking away all discriminatory attributes , is shown to be not enough when other background knowledge is available . our approach leads to a precise formulation of the redlining problem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowledge . an empirical assessment of the results on the german credit dataset is also provided .

['classification rules', 'discrimination']

discrimination NOUN nsubj refers
discrimination NOUN pobj of

fastanova : an efficient algorithm for genome-wide association study studying the association between quantitative phenotype ( such as height or weight ) and single nucleotide polymorphisms ( snps ) is an important problem in biology . to understand underlying mechanisms of complex phenotypes , it is often necessary to consider joint genetic effects across multiple snps . anova ( analysis of variance ) test is routinely used in association study . important findings from studying gene-gene ( snp-pair ) interactions are appearing in the literature . however , the number of snps can be up to millions . evaluating joint effects of snps is a challenging task even for snp-pairs . moreover , with large number of snps correlated , permutation procedure is preferred over simple bonferroni correction for properly controlling family-wise error rate and retaining mapping power , which dramatically increases the computational cost of association study . in this paper , we study the problem of finding snp-pairs that have significant associations with a given quantitative phenotype . we propose an efficient algorithm , fastanova , for performing anova tests on snp-pairs in a batch mode , which also supports large permutation test . we derive an upper bound of snp-pair anova test , which can be expressed as the sum of two terms . the first term is based on single-snp anova test . the second term is based on the snps and independent of any phenotype permutation . furthermore , snp-pairs can be organized into groups , each of which shares a common upper bound . this allows for maximum reuse of intermediate computation , efficient upper bound estimation , and effective snp-pair pruning . consequently , fastanova only needs to perform the anova test on a small number of candidate snp-pairs without the risk of missing any significant ones . extensive experiments demonstrate that fastanova is orders of magnitude faster than the brute-force implementation of anova tests on all snp pairs .

['anova test', 'association study']


on the tradeoff between privacy and utility in data publishing in data publishing , anonymization techniques such as generalization and bucketization have been designed to provide privacy protection . in the meanwhile , they reduce the utility of the data . it is important to consider the tradeoff between privacy and utility . in a paper that appeared in kdd 2008 , brickell and shmatikov proposed an evaluation methodology by comparing privacy gain with utility gain resulted from anonymizing the data , and concluded that `` even modest privacy gains require almost complete destruction of the data-mining utility '' . this conclusion seems to undermine existing work on data anonymization . in this paper , we analyze the fundamental characteristics of privacy and utility , and show that it is inappropriate to directly compare privacy with utility . we then observe that the privacy-utility tradeoff in data publishing is similar to the risk-return tradeoff in financial investment , and propose an integrated framework for considering privacy-utility tradeoff , borrowing concepts from the modern portfolio theory for financial investment . finally , we evaluate our methodology on the adult dataset from the uci machine learning repository . our results clarify several common misconceptions about data utility and provide data publishers useful guidelines on choosing the right tradeoff between privacy and utility .

['anonymity', 'data publishing', 'privacy']

privacy NOUN compound protection
privacy NOUN pobj between
privacy NOUN compound gain
privacy NOUN compound gains
privacy NOUN pobj of
privacy NOUN dobj compare
privacy NOUN compound utility
privacy NOUN compound utility
privacy NOUN pobj between

user grouping behavior in online forums online forums represent one type of social media that is particularly rich for studying human behavior in information seeking and diffusing . the way users join communities is a reflection of the changing and expanding of their interests toward information . in this paper , we study the patterns of user participation behavior , and the feature factors that influence such behavior on different forum datasets . we find that , despite the relative randomness and lesser commitment of structural relationships in online forums , users ' community joining behaviors display some strong regularities . one particularly interesting observation is that the very weak relationships between users defined by online replies have similar diffusion curves as those of real friendships or co-authorships . we build social selection models , bipartite markov random field ( bimrf ) , to quantitatively evaluate the prediction performance of those feature factors and their relationships . using these models , we show that some features carry supplementary information , and the effectiveness of different features vary in different types of forums . moreover , the results of bimrf with two-star configurations suggest that the feature of user similarity defined by frequency of communication or number of common friends is inadequate to predict grouping behavior , but adding node-level features can improve the fit of the model .

['information diffusion', 'online forums', 'social networks', 'social selection model']


towards nic-based intrusion detection we present and evaluate a nic-based network intrusion detection system . intrusion detection at the nic makes the system potentially tamper-proof and is naturally extensible to work in a distributed setting . simple anomaly detection and signature detection based models have been implemented on the nic firmware , which has its own processor and memory . we empirically evaluate such systems from the perspective of quality and performance ( bandwidth of acceptable messages ) under varying conditions of host load . the preliminary results we obtain are very encouraging and lead us to believe that such nic-based security schemes could very well be a crucial part of next generation network security systems .

['data mining', 'network interface cards', 'network intrusion detection', 'network security', 'nics']


event detection from evolution of click-through data previous efforts on event detection from the web have focused primarily on web content and structure data ignoring the rich collection of web log data . in this paper , we propose the first approach to detect events from the click-through data , which is the log data of web search engines . the intuition behind event detection from click-through data is that such data is often event-driven and each event can be represented as a set ofquery-page pairs that are not only semantically similar but also have similar evolution pattern over time . given the click-through data , in our proposed approach , we first segment it into a sequence of bipartite graphs based on theuser-defined time granularity . next , the sequence of bipartite graphs is represented as a vector-based graph , which records the semantic and evolutionary relationships between queries and pages . after that , the vector-based graph is transformed into its dual graph , where each node is a query-page pair that will be used to represent real world events . then , the problem of event detection is equivalent to the problem of clustering the dual graph of the vector-based graph . the clustering process is based on a two-phase graph cut algorithm . in the first phase , query-page pairs are clustered based on thesemantic-based similarity such that each cluster in the result corresponds to a specific topic . in the second phase , query-page pairs related to the same topic are further clustered based on the evolution pattern-based similarity such that each cluster is expected to represent a specific event under the specific topic . experiments with real click-through data collected from a commercial web search engine show that the proposed approach produces high quality results .

['click-through data', 'dynamic web', 'event detection', 'evolution pattern']


catching the drift : learning broad matches from clickthrough data identifying similar keywords , known as broad matches , is an important task in online advertising that has become a standard feature on all major keyword advertising platforms . effective broad matching leads to improvements in both relevance and monetization , while increasing advertisers ' reach and making campaign management easier . in this paper , we present a learning-based approach to broad matching that is based on exploiting implicit feedback in the form of advertisement clickthrough logs . our method can utilize arbitrary similarity functions by incorporating them as features . we present an online learning algorithm , amnesiac averaged perceptron , that is highly efficient yet able to quickly adjust to the rapidly-changing distributions of bidded keywords , advertisements and user behavior . experimental results obtained from ( 1 ) historical logs and ( 2 ) live trials on a large-scale advertising platform demonstrate the effectiveness of the proposed algorithm and the overall success of our approach in identifying high-quality broad match mappings .

['information search and retrieval', 'keyword similarity', 'keyword-based advertising', 'on-line information services', 'online learning']


olap on search logs : an infrastructure supporting data-driven applications in search engines search logs , which contain rich and up-to-date information about users ' needs and preferences , have become a critical data source for search engines . recently , more and more data-driven applications are being developed in search engines based on search logs , such as query suggestion , keyword bidding , and dissatisfactory query analysis . in this paper , by observing that many data-driven applications in search engines highly rely on online mining of search logs , we develop an olap system on search logs which serves as an infrastructure supporting various data-driven applications . an empirical study using real data of over two billion query sessions demonstrates the usefulness and feasibility of our design .

['olap', 'query session', 'search log', 'suffix tree']

olap NOUN nsubj become
olap NOUN compound system

incorporating site-level knowledge for incremental crawling of web forums : a list-wise strategy we study in this paper the problem of incremental crawling of web forums , which is a very fundamental yet challenging step in many web applications . traditional approaches mainly focus on scheduling the revisiting strategy of each individual page . however , simply assigning different weights for different individual pages is usually inefficient in crawling forum sites because of the different characteristics between forum sites and general websites . instead of treating each individual page independently , we propose a list-wise strategy by taking into account the site-level knowledge . such site-level knowledge is mined through reconstructing the linking structure , called sitemap , for a given forum site . with the sitemap , posts from the same thread but distributed on various pages can be concatenated according to their timestamps . after that , for each thread , we employ a regression model to predict the time when the next post arrives . based on this model , we develop an efficient crawler which is 260 % faster than some state-of-the-art methods in terms of fetching new generated content ; and meanwhile our crawler also ensure a high coverage ratio . experimental results show promising performance of coverage , bandwidth utilization , and timeliness of our crawler on 18 various forums .

['incremental crawling', 'sitemap', 'web forum']

sitemap NOUN oprd called
sitemap NOUN pobj with

towards scalable support vector machines using squashing

['boosting', 'database applications', 'scalability', 'squashing', 'support vector machines']

squashing VERB xcomp using

a theoretical framework for learning from a pool of disparate data sources many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task . for example , aiming towards the design of an automated diagnostic tool for some disease , one may wish to integrate data gathered in many different hospitals . a major obstacle to such endeavors is that different data sources may vary considerably in the way they choose to represent related data . in practice , the problem is usually solved by a manual construction of semantic mappings and translations between the different sources . recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations . in this work we propose a theoretical framework for making classification predictions from a collection of different data sources , without creating explicit translations between them . our framework allows a precise mathematical analysis of the complexity of such tasks , and it provides a tool for the development and comparison of different learning algorithms . our main objective , at this stage , is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework .

['probabilistic algorithms']


mining intrusion detection alarms for actionable knowledge in response to attacks against enterprise networks , administrators increasingly deploy intrusion detection systems . these systems monitor hosts , networks , and other resources for signs of security violations . the use of intrusion detection has given rise to another difficult problem , namely the handling of a generally large number of alarms . in this paper , we mine historical alarms to learn how future alarms can be handled more efficiently . first , we investigate episode rules with respect to their suitability in this approach . we report the difficulties encountered and the unexpected insights gained . in addition , we introduce a new conceptual clustering technique , and use it in extensive experiments with real-world data to show that intrusion detection alarms can be handled efficiently by using previously mined knowledge .

['alarm investigation', 'conceptual clustering', 'episode rules', 'intrusion detection']


partitioned logistic regression for spam filtering naive bayes and logistic regression perform well in different regimes . while the former is a very simple generative model which is efficient to train and performs well empirically in many applications , the latter is a discriminative model which often achieves better accuracy and can be shown to outperform naive bayes asymptotically . in this paper , we propose a novel hybrid model , partitioned logistic regression , which has several advantages over both naive bayes and logistic regression . this model separates the original feature space into several disjoint feature groups . individual models on these groups of features are learned using logistic regression and their predictions are combined using the naive bayes principle to produce a robust final estimation . we show that our model is better both theoretically and empirically . in addition , when applying it in a practical application , email spam filtering , it improves the normalized auc score at 10 % false-positive rate by 28.8 % and 23.6 % compared to naive bayes and logistic regression , when using the exact same training examples .

['email spam filtering', 'learning', 'logistic regression', 'naive bayes']


privacy preserving regression modelling via distributed computation reluctance of data owners to share their possibly confidential or proprietary data with others who own related databases is a serious impediment to conducting a mutually beneficial data mining analysis . we address the case of vertically partitioned data -- multiple data owners\/agencies each possess a few attributes of every data record . we focus on the case of the agencies wanting to conduct a linear regression analysis with complete records without disclosing values of their own attributes . this paper describes an algorithm that enables such agencies to compute the exact regression coefficients of the global regression equation and also perform some basic goodness-of-fit diagnostics while protecting the confidentiality of their data . in more general settings beyond the privacy scenario , this algorithm can also be viewed as method for the distributed computation for regression analyses .

['data confidentiality', 'data integration', 'learning', 'regression', 'secure multi-party computation']

regression NOUN compound analysis
regression NOUN compound coefficients
regression NOUN compound equation
regression NOUN compound analyses

angle-based outlier detection in high-dimensional data detecting outliers in a large set of data objects is a major data mining task aiming at finding different mechanisms responsible for different groups of objects in a data set . all existing approaches , however , are based on an assessment of distances ( sometimes indirectly by assuming certain distributions ) in the full-dimensional euclidean data space . in high-dimensional data , these approaches are bound to deteriorate due to the notorious `` curse of dimensionality '' . in this paper , we propose a novel approach named abod ( angle-based outlier detection ) and some variants assessing the variance in the angles between the difference vectors of a point to the other points . this way , the effects of the `` curse of dimensionality '' are alleviated compared to purely distance-based approaches . a main advantage of our new approach is that our method does not rely on any parameter selection influencing the quality of the achieved ranking . in a thorough experimental evaluation , we compare abod to the well-established distance-based method lof for various artificial and a real world data set and show abod to perform especially well on high-dimensional data .

['angle-based', 'high-dimensional', 'outlier detection']

angle-based NOUN amod detection

towards exploratory test instance specific algorithms for high dimensional classification in an interactive classification application , a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar records . such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification methods . in this paper , we propose the subspace decision path method which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most significant classification characteristics for a given test instance are revealed . in addition , the sd-path method can provide enormous interpretability by constructing views of the data in which the different classes are clearly separated out . even in cases where the classification behavior of the test instance is ambiguous , the sd-path method provides a diagnostic understanding of the characteristics which result in this ambiguity . therefore , this method combines the abilities of the human and the computer in creating an effective diagnostic tool for instance-centered high dimensional classification .

['classification', 'database applications', 'visual data mining']

classification NOUN pobj for
classification NOUN compound application
classification NOUN compound behavior
classification NOUN compound methods
classification NOUN compound characteristics
classification NOUN compound behavior
classification NOUN pobj for

algorithms for discovering bucket orders from data ordering and ranking items of different types are important tasks in various applications , such as query processing and scientific data mining . a total order for the items can be misleading , since there are groups of items that have practically equal ranks . we consider bucket orders , i.e. , total orders with ties . they can be used to capture the essential order information without overfitting the data : they form a useful concept class between total orders and arbitrary partial orders . we address the question of finding a bucket order for a set of items , given pairwise precedence information between the items . we also discuss methods for computing the pairwise precedence data . we describe simple and efficient algorithms for finding good bucket orders . several of the algorithms have a provable approximation guarantee , and they scale well to large datasets . we provide experimental results on artificial and a real data that show the usefulness of bucket orders and demonstrate the accuracy and efficiency of the algorithms .

['bucket order', 'ordering', 'partial order', 'ranking']


evaluating the novelty of text-mined rules using lexical knowledge in this paper , we present a new method of estimating the novelty of rules discovered by data-mining methods using wordnet , a lexical knowledge-base of english words . we assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance , more is the novelty of the rule . the novelty of rules extracted by the discotex text-mining system on amazon.com book descriptions were evaluated by both human subjects and by our algorithm . by computing correlation coefficients between pairs of human ratings and between human and automatic ratings , we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another . @text mining

['interesting rules', 'knowledge hierarchy', 'novelty', 'semantic distance', 'wordnet']

novelty NOUN dobj estimating
wordnet NOUN dobj using
novelty NOUN dobj assess
novelty NOUN attr is
novelty NOUN nsubjpass evaluated
novelty NOUN compound measure

constant-factor approximation algorithms for identifying dynamic communities we propose two approximation algorithms for identifying communities in dynamic social networks . communities are intuitively characterized as `` unusually densely knit '' subsets of a social network . this notion becomes more problematic if the social interactions change over time . aggregating social networks over time can radically misrepresent the existing and changing community structure . recently , we have proposed an optimization-based framework for modeling dynamic community structure . also , we have proposed an algorithm for finding such structure based on maximum weight bipartite matching . in this paper , we analyze its performance guarantee for a special case where all actors can be observed at all times . in such instances , we show that the algorithm is a small constant factor approximation of the optimum . we use a similar idea to design an approximation algorithm for the general case where some individuals are possibly unobserved at times , and to show that the approximation factor increases twofold but remains a constant regardless of the input size . this is the first algorithm for inferring communities in dynamic networks with a provable approximation guarantee . we demonstrate the general algorithm on real data sets . the results confirm the efficiency and effectiveness of the algorithm in identifying dynamic communities .

['approximation algorithms', 'community identification', 'dynamic social networks', 'general']

general ADJ amod case
general ADJ amod algorithm

paintingclass : interactive construction , visualization and exploration of decision trees decision trees are commonly used for classification . we propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery , because visualizing the decision tree can reveal much valuable information in the data . we introduce paintingclass , a system for interactive construction , visualization and exploration of decision trees . paintingclass provides an intuitive layout and convenient navigation of the decision tree . paintingclass also provides the user the means to interactively construct the decision tree . each node in the decision tree is displayed as a visual projection of the data . through actual examples and comparison with other classification methods , we show that the user can effectively use paintingclass to construct a decision tree and explore the decision tree to gain additional knowledge .

['classification', 'decision trees', 'information visualization', 'interactive visualization', 'visual data mining']

classification NOUN pobj for
classification NOUN pobj for
classification NOUN compound methods

maximum profit mining and its application in software development while most software defects ( i.e. , bugs ) are corrected and tested as part of the lengthy software development cycle , enterprise software vendors often have to release software products before all reported defects are corrected , due to deadlines and limited resources . a small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost . in this paper , we develop an escalation prediction ( ep ) system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit . more specifically , we first describe a simple and general framework to convert the maximum net profit problem to cost-sensitive learning . we then apply and compare several well-known cost-sensitive learning approaches for ep . our experiments suggest that the cost-sensitive decision tree is the best method for producing the highest positive net profit and comprehensible results . the ep system has been deployed successfully in the product group of an enterprise software vendor .

['cost-sensitive learning', 'data mining', 'escalation prediction', 'learning']

learning NOUN pobj to
learning NOUN compound approaches

land cover change detection : a case study the study of land cover change is an important problem in the earth science domain because of its impacts on local climate , radiation balance , biogeochemistry , hydrology , and the diversity and abundance of terrestrial species . most well-known change detection techniques from statistics , signal processing and control theory are not well-suited for the massive high-dimensional spatio-temporal data sets from earth science due to limitations such as high computational complexity and the inability to take advantage of seasonality and spatio-temporal autocorrelation inherent in earth science data . in our work , we seek to address these challenges with new change detection techniques that are based on data mining approaches . specifically , in this paper we have performed a case study for a new change detection technique for the land cover change detection problem . we study land cover change in the state of california , focusing on the san francisco bay area and perform an extended study on the entire state . we also perform a comparative evaluation on forests in the entire state . these results demonstrate the utility of data mining techniques for the land cover change detection problem .

['change detection', 'land cover', 'land use', 'time series']


depth first generation of long patterns

['association rules']


unifying dependent clustering and disparate clustering for non-homogeneous data modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relationships between these entities . we present an approach to cluster such non-homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints . unlike prior work that views constraints as boolean criteria , we present a formulation that allows constraints to be satisfied or violated in a smooth manner . this enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function . we present results on both synthetic data as well as several real-world datasets .

['clustering', 'contingency tables', 'learning', 'multi-criteria optimization.', 'relational clustering']

clustering NOUN dobj impose
clustering NOUN compound constraints
clustering NOUN dobj achieve
clustering NOUN dobj disparate

beyond heuristics : learning to classify vulnerabilities and predict exploits the security demands on modern system administration are enormous and getting worse . chief among these demands , administrators must monitor the continual ongoing disclosure of software vulnerabilities that have the potential to compromise their systems in some way . such vulnerabilities include buffer overflow errors , improperly validated inputs , and other unanticipated attack modalities . in 2008 , over 7,400 new vulnerabilities were disclosed -- well over 100 per week . while no enterprise is affected by all of these disclosures , administrators commonly face many outstanding vulnerabilities across the software systems they manage . vulnerabilities can be addressed by patches , reconfigurations , and other workarounds ; however , these actions may incur down-time or unforeseen side-effects . thus , a key question for systems administrators is which vulnerabilities to prioritize . from publicly available databases that document past vulnerabilities , we show how to train classifiers that predict whether and how soon a vulnerability is likely to be exploited . as input , our classifiers operate on high dimensional feature vectors that we extract from the text fields , time stamps , cross references , and other entries in existing vulnerability disclosure reports . compared to current industry-standard heuristics based on expert knowledge and static formulas , our classifiers predict much more accurately whether and how soon individual vulnerabilities are likely to be exploited .

['exploits', 'security and protection', 'security and protection', 'supervised learning', 'svm', 'vulnerabilities']

vulnerabilities NOUN dobj classify
exploits VERB dobj predict
vulnerabilities NOUN pobj of
vulnerabilities NOUN nsubj include
vulnerabilities NOUN nsubjpass disclosed
vulnerabilities NOUN dobj face
vulnerabilities NOUN nsubjpass addressed
vulnerabilities VERB ccomp is
vulnerabilities NOUN pobj past
vulnerabilities NOUN nsubj are

extracting temporal signatures for comprehending systems biology models systems biology has made massive strides in recent years , with capabilities to model complex systems including cell division , stress response , energy metabolism , and signaling pathways . concomitant with their improved modeling capabilities , however , such biochemical network models have also become notoriously complex for humans to comprehend . we propose network comprehension as a key problem for the kdd community , where the goal is to create explainable representations of complex biological networks . we formulate this problem as one of extracting temporal signatures from multi-variate time series data , where the signatures are composed of ordinal comparisons between time series components . we show how such signatures can be inferred by formulating the data mining problem as one of feature selection in rank-order space . we propose five new feature selection strategies for rank-order space and assess their selective superiorities . experimental results on budding yeast cell cycle models demonstrate compelling results comparable to human interpretations of the cell cycle .

['biological networks', 'feature selection', 'rank-order spaces', 'systems biology', 'temporal signatures']


diagnosing memory leaks using graph mining on heap dumps memory leaks are caused by software programs that prevent the reclamation of memory that is no longer in use . they can cause significant slowdowns , exhaustion of available storage space and , eventually , application crashes . detecting memory leaks is challenging because real-world applications are built on multiple layers of software frameworks , making it difficult for a developer to know whether observed references to objects are legitimate or the cause of a leak . we present a graph mining solution to this problem wherein we analyze heap dumps to automatically identify subgraphs which could represent potential memory leak sources . although heap dumps are commonly analyzed in existing heap profiling tools , our work is the first to apply a graph grammar mining solution to this problem . unlike classical graph mining work , we show that it suffices to mine the dominator tree of the heap dump , which is significantly smaller than the underlying graph . our approach identifies not just leaking candidates and their structure , but also provides aggregate information about the access path to the leaks . we demonstrate several synthetic as well as real-world examples of heap dumps for which our approach provides more insight into the problem than state-of-the-art tools such as eclipse 's mat .

['dominator tree', 'graph grammars', 'graph mining', 'heap profiling', 'memory leaks']


designing efficient cascaded classifiers : tradeoff between accuracy and cost we propose a method to train a cascade of classifiers by simultaneously optimizing all its stages . the approach relies on the idea of optimizing soft cascades . in particular , instead of optimizing a deterministic hard cascade , we optimize a stochastic soft cascade where each stage accepts or rejects samples according to a probability distribution induced by the previous stage-specific classifier . the overall system accuracy is maximized while explicitly controlling the expected cost for feature acquisition . experimental results on three clinically relevant problems show the effectiveness of our proposed approach in achieving the desired tradeoff between accuracy and feature acquisition cost .

['accuracy vs cost', 'cascade design', 'cost sensitive learning']


inferring networks of diffusion and influence information diffusion and virus propagation are fundamental processes talking place in networks . while it is often possible to directly observe when nodes become infected , observing individual transmissions ( i.e. , who infects whom or who influences whom ) is typically very difficult . furthermore , in many applications , the underlying network over which the diffusions and propagations spread is actually unobserved . we tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate . given the times when nodes adopt pieces of information or become infected , we identify the optimal network that best explains the observed infection times . since the optimization problem is np-hard to solve exactly , we develop an efficient approximation algorithm that scales to large datasets and in practice gives provably near-optimal performance . we demonstrate the effectiveness of our approach by tracing information cascades in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space . we find that the diffusion network of news tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the web . these sites tend to have stable circles of influence with more general news media sites acting as connectors between them .

['blogs', 'information cascades', 'meme-tracking', 'networks of diffusion', 'news media', 'social networks']

blogs NOUN pobj of

ensemble pruning via individual contribution ordering an ensemble is a set of learned models that make decisions collectively . although an ensemble is usually more accurate than a single learner , existing ensemble methods often tend to construct unnecessarily large ensembles , which increases the memory consumption and computational cost . ensemble pruning tackles this problem by selecting a subset of ensemble members to form subensembles that are subject to less resource consumption and response time with accuracy that is similar to or better than the original ensemble . in this paper , we analyze the accuracy\/diversity trade-off and prove that classifiers that are more accurate and make more predictions in the minority group are more important for subensemble construction . based on the gained insights , a heuristic metric that considers both accuracy and diversity is proposed to explicitly evaluate each individual classifier 's contribution to the whole ensemble . by incorporating ensemble members in decreasing order of their contributions , subensembles are formed such that users can select the top $ p $ percent of ensemble members , depending on their resource availability and tolerable waiting time , for predictions . experimental results on 26 uci data sets show that subensembles formed by the proposed epic ( ensemble pruning via individual contribution ordering ) algorithm outperform the original ensemble and a state-of-the-art ensemble pruning method , orientation ordering ( oo ) .

['ensemble learning', 'ensemble pruning']


a cross-collection mixture model for comparative text mining in this paper , we define and study a novel text mining problem , which we refer to as comparative text mining ( ctm ) . given a set of comparable text collections , the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme . this general problem subsumes many interesting applications , including business intelligence and opinion summarization . we propose a generative probabilistic mixture model for comparative text mining . the model simultaneously performs cross-collection clustering and within-collection clustering , and can be applied to an arbitrary set of comparable text collections . the model can be estimated efficiently using the expectation-maximization ( em ) algorithm . we evaluate the model on two different text data sets ( i.e. , a news article data set and a laptop review data set ) , and compare it with a baseline clustering method also based on a mixture model . experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model .

['clustering', 'comparative text mining', 'information search and retrieval', 'mixture models']

clustering NOUN dobj performs
clustering NOUN conj clustering
clustering NOUN compound method

latent aspect rating analysis on review text data : a rating regression approach in this paper , we define and study a new opinionated text data analysis problem called latent aspect rating analysis ( lara ) , which aims at analyzing opinions expressed about an entity in an online review at the level of topical aspects to discover each individual reviewer 's latent opinion on each aspect as well as the relative emphasis on different aspects when forming the overall judgment of the entity . we propose a novel probabilistic rating regression model to solve this new text mining problem in a general way . empirical experiments on a hotel review data set show that the proposed latent rating regression model can effectively solve the problem of lara , and that the detailed analysis of opinions at the level of topical aspects enabled by the proposed model can support a wide range of application tasks , such as aspect opinion summarization , entity ranking based on aspect ratings , and analysis of reviewers rating behavior .

['information search and retrieval']


mining closed relational graphs with connectivity constraints relational graphs are widely used in modeling large scale networks such as biological networks and social networks . in this kind of graph , connectivity becomes critical in identifying highly associated groups and clusters . in this paper , we investigate the issues of mining closed frequent graphs with connectivity constraints in massive relational graphs where each graph has around 10k nodes and 1m edges . we adopt the concept of edge connectivity and apply the results from graph theory , to speed up the mining process . two approaches are developed to handle different mining requests : closecut , a pattern-growth approach , and splat , a pattern-reduction approach . we have applied these methods in biological datasets and found the discovered patterns interesting .

['closed pattern', 'connectivity', 'graph']

connectivity NOUN compound constraints
graph NOUN pobj of
connectivity NOUN nsubj becomes
connectivity NOUN compound constraints
graph NOUN nsubj has
connectivity NOUN pobj of
graph NOUN compound theory

learning to rank networked entities several algorithms have been proposed to learn to rank entities modeled as feature vectors , based on relevance feedback . however , these algorithms do not model network connections or relations between entities . meanwhile , pagerank and variants find the stationary distribution of a reasonable but arbitrary markov walk over a network , but do not learn from relevance feedback . we present a framework for ranking networked entities based on markov walks with parameterized conductance values associated with the network edges . we propose two flavors of conductance learning problems in our framework . in the first setting , relevance feedback comparing node-pairs hints that the user has one or more hidden preferred communities with large edge conductance , and the algorithm must discover these communities . we present a constrained maximum entropy network flow formulation whose dual can be solved efficiently using a cutting-plane approach and a quasi-newton optimizer . in the second setting , edges have types , and relevance feedback hints that each edge type has a potentially different conductance , but this is fixed across the whole network . our algorithm learns the conductances using an approximate newton method .

['conductance matrix', 'maximum entropy', 'network flow', 'pagerank']

pagerank NOUN nsubj find

data mining to predict and prevent errors in health insurance claims processing health insurance costs across the world have increased alarmingly in recent years . a major cause of this increase are payment errors made by the insurance companies while processing claims . these errors often result in extra administrative effort to re-process ( or rework ) the claim which accounts for up to 30 % of the administrative staff in a typical health insurer . we describe a system that helps reduce these errors using machine learning techniques by predicting claims that will need to be reworked , generating explanations to help the auditors correct these claims , and experiment with feature selection , concept drift , and active learning to collect feedback from the auditors to improve over time . we describe our framework , problem formulation , evaluation metrics , and experimental results on claims data from a large us health insurer . we show that our system results in an order of magnitude better precision ( hit rate ) over existing approaches which is accurate enough to potentially result in over $ 15-25 million in savings for a typical insurer . we also describe interesting research problems in this domain as well as design choices made to make the system easily deployable across health insurance companies .

['claim rework identification', 'decision support', 'health insurance claims', 'machine learning']


understandable models of music collections based on exhaustive feature generation with temporal statistics data mining in large collections of polyphonic music has recently received increasing interest by companies along with the advent of commercial online distribution of music . important applications include the categorization of songs into genres and the recommendation of songs according to musical similarity and the customer 's musical preferences . modeling genre or timbre of polyphonic music is at the core of these tasks and has been recognized as a difficult problem . many audio features have been proposed , but they do not provide easily understandable descriptions of music . they do not explain why a genre was chosen or in which way one song is similar to another . we present an approach that combines large scale feature generation with meta learning techniques to obtain meaningful features for musical similarity . we perform exhaustive feature generation based on temporal statistics and train regression models to summarize a subset of these features into a single descriptor of a particular notion of music . using several such models we produce a concise semantic description of each song . genre classification models based on these semantic features are shown to be better understandable and almost as accurate as traditional methods .

['feature generation', 'genre classification', 'logistic regression', 'meta learning', 'miscellaneous', 'music mining']


pervasive parallelism in data mining : dataflow solution to co-clustering large and sparse netflix data all netflix prize algorithms proposed so far are prohibitively costly for large-scale production systems . in this paper , we describe an efficient dataflow implementation of a collaborative filtering ( cf ) solution to the netflix prize problem ( 1 ) based on weighted coclustering ( 5 ) . the dataflow library we use facilitates the development of sophisticated parallel programs designed to fully utilize commodity multicore hardware , while hiding traditional difficulties such as queuing , threading , memory management , and deadlocks . the dataflow cf implementation first compresses the large , sparse training dataset into co-clusters . then it generates recommendations by combining the average ratings of the co-clusters with the biases of the users and movies . when configured to identify 20x20 co-clusters in the netflix training dataset , the implementation predicted over 100 million ratings in 16.31 minutes and achieved an rmse of 0.88846 without any fine-tuning or domain knowledge . this is an effective real-time prediction runtime of 9.7 us per rating which is far superior to previously reported results . moreover , the implemented co-clustering framework supports a wide variety of other large-scale data mining applications and forms the basis for predictive modeling on large , dyadic datasets ( 4 , 7 ) .

['co-clustering', 'dataflow', 'predictive modeling', 'scalability']

dataflow ADJ amod solution
dataflow ADJ amod implementation
dataflow ADJ amod library
dataflow ADJ amod cf

learning patterns in the dynamics of biological networks our dynamic graph-based relational mining approach has been developed to learn structural patterns in biological networks as they change over time . the analysis of dynamic networks is important not only to understand life at the system-level , but also to discover novel patterns in other structural data . most current graph-based data mining approaches overlook dynamic features of biological networks , because they are focused on only static graphs . our approach analyzes a sequence of graphs and discovers rules that capture the changes that occur between pairs of graphs in the sequence . these rules represent the graph rewrite rules that the first graph must go through to be isomorphic to the second graph . then , our approach feeds the graph rewrite rules into a machine learning system that learns general transformation rules describing the types of changes that occur for a class of dynamic biological networks . the discovered graph-rewriting rules show how biological networks change over time , and the transformation rules show the repeated patterns in the structural changes . in this paper , we apply our approach to biological networks to evaluate our approach and to understand how the biosystems change over time . we evaluate our results using coverage and prediction metrics , and compare to biological literature .

['biological network', 'dynamic network analysis', 'graph mining', 'graph rewriting rule', 'learning']

learning VERB advcl developed
learning NOUN compound system

improving classification accuracy using automatically extracted training data classification is a core task in knowledge discovery and data mining , and there has been substantial research effort in developing sophisticated classification models . in a parallel thread , recent work from the nlp community suggests that for tasks such as natural language disambiguation even a simple algorithm can outperform a sophisticated one , if it is provided with large quantities of high quality training data . in those applications , training data occurs naturally in text corpora , and high quality training data sets running into billions of words have been reportedly used . we explore how we can apply the lessons from the nlp community to kdd tasks . specifically , we investigate how to identify data sources that can yield training data at low cost and study whether the quantity of the automatically extracted training data can compensate for its lower quality . we carry out this investigation for the specific task of inferring whether a search query has commercial intent . we mine toolbar and click logs to extract queries from sites that are predominantly commercial ( e.g. , amazon ) and non-commercial ( e.g. , wikipedia ) . we compare the accuracy obtained using such training data against manually labeled training data . our results show that we can have large accuracy gains using automatically extracted training data at much lower cost .

['automatically labeled data', 'classification', 'query intent']

classification NOUN dobj using
classification NOUN compound models

exploring social tagging graph for web object classification this paper studies web object classification problem with the novel exploration of social tags . automatically classifying web objects into manageable semantic categories has long been a fundamental preprocess for indexing , browsing , searching , and mining these objects . the explosive growth of heterogeneous web objects , especially non-textual objects such as products , pictures , and videos , has made the problem of web classification increasingly challenging . such objects often suffer from a lack of easy-extractable features with semantic information , interconnections between each other , as well as training examples with category labels . in this paper , we explore the social tagging data to bridge this gap . we cast web object classification problem as an optimization problem on a graph of objects and tags . we then propose an efficient algorithm which not only utilizes social tags as enriched semantic features for the objects , but also infers the categories of unlabeled objects from both homogeneous and heterogeneous labeled objects , through the implicit connection of social tags . experiment results show that the exploration of social tags effectively boosts web object classification . our algorithm significantly outperforms the state-of-the-art of general classification methods .

['general', 'optimization', 'social tagging', 'web classification']

optimization NOUN compound problem
general ADJ amod methods

a scalable two-stage approach for a class of dimensionality reduction techniques dimensionality reduction plays an important role in many data mining applications involving high-dimensional data . many existing dimensionality reduction techniques can be formulated as a generalized eigenvalue problem , which does not scale to large-size problems . prior work transforms the generalized eigenvalue problem into an equivalent least squares formulation , which can then be solved efficiently . however , the equivalence relationship only holds under certain assumptions without regularization , which severely limits their applicability in practice . in this paper , an efficient two-stage approach is proposed to solve a class of dimensionality reduction techniques , including canonical correlation analysis , orthonormal partial least squares , linear discriminant analysis , and hypergraph spectral learning . the proposed two-stage approach scales linearly in terms of both the sample size and data dimensionality . the main contributions of this paper include ( 1 ) we rigorously establish the equivalence relationship between the proposed two-stage approach and the original formulation without any assumption ; and ( 2 ) we show that the equivalence relationship still holds in the regularization setting . we have conducted extensive experiments using both synthetic and real-world data sets . our experimental results confirm the equivalence relationship established in this paper . results also demonstrate the scalability of the proposed two-stage approach .

['dimensionality reduction', 'generalized eigenvalue problem', 'least squares', 'regularization', 'scalability']

regularization NOUN pobj without
regularization NOUN compound setting
scalability NOUN dobj demonstrate

learning incoherent sparse and low-rank patterns from multiple tasks we consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks . our approach is based on a linear multi-task learning formulation , in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint , respectively . this formulation is non-convex ; we convert it into its convex surrogate , which can be routinely solved via semidefinite programming for small-size problems . we propose to employ the general projected gradient scheme to efficiently solve such a convex surrogate ; however , in the optimization formulation , the objective function is non-differentiable and the feasible domain is non-trivial . we present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme . the computation of projected gradient involves a constrained optimization problem ; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and an euclidean projection subproblem . in addition , we present two projected gradient algorithms and discuss their rates of convergence . experimental results on benchmark data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms .

['multi-task learning', 'sparse and low-rank patterns', 'trace norm']


social action tracking via noise tolerant time-varying factor graphs it is well known that users ' behaviors ( actions ) in a social network are influenced by various factors such as personal interests , social influence , and global trends . however , few publications systematically study how social actions evolve in a dynamic social network and to what extent different factors affect the user actions . in this paper , we propose a noise tolerant time-varying factor graph model ( ntt-fgm ) for modeling and predicting social actions . ntt-fgm simultaneously models social network structure , user attributes and user action history for better prediction of the users ' future actions . more specifically , a user 's action at time t is generated by her latent state at t , which is influenced by her attributes , her own latent state at time t-1 and her neighbors ' states at time t and t-1 . based on this intuition , we formalize the social action tracking problem using the ntt-fgm model ; then present an efficient algorithm to learn the model , by combining the ideas from both continuous linear system and markov random field . finally , we present a case study of our model on predicting future social actions . we validate the model on three different types of real-world data sets . qualitatively , our model can uncover some interesting patterns of the social dynamics . quantitatively , experimental results show that the proposed method outperforms several baseline methods for action prediction .

['social action tracking', 'social influence analysis', 'time-varying factor graphs']


model compression often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers . unfortunately , the space required to store this many classifiers , and the time required to execute them at run-time , prohibits their use in applications where test sets are large ( e.g. google ) , where storage space is at a premium ( e.g. pdas ) , and where computational power is limited ( e.g. hea-ring aids ) . we present a method for `` compressing '' large , complex ensembles into smaller , faster models , usually without significant loss in performance .

['model compression', 'supervised learning']


large human communication networks : patterns and a utility-driven generator given a real , and weighted person-to-person network which changes over time , what can we say about the cliques that it contains ? do the incidents of communication , or weights on the edges of a clique follow any pattern ? real , and in-person social networks have many more triangles than chance would dictate . as it turns out , there are many more cliques than one would expect , in surprising patterns . in this paper , we study massive real-world social networks formed by direct contacts among people through various personal communication services , such as phone-call , sms , im etc. . the contributions are the following : ( a ) we discover surprising patterns with the cliques , ( b ) we report power-laws of the weights on the edges of cliques , ( c ) our real networks follow these patterns such that we can trust them to spot outliers and finally , ( d ) we propose the first utility-driven graph generator for weighted time-evolving networks , which match the observed patterns . our study focused on three large datasets , each of which is a different type of communication service , with over one million records , and spans several months of activity .

['cliques', 'graph generators', 'social networks']

cliques NOUN pobj about
cliques NOUN attr are
cliques NOUN pobj with
cliques NOUN pobj of

streaming feature selection using alpha-investing in streaming feature selection ( sfs ) , new features are sequentially considered for addition to a predictive model . when the space of potential features is large , sfs offers many advantages over traditional feature selection methods , which assume that all features are known in advance . features can be generated dynamically , focusing the search for new features on promising subspaces , and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model . we describe î±-investing , an adaptive complexity penalty method for sfs which dynamically adjusts the threshold on the error reduction required for adding a new feature . î±-investing gives false discovery rate-style guarantees against overfitting . it differs from standard penalty methods such as aic , bic or ric , which always drastically over - or under-fit in the limit of infinite numbers of non-predictive features . empirical results show that sfs is competitive with much more compute-intensive feature selection methods such as stepwise regression , and allows feature selection on problems with over a million potential features .

['classification', 'false discovery rate', 'feature selection', 'multiple regression']

feature selection NOUN dobj streaming

fast query execution for retrieval models based on path-constrained random walks many recommendation and retrieval tasks can be represented as proximity queries on a labeled directed graph , with typed nodes representing documents , terms , and metadata , and labeled edges representing the relationships between them . recent work has shown that the accuracy of the widely-used random-walk-based proximity measures can be improved by supervised learning - in particular , one especially effective learning technique is based on path-constrained random walks ( pcrw ) , in which similarity is defined by a learned combination of constrained random walkers , each constrained to follow only a particular sequence of edge labels away from the query nodes . the pcrw based method significantly outperformed unsupervised random walk based queries , and models with learned edge weights . unfortunately , pcrw query systems are expensive to evaluate . in this study we evaluate the use of approximations to the computation of the pcrw distributions , including fingerprinting , particle filtering , and truncation strategies . in experiments on several recommendation and retrieval problems using two large scientific publications corpora we show speedups of factors of 2 to 100 with little loss in accuracy .

['filtering and recommending', 'information search and retrieval', 'learning to rank', 'path-constrained random walks', 'relational retrieval']


maximally informative k-itemsets and their efficient discovery in this paper we present a new approach to mining binary data . we treat each binary feature ( item ) as a means of distinguishing two sets of examples . our interest is in selecting from the total set of items an itemset of specified size , such that the database is partitioned with as uniform a distribution over the parts as possible . to achieve this goal , we propose the use of joint entropy as a quality measure for itemsets , and refer to optimal itemsets of cardinality k as maximally informative k-itemsets . we claim that this approach maximises distinctive power , as well as minimises redundancy within the feature set . a number of algorithms is presented for computing optimal itemsets efficiently .

['binary data', 'feature selection', 'information theory', 'joint entropy', 'learning', 'maximally informative k-itemsets', 'subgroup discovery', 'systems and information theory']


scalable influence maximization for prevalent viral marketing in large-scale social networks influence maximization , defined by kempe , kleinberg , and tardos ( 2003 ) , is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models . the scalability of influence maximization is a key factor for enabling prevalent viral marketing in large-scale online social networks . prior solutions , such as the greedy algorithm of kempe et al. ( 2003 ) and its improvements are slow and not scalable , while other heuristic algorithms do not provide consistently good performance on influence spreads . in this paper , we design a new heuristic algorithm that is easily scalable to millions of nodes and edges in our experiments . our algorithm has a simple tunable parameter for users to control the balance between the running time and the influence spread of the algorithm . our results from extensive simulations on several real-world and synthetic networks demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem : ( a ) our algorithm scales beyond million-sized graphs where the greedy algorithm becomes infeasible , and ( b ) in all size ranges , our algorithm performs consistently well in influence spread -- it is always among the best algorithms , and in most cases it significantly outperforms all other scalable heuristics to as much as 100 % -- 260 % increase in influence spread .

['influence maximization', 'social networks', 'viral marketing']


combining linguistic and statistical analysis to extract relations from web documents the world wide web provides a nearly endless source of knowledge , which is mostly given in natural language . a first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate . one strategy for this task is to find text patterns that express the semantic relation , to generalize these patterns , and to apply them to a corpus to find new pairs . in this paper , we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns . we demonstrate how linguistic structures can be represented for machine learning , and we provide a theoretical analysis of the pattern matching approach . we show the benefits of our approach by extensive experiments with our prototype system leila .

['learning', 'machine learning', 'pattern matching', 'relation extraction']

learning NOUN pobj for

classification of software behaviors for failure detection : a discriminative pattern mining approach software is a ubiquitous component of our daily life . we often depend on the correct working of software systems . due to the difficulty and complexity of software systems , bugs and anomalies are prevalent . bugs have caused billions of dollars loss , in addition to privacy and security threats . in this work , we address software reliability issues by proposing a novel method to classify software behaviors based on past history or runs . with the technique , it is possible to generalize past known errors and mistakes to capture failures and anomalies . our technique first mines a set of discriminative features capturing repetitive series of events from program execution traces . it then performs feature selection to select the best features for classification . these features are then used to train a classifier to detect failures . experiments and case studies on traces of several benchmark software systems and a real-life concurrency bug from mysql server show the utility of the technique in capturing failures and anomalies . on average , our pattern-based classification technique outperforms the baseline approach by 24.68 % in accuracy .

['closed unique patterns', 'failure detection', 'iterative patterns', 'pattern-based classification', 'sequential database', 'software behaviors']


large linear classification when data can not fit in memory recent advances in linear classification have shown that for applications such as document classification , the training can be extremely efficient . however , most of the existing training methods are designed by assuming that data can be stored in the computer memory . these methods can not be easily applied to data larger than the memory capacity due to the random access to the disk . we propose and analyze a block minimization framework for data larger than the memory size . at each step a block of data is loaded from the disk and handled by certain learning methods . we investigate two implementations of the proposed framework for primal and dual svms , respectively . as data can not fit in memory , many design considerations are very different from those for traditional algorithms . experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method .

['block minimization', 'large scale learning', 'svm']


efficient methods for topic model inference on streaming document collections topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace . fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive . with today 's large-scale , constantly expanding document collections , it is useful to be able to infer topic distributions for new documents without retraining the model . in this paper , we empirically evaluate the performance of several methods for topic inference in previously unseen documents , including methods based on gibbs sampling , variational inference , and a new method inspired by text classification . the classification-based inference method produces results similar to iterative inference methods , but requires only a single matrix multiplication . in addition to these inference methods , we present sparselda , an algorithm and data structure for evaluating gibbs sampling distributions . empirical results indicate that sparselda can be approximately 20 times faster than traditional lda and provide twice the speedup of previously published fast sampling methods , while also using substantially less memory .

['inference', 'miscellaneous', 'topic modeling']

inference NOUN compound techniques
inference NOUN pobj for
inference NOUN conj sampling
inference NOUN compound method
inference NOUN compound methods
inference NOUN compound methods

a maximum entropy web recommendation system : combining collaborative and content features web users display their preferences implicitly by navigating through a sequence of pages or by providing numeric ratings to some items . web usage mining techniques are used to extract useful knowledge about user interests from such data . the discovered user models are then used for a variety of applications such as personalized recommendations . web site content or semantic features of objects provide another source of knowledge for deciphering users ' needs or interests . we propose a novel web recommendation system in which collaborative features such as navigation or rating data as well as the content features accessed by the users are seamlessly integrated under the maximum entropy principle . both the discovered user patterns and the semantic relationships among web objects are represented as sets of constraints that are integrated to fit the model . in the case of content features , we use a new approach based on latent dirichlet allocation ( lda ) to discover the hidden semantic relationships among items and derive constraints used in the model . experiments on real web site usage data sets show that this approach can achieve better recommendation accuracy , when compared to systems using only usage information . the integration of semantic information also allows for better interpretation of the generated recommendations .

['maximum entropy', 'recommendation', 'user profiling', 'web usage mining']

recommendation NOUN compound system
recommendation NOUN compound system
recommendation NOUN compound accuracy

an efficient algorithm for a class of fused lasso problems the fused lasso penalty enforces sparsity in both the coefficients and their successive differences , which is desirable for applications with features ordered in some meaningful way . the resulting problem is , however , challenging to solve , as the fused lasso penalty is both non-smooth and non-separable . existing algorithms have high computational complexity and do not scale to large-size problems . in this paper , we propose an efficient fused lasso algorithm ( efla ) for optimizing this class of problems . one key building block in the proposed efla is the fused lasso signal approximator ( flsa ) . to efficiently solve flsa , we propose to reformulate it as the problem of finding an `` appropriate '' subgradient of the fused penalty at the minimizer , and develop a subgradient finding algorithm ( sfa ) . we further design a restart technique to accelerate the convergence of sfa , by exploiting the special `` structures '' of both the original and the reformulated flsa problems . our empirical evaluations show that , both sfa and efla significantly outperform existing solvers . we also demonstrate several applications of the fused lasso .

['fused lasso', 'l1 regularization', 'restart', 'subgradient']

subgradient NOUN dobj finding
subgradient NOUN npadvmod finding
restart NOUN compound technique

transfer metric learning by learning task relationships distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric . however , the labeled data available in many applications is scarce and hence the metrics learned are often unsatisfactory . in this paper , we consider a transfer learning setting in which some related source tasks with labeled data are available to help the learning of the target task . we first propose a convex formulation for multi-task metric learning by modeling the task relationships in the form of a task covariance matrix . then we regard transfer learning as a special case of multi-task learning and adapt the formulation of multi-task metric learning to the transfer learning setting for our method , called transfer metric learning ( tml ) . in tml , we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation . to solve the convex optimization problem , we use an alternating method in which each subproblem has an efficient solution . experimental results on some commonly used transfer learning applications demonstrate the effectiveness of our method .

['learning', 'metric learning', 'multi-task learning', 'transfer learning']

metric learning ADJ dobj transfer
learning VERB pcomp by
learning NOUN dobj learning
learning VERB acl transfer
learning NOUN dobj help
learning NOUN pobj for
learning VERB advcl regard
learning NOUN pobj of
learning VERB pobj of
learning VERB acl transfer
learning NOUN dobj called
learning VERB compound applications

growing a tree in the forest : constructing folksonomies by integrating structured metadata many social web sites allow users to annotate the content with descriptive metadata , such as tags , and more recently to organize content hierarchically . these types of structured metadata provide valuable evidence for learning how a community organizes knowledge . for instance , we can aggregate many personal hierarchies into a common taxonomy , also known as a folksonomy , that will aid users in visualizing and browsing social content , and also to help them in organizing their own content . however , learning from social metadata presents several challenges , since it is sparse , shallow , ambiguous , noisy , and inconsistent . we describe an approach to folksonomy learning based on relational clustering , which exploits structured metadata contained in personal hierarchies . our approach clusters similar hierarchies using their structure and tag statistics , then incrementally weaves them into a deeper , bushier tree . we study folksonomy learning using social metadata extracted from the photo-sharing site flickr , and demonstrate that the proposed approach addresses the challenges . moreover , comparing to previous work , the approach produces larger , more accurate folksonomies , and in addition , scales better .

['collective knowledge', 'folksonomies', 'relational clustering', 'social information processing', 'social metadata', 'taxonomies']

folksonomies NOUN dobj constructing
folksonomies NOUN dobj produces

discovering informative content blocks from web documents in this paper , we propose a new approach to discover informative contents from a set of tabular documents ( or web pages ) of a web site . our system , infodiscoverer , first partitions a page into several content blocks according to html tag ( table ) in a web page . based on the occurrence of the features ( terms ) in the set of pages , it calculates entropy value of each feature . according to the entropy value of each feature in a content block , the entropy value of the block is defined . by analyzing the information measure , we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant . informative content blocks are distinguished parts of the page , whereas redundant content blocks are common parts . based on the answer set generated from 13 manually tagged news web sites with a total of 26,518 web pages , experiments show that both recall and precision rates are greater than 0.956 . that is , using the approach , informative blocks ( news articles ) of these sites can be automatically separated from semantically redundant contents such as advertisements , banners , navigation panels , news categories , etc. . by adopting infodiscoverer as the preprocessor of information retrieval and extraction applications , the retrieval and extracting precision will be increased , and the indexing size and extracting complexity will also be reduced .

['entropy', 'information extraction', 'information retrieval', 'informative content discovery']

entropy ADJ compound value
entropy ADJ compound value
entropy ADJ compound value
entropy NOUN compound threshold

connecting the dots between news articles the process of extracting useful knowledge from large datasets has become one of the most pressing problems in today 's society . the problem spans entire sectors , from scientists to intelligence analysts and web users , all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day . with this much data , it is often easy to miss the big picture . in this paper , we investigate methods for automatically connecting the dots -- providing a structured , easy way to navigate within a new topic and discover hidden connections . we focus on the news domain : given two news articles , our system automatically finds a coherent chain linking them together . for example , it can recover the chain of events starting with the decline of home prices ( january 2007 ) , and ending with the ongoing health-care debate . we formalize the characteristics of a good chain and provide an efficient algorithm ( with theoretical guarantees ) to connect two fixed endpoints . we incorporate user feedback into our framework , allowing the stories to be refined and personalized . finally , we evaluate our algorithm over real news data . our user studies demonstrate the algorithm 's effectiveness in helping users understanding the news .

['coherence', 'learning', 'news']

news NOUN compound domain
news NOUN compound articles
news NOUN compound data
news NOUN dobj understanding

online multiscale dynamic topic models we propose an online topic model for sequentially analyzing the time evolution of topics in document collections . topics naturally evolve with multiple timescales . for example , some words may be used consistently over one hundred years , while other words emerge and disappear over periods of a few days . thus , in the proposed model , current topic-specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch . considering both the long-timescale dependency as well as the short-timescale dependency yields a more robust model . we derive efficient online inference procedures based on a stochastic em algorithm , in which the model is sequentially updated using newly obtained data ; this means that past data are not required to make the inference . we demonstrate the effectiveness of the proposed method in terms of predictive performance and computational efficiency by examining collections of real documents with timestamps .

['online learning', 'time-series analysis', 'topic model']


cross-sell : a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities

['collaborative filtering', 'cross-sell', 'electronic commerce', 'imputation', 'recommendation']

cross-sell ADJ amod method
recommendation NOUN compound method

evolutionary clustering we consider the problem of clustering data over time . an evolutionary clustering should simultaneously optimize two potentially conflicting criteria : first , the clustering at any point in time should remain faithful to the current data as much as possible ; and second , the clustering should not shift dramatically from one timestep to the next . we present a generic framework for this problem , and discuss evolutionary versions of two widely-used clustering algorithms within this framework : k-means and agglomerative hierarchical clustering . we extensively evaluate these algorithms on real data sets and show that our algorithms can simultaneously attain both high accuracy in capturing today 's data , and high fidelity in reflecting yesterday 's clustering .

['agglomerative', 'clustering', 'information search and retrieval', 'k-means', 'temporal evolution']

clustering NOUN npadvmod consider
clustering VERB pcomp of
clustering NOUN nsubj optimize
clustering NOUN nsubj remain
clustering NOUN nsubj shift
clustering NOUN compound algorithms
agglomerative ADJ amod clustering
clustering NOUN conj means
clustering NOUN dobj reflecting

frequent regular itemset mining concise representations of frequent itemsets sacrifice readability and direct interpretability by a data analyst of the concise patterns extracted . in this paper , we introduce an extension of itemsets , called regular , with an immediate semantics and interpretability , and a conciseness comparable to closed itemsets . regular itemsets allow for specifying that an item may or may not be present ; that any subset of an itemset may be present ; and that any non-empty subset of an itemset may be present . we devise a procedure , called regularmine , for mining a set of regular itemsets that is a concise representation of frequent itemsets . the procedure computes a covering , in terms of regular itemsets , of the frequent itemsets in the class of equivalence of a closed one . we report experimental results on several standard dense and sparse datasets that validate the proposed approach .

['closed and free itemsets', 'concise representations']


exploiting wikipedia as external knowledge for document clustering in traditional text clustering methods , documents are represented as `` bags of words '' without considering the semantic information of each document . for instance , if two documents use different collections of core words to represent the same topic , they may be falsely assigned to different clusters due to the lack of shared core words , although the core words they use are probably synonyms or semantically associated in other forms . the most common way to solve this problem is to enrich document representation with the background knowledge in an ontology . there are two major issues for this approach : ( 1 ) the coverage of the ontology is limited , even for wordnet or mesh , ( 2 ) using ontology terms as replacement or additional features may cause information loss , or introduce noise . in this paper , we present a novel text clustering method to address these two issues by enriching document representation with wikipedia concept and category information . we develop two approaches , exact match and relatedness-match , to map text documents to wikipedia concepts , and further to wikipedia categories . then the text documents are clustered based on a similarity metric which combines document content information , concept information as well as category information . the experimental results using the proposed clustering framework on three datasets ( 20-newsgroup , tdt2 , and la times ) show that clustering performance improves significantly by enriching document representation with wikipedia concepts and categories .

['document representation', 'text clustering', 'wikipedia']

wikipedia NOUN dobj exploiting
wikipedia NOUN compound concept
wikipedia NOUN relcl documents
wikipedia NOUN compound categories
wikipedia NOUN compound concepts

overlapping experiment infrastructure : more , better , faster experimentation at google , experimentation is practically a mantra ; we evaluate almost every change that potentially affects what our users experience . such changes include not only obvious user-visible changes such as modifications to a user interface , but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection . our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments , how to run experiments that produce better decisions , and how to run them faster . in this paper , we describe google 's overlapping experiment infrastructure that is a key component to solving these problems . in addition , because an experiment infrastructure alone is insufficient , we also discuss the associated tools and educational processes required to use it effectively . we conclude by describing trends that show the success of this overall experimental environment . while the paper specifically describes the experiment system and experimental processes we have in place at google , we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications .

['a/b testing', 'controlled experiments', 'multivariable testing', 'website testing']


suggesting friends using the implicit social graph although users of online communication tools rarely categorize their contacts into groups such as `` family '' , `` co-workers '' , or `` jogging buddies '' , they nonetheless implicitly cluster contacts , by virtue of their interactions with them , forming implicit groups . in this paper , we describe the implicit social graph which is formed by users ' interactions with contacts and groups of contacts , and which is distinct from explicit social graphs in which users explicitly add other individuals as their `` friends '' . we introduce an interaction-based metric for estimating a user 's affinity to his contacts and groups . we then describe a novel friend suggestion algorithm that uses a user 's implicit social graph to generate a friend group , given a small seed set of contacts which the user has already labeled as friends . we show experimental results that demonstrate the importance of both implicit group relationships and interaction-based affinity ranking in suggesting friends . finally , we discuss two applications of the friend suggest algorithm that have been released as gmail labs features .

['clustering', 'contact group clustering', 'group and organization interfaces', 'implicit social graph', 'tie strength']


the igrid index : reversing the dimensionality curse for similarity indexing in high dimensional space

['database applications', 'dimensionality curse', 'indexing']


cold start link prediction in the traditional link prediction problem , a snapshot of a social network is used as a starting point to predict , by means of graph-theoretic measures , the links that are likely to appear in the future . in this paper , we introduce cold start link prediction as the problem of predicting the structure of a social network when the network itself is totally missing while some other information regarding the nodes is available . we propose a two-phase method based on the bootstrap probabilistic graph . the first phase generates an implicit social network under the form of a probabilistic graph . the second phase applies probabilistic graph-based measures to produce the final prediction . we assess our method empirically over a large data collection obtained from flickr , using interest groups as the initial information . the experiments confirm the effectiveness of our approach .

['link prediction', 'probabilistic graph', 'social networks']


a probabilistic model for personalized tag prediction social tagging systems have become increasingly popular for sharing and organizing web resources . tag prediction is a common feature of social tagging systems . social tagging by nature is an incremental process , meaning that once a user has saved a web page with tags , the tagging system can provide more accurate predictions for the user , based on user 's incremental behaviors . however , existing tag prediction methods do not consider this important factor , in which their training and test datasets are either split by a fixed time stamp or randomly sampled from a larger corpus . in our temporal experiments , we perform a time-sensitive sampling on an existing public dataset , resulting in a new scenario which is much closer to `` real-world '' . in this paper , we address the problem of tag prediction by proposing a probabilistic model for personalized tag prediction . the model is a bayesian approach , and integrates three factors - ego-centric effect , environmental effects and web page content . two methods - both intuitive calculation and learning optimization - are provided for parameter estimation . pure graphbased methods which may have significant constraints ( such as every user , every item and every tag has to occur in at least p posts ) , can not make a prediction in most of `` real world '' cases while our model improves the f-measure by over 30 % compared to a leading algorithm , in our `` real-world '' use case .

['information search and retrieval', 'personalized tag prediction', 'social tagging', 'tag prediction']


extracting discriminative concepts for domain adaptation in text mining one common predictive modeling challenge occurs in text mining problems is that the training data and the operational ( testing ) data are drawn from different underlying distributions . this poses a great difficulty for many statistical learning methods . however , when the distribution in the source domain and the target domain are not identical but related , there may exist a shared concept space to preserve the relation . consequently a good feature representation can encode this concept space and minimize the distribution gap . to formalize this intuition , we propose a domain adaptation method that parameterizes this concept space by linear transformation under which we explicitly minimize the distribution difference between the source domain with sufficient labeled data and target domains with only unlabeled data , while at the same time minimizing the empirical loss on the labeled data in the source domain . another characteristic of our method is its capability for considering multiple classes and their interactions simultaneously . we have conducted extensive experiments on two common text mining problems , namely , information extraction and document classification to demonstrate the effectiveness of our proposed method .

['domain adaptation', 'feature extraction', 'miscellaneous', 'text mining']


parallel computation of high dimensional robust correlation and covariance matrices the computation of covariance and correlation matrices are critical to many data mining applications and processes . unfortunately the classical covariance and correlation matrices are very sensitive to outliers . robust methods , such as qc and the maronna method , have been proposed . however , existing algorithms for qc only give acceptable performance when the dimensionality of the matrix is in the hundreds ; and the maronna method is rarely used in practice because of its high computational cost . in this paper , we develop parallel algorithms for both qc and the maronna method . we evaluate these parallel algorithms using a real data set of the gene expression of over 6,000 genes , giving rise to a matrix of over 18 million entries . in our experimental evaluation , we explore scalability in dimensionality and in the number of processors . we also compare the parallel behaviors of the two methods . after thorough experimentation , we conclude that for many data mining applications , both qc and maronna are viable options . less robust , but faster , qc is the recommended choice for small parallel platforms . on the other hand , the maronna method is the recommended choice when a high degree of robustness is required , or when the parallel platform features a high number of processors .

['correlation', 'covariance', 'maronna', 'parallel', 'robust']

parallel ADJ amod computation
covariance NOUN pobj of
correlation NOUN compound matrices
covariance NOUN nsubj are
correlation NOUN compound matrices
robust ADJ amod methods
maronna NOUN compound method
maronna NOUN compound method
parallel ADJ amod algorithms
maronna NOUN compound method
parallel ADJ amod algorithms
parallel ADJ amod behaviors
maronna NOUN conj qc
robust ADJ ROOT robust
parallel ADJ amod platforms
maronna NOUN compound method
parallel ADJ amod platform

grafting-light : fast , incremental feature selection and structure learning of markov random fields feature selection is an important task in order to achieve better generalizability in high dimensional learning , and structure learning of markov random fields ( mrfs ) can automatically discover the inherent structures underlying complex data . both problems can be cast as solving an l1-norm regularized parameter estimation problem . the existing grafting method can avoid doing inference on dense graphs in structure learning by incrementally selecting new features . however , grafting performs a greedy step to optimize over free parameters once new features are included . this greedy strategy results in low efficiency when parameter learning is itself non-trivial , such as in mrfs , in which parameter learning depends on an expensive subroutine to calculate gradients . the complexity of calculating gradients in mrfs is typically exponential to the size of maximal cliques . in this paper , we present a fast algorithm called grafting-light to solve the l1-norm regularized maximum likelihood estimation of mrfs for efficient feature selection and structure learning . grafting-light iteratively performs one-step of orthant-wise gradient descent over free parameters and selects new features . this lazy strategy is guaranteed to converge to the global optimum and can effectively select significant features . on both synthetic and real data sets , we show that grafting-light is much more efficient than grafting for both feature selection and structure learning , and performs comparably with the optimal batch method that directly optimizes over all the features for feature selection but is much more efficient and accurate for structure learning of mrfs .

['feature selection', 'markov random fields', 'structure learning']


neighbor query friendly compression of social networks compressing social networks can substantially facilitate mining and advanced analysis of large social networks . preferably , social networks should be compressed in a way that they still can be queried efficiently without decompression . arguably , neighbor queries , which search for all neighbors of a query vertex , are the most essential operations on social networks . can we compress social networks effectively in a neighbor query friendly manner , that is , neighbor queries still can be answered in sublinear time using the compression ? in this paper , we develop an effective social network compression approach achieved by a novel eulerian data structure using multi-position linearizations of directed graphs . our method comes with a nontrivial theoretical bound on the compression rate . to the best of our knowledge , our approach is the first that can answer both out-neighbor and in-neighbor queries in sublinear time . an extensive empirical study on more than a dozen benchmark real data sets verifies our design .

['compression', 'mpk linearization', 'social networks']

compression NOUN nsubj facilitate
compression NOUN dobj using
compression NOUN compound approach
compression NOUN compound rate

mining advisor-advisee relationships from research publication networks information network contains abundant knowledge about relationships among people or entities . unfortunately , such kind of knowledge is often hidden in a network where different kinds of relationships are not explicitly categorized . for example , in a research publication network , the advisor-advisee relationships among researchers are hidden in the coauthor network . discovery of those relationships can benefit many interesting applications such as expert finding and research community analysis . in this paper , we take a computer science bibliographic network as an example , to analyze the roles of authors and to discover the likely advisor-advisee relationships . in particular , we propose a time-constrained probabilistic factor graph model ( tpfg ) , which takes a research publication network as input and models the advisor-advisee relationship mining problem using a jointly likelihood objective function . we further design an efficient learning algorithm to optimize the objective function . based on that our model suggests and ranks probable advisors for every author . experimental results show that the proposed approach infer advisor-advisee relationships efficiently and achieves a state-of-the-art accuracy ( 80-90 % ) . we also apply the discovered advisor-advisee relationships to bole search , a specific expert finding task and empirical study shows that the search performance can be effectively improved ( +4.09 % by ndcg@5 ) .

['advisor-advisee prediction', 'coauthor network', 'relationship mining', 'time-constrained factor graph']


online discovery and maintenance of time series motifs the detection of repeated subsequences , time series motifs , is a problem which has been shown to have great utility for several higher-level data mining algorithms , including classification , clustering , segmentation , forecasting , and rule discovery . in recent years there has been significant research effort spent on efficiently discovering these motifs in static offline databases . however , for many domains , the inherent streaming nature of time series demands online discovery and maintenance of time series motifs . in this paper , we develop the first online motif discovery algorithm which monitors and maintains motifs exactly in real time over the most recent history of a stream . our algorithm has a worst-case update time which is linear to the window size and is extendible to maintain more complex pattern structures . in contrast , the current offline algorithms either need significant update time or require very costly pre-processing steps which online algorithms simply can not afford . our core ideas allow useful extensions of our algorithm to deal with arbitrary data rates and discovering multidimensional motifs . we demonstrate the utility of our algorithms with a variety of case studies in the domains of robotics , acoustic monitoring and online compression .

['information search and retrieval', 'motifs', 'online algorithms', 'time series']

motifs ADP ROOT motifs
motifs NOUN appos subsequences
motifs NOUN dobj discovering
motifs ADV pobj of
motifs NOUN dobj maintains
motifs NOUN conj rates

discovering frequent patterns in sensitive data discovering frequent patterns from data is a popular exploratory technique in datamining . however , if the data are sensitive ( e.g. , patient health records , user behavior records ) releasing information about significant patterns or trends carries significant risk to privacy . this paper shows how one can accurately discover and release the most significant patterns along with their frequencies in a data set containing sensitive information , while providing rigorous guarantees of privacy for the individuals whose information is stored there . we present two efficient algorithms for discovering the k most frequent patterns in a data set of sensitive records . our algorithms satisfy differential privacy , a recently introduced definition that provides meaningful privacy guarantees in the presence of arbitrary external information . differentially private algorithms require a degree of uncertainty in their output to preserve privacy . our algorithms handle this by returning ` noisy ' lists of patterns that are close to the actual list of k most frequent patterns in the data . we define a new notion of utility that quantifies the output accuracy of private top-k pattern mining algorithms . in typical data sets , our utility criterion implies low false positive and false negative rates in the reported lists . we prove that our methods meet the new utility criterion ; we also demonstrate the performance of our algorithms through extensive experiments on the transaction data sets from the fimi repository . while the paper focuses on frequent pattern mining , the techniques developed here are relevant whenever the data mining output is a list of elements ordered according to an appropriately ` robust ' measure of interest .

['differential privacy', 'exponential mechanism', 'frequent itemsets', 'frequent patterns', 'general', 'privacy']

privacy NOUN pobj to
privacy NOUN pobj of
privacy NOUN dobj satisfy
privacy NOUN compound guarantees
privacy NOUN dobj preserve

the offset tree for learning with partial labels we present an algorithm , called the offset tree , for learning to make decisions in situations where the payoff of only one choice is observed , rather than all choices . the algorithm reduces this setting to binary classification , allowing one to reuse any existing , fully supervised binary classification algorithm in this partial information setting . we show that the offset tree is an optimal reduction to binary classification . in particular , it has regret at most ( k-1 ) times the regret of the binary classifier it uses ( where k is the number of choices ) , and no reduction to binary classification can do better . this reduction is also computationally optimal , both at training and test time , requiring just o ( log2 k ) work to train on an example or make a prediction . experiments with the offset tree show that it generally performs better than several alternative approaches .

['associative reinforcement learning', 'contextual bandits', 'interactive learning', 'learning']

learning VERB pcomp for

an approach to spacecraft anomaly detection problem using kernel feature space development of advanced anomaly detection and failure diagnosis technologies for spacecraft is a quite significant issue in the space industry , because the space environment is harsh , distant and uncertain . while several modern approaches based on qualitative reasoning , expert systems , and probabilistic reasoning have been developed recently for this purpose , any of them has a common difficulty in obtaining accurate and complete a priori knowledge on the space systems from human experts . a reasonable alternative to this conventional anomaly detection method is to reuse a vast amount of telemetry data which is multi-dimensional time-series continuously produced from a number of system components in the spacecraft . this paper proposes a novel `` knowledge-free '' anomaly detection method for spacecraft based on kernel feature space and directional distribution , which constructs a system behavior model from the past normal telemetry data from a set of telemetry data in normal operation and monitors the current system status by checking incoming data with the model . in this method , we regard anomaly phenomena as unexpected changes of causal associations in the spacecraft system , and hypothesize that the significant causal associations inside the system will appear in the form of principal component directions in a high-dimensional non-linear feature space which is constructed by a kernel function and a set of data . we have confirmed the effectiveness of the proposed anomaly detection method by applying it to the telemetry data obtained from a simulator of an orbital transfer vehicle designed to make a rendezvous maneuver with the international space station .

['anomaly detection', 'kernel feature space', 'learning', 'principal component analysis', 'spacecraft', 'time series data', 'von mises fisher distribution']

spacecraft NOUN pobj for
spacecraft NOUN pobj in
spacecraft NOUN pobj for
spacecraft NOUN compound system

eigenspace-based anomaly detection in computer systems we report on an automated runtime anomaly detection method at the application layer of multi-node computer systems . although several network management systems are available in the market , none of them have sufficient capabilities to detect faults in multi-tier web-based systems with redundancy . we model a web-based system as a weighted graph , where each node represents a `` service '' and each edge represents a dependency between services . since the edge weights vary greatly over time , the problem we address is that of anomaly detection from a time sequence of graphs . in our method , we first extract a feature vector from the adjacency matrix that represents the activities of all of the services . the heart of our method is to use the principal eigenvector of the eigenclusters of the graph . then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived from the graph sequence . given a critical probability , the threshold value is adaptively updated using a novel online algorithm . we demonstrate that a fault in a web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system .

['learning', 'perron-frobenius theorem', 'principal eigenvector', 'singular value decomposition', 'time sequence of graphs', 'von mises-fisher distribution']


weighting versus pruning in rule validation for detecting network and host anomalies for intrusion detection , the lerad algorithm learns a succinct set of comprehensible rules for detecting anomalies , which could be novel attacks . lerad validates the learned rules on a separate held-out validation set and removes rules that cause false alarms . however , removing rules with possible high coverage can lead to missed detections . we propose to retain these rules and associate weights to them . we present three weighting schemes and our empirical results indicate that , for lerad , rule weighting can detect more attacks than pruning with minimal computational overhead .

['anomaly detection', 'invasive software', 'machine learning', 'rule pruning', 'rule weighting', 'unauthorized access']


simultaneous record detection and attribute labeling in web data extraction recent work has shown the feasibility and promise of template-independent web data extraction . however , existing approaches use decoupled strategies - attempting to do data record detection and attribute labeling in two separate phases . in this paper , we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously . in our approach , record detection can benefit from the availability of semantics required in attribute labeling and , at the same time , the accuracy of attribute labeling can be improved when data records are labeled in a collective manner . the proposed model is called hierarchical conditional random fields . it can efficiently integrate all useful features by learning their importance , and it can also incorporate hierarchical interactions which are very important for web data extraction . we empirically compare the proposed model with existing decoupled approaches for product information extraction , and the results show significant improvements in both record detection and attribute labeling .

['attribute labeling', 'conditional random fields', 'data record detection', 'hierarchical conditional random fields', 'web page segmentation']


document preprocessing for naive bayes classification and clustering with mixture of multinomials naive bayes classifier has long been used for text categorization tasks . its sibling from the unsupervised world , the probabilistic mixture of multinomial models , has likewise been successfully applied to text clustering problems . despite the strong independence assumptions that these models make , their attractiveness come from low computational cost , relatively low memory consumption , ability to handle heterogeneous features and multiple classes , and often competitiveness with the top of the line models . recently , there has been several attempts to alleviate the problems of naive bayes by performing heuristic feature transformations , such as idf , normalization by the length of the documents and taking the logarithms of the counts . we justify the use of these techniques and apply them to two problems : classification of products in yahoo ! shopping and clustering the vectors of collocated terms in user queries to yahoo ! search . the experimental evaluation allows us to draw conclusions about the promise that these transformations carry with regard to alleviating the strong assumptions of the multinomial model .

['classification', 'clustering', 'data transformations', 'learning', 'mixture of multinomials', 'naive bayes']

clustering NOUN conj preprocessing
clustering NOUN acl text
classification NOUN appos problems
clustering VERB conj shopping

a general framework for accurate and fast regression by data summarization in random decision trees predicting the values of continuous variable as a function of several independent variables is one of the most important problems for data mining . a very large number of regression methods , both parametric and nonparametric , have been proposed in the past . however , since the list is quite extensive and many of these models make rather explicit , strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options , choosing the appropriate regression methodology and then specifying the parameter values is a none-trivial , sometimes frustrating , task for data mining practitioners . choosing the inappropriate methodology can have rather disappointing results . this issue is against the general utility of data mining software . for example , linear regression methods are straightforward and well-understood . however , since the linear assumption is very strong , its performance is compromised for complicated non-linear problems . kernel-based methods perform quite well if the kernel functions are selected correctly . in this paper , we propose a straightforward approach based on summarizing the training data using an ensemble of random decisions trees . it requires very little knowledge from the user , yet is applicable to every type of regression problem that we are currently aware of . we have experimented on a wide range of problems including those that parametric methods performwell , a large selection of benchmark datasets for nonparametric regression , as well as highly non-linear stochastic problems . our results are either significantly better than or identical to many approaches that are known to perform well on these problems .

['decision trees', 'random', 'regression']

regression NOUN compound methods
regression NOUN compound methodology
regression NOUN compound methods
random ADJ amod decisions
regression NOUN compound problem
regression NOUN pobj for

entity categorization over large document collections extracting entities ( such as people , movies ) from documents and identifying the categories ( such as painter , writer ) they belong to enable structured querying and data analysis over unstructured document collections . in this paper , we focus on the problem of categorizing extracted entities . most prior approaches developed for this task only analyzed the local document context within which entities occur . in this paper , we significantly improve the accuracy of entity categorization by ( i ) considering an entity 's context across multiple documents containing it , and ( ii ) exploiting existing large lists of related entities ( e.g. , lists of actors , directors , books ) . these approaches introduce computational challenges because ( a ) the context of entities has to be aggregated across several documents and ( b ) the lists of related entities may be very large . we develop techniques to address these challenges . we present a thorough experimental study on real data sets that demonstrates the increase in accuracy and the scalability of our approaches .

['information extraction']


gplag : detection of software plagiarism by program dependence graph analysis along with the blossom of open source projects comes the convenience for software plagiarism . a company , if less self-disciplined , may be tempted to plagiarize some open source projects for its own products . although current plagiarism detection tools appear sufficient for academic use , they are nevertheless short for fighting against serious plagiarists . for example , disguises like statement reordering and code insertion can effectively confuse these tools . in this paper , we develop a new plagiarism detection tool , called gplag , which detects plagiarism by mining program dependence graphs ( pdgs ) . a pdg is a graphic representation of the data and control dependencies within a procedure . because pdgs are nearly invariant during plagiarism , gplag is more effective than state-of-the-art tools for plagiarism detection . in order to make gplag scalable to large programs , a statistical lossy filter is proposed to prune the plagiarism search space . experiment study shows that gplag is both effective and efficient : it detects plagiarism that easily slips over existing tools , and it usually takes a few seconds to find ( simulated ) plagiarism in programs having thousands of lines of code .

['graph mining', 'program dependence graph', 'software plagiarism detection']


coherent closed quasi-clique discovery from large dense graph databases frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database , and mining frequently occurring coherent subgraphs from large dense graph databases has been witnessed several applications and received considerable attention in the graph mining community recently . in this paper , we study how to efficiently mine the complete set of coherent closed quasi-cliques from large dense graph databases , which is an especially challenging task due to the downward-closure property no longer holds . by fully exploring some properties of quasi-cliques , we propose several novel optimization techniques , which can prune the unpromising and redundant sub-search spaces effectively . meanwhile , we devise an efficient closure checking scheme to facilitate the discovery of only closed quasi-cliques . we also develop a coherent closed quasi-clique mining algorithm , ( b ) cocain ( \/ b ) 1 thorough performance study shows that cocain is very efficient and scalable for large dense graph databases .

['coherent subgraph', 'graph mining', 'quasi-clique']


unsupervised transfer classification : application to text categorization we study the problem of building the classification model for a target class in the absence of any labeled training example for that class . to address this difficult learning problem , we extend the idea of transfer learning by assuming that the following side information is available : ( i ) a collection of labeled examples belonging to other classes in the problem domain , called the auxiliary classes ; ( ii ) the class information including the prior of the target class and the correlation between the target class and the auxiliary classes . our goal is to construct the classification model for the target class by leveraging the above data and information . we refer to this learning problem as unsupervised transfer classification . our framework is based on the generalized maximum entropy model that is effective in transferring the label information of the auxiliary classes to the target class . a theoretical analysis shows that under certain assumption , the classification model obtained by the proposed approach converges to the optimal model when it is learned from the labeled examples for the target class . empirical study on text categorization over four different data sets verifies the effectiveness of the proposed approach .

['generalized maximum entropy model', 'miscellaneous', 'text categorization', 'unsupervised transfer classification']


boosting with structure information in the functional space : an application to graph classification boosting is a very successful classification algorithm that produces a linear combination of `` weak '' classifiers ( a.k.a. base learners ) to obtain high quality classification models . in this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space . though such relationships are generic , our work is particularly motivated by the emerging topic of pattern based classification for semi-structured data including graphs . towards an efficient incorporation of the structure information , we have designed a general model where we use an undirected graph to capture the relationship of subgraph-based base learners . in our method , we combine both l1 norm and laplacian based l2 norm penalty with logit loss function of logit boost . in this approach , we enforce model sparsity and smoothness in the functional space spanned by the basis functions . we have derived efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features . using comprehensive experimental study , we have demonstrated the effectiveness of the proposed learning methods .

['boosting', 'feature selection', 'graph classification', 'l1 regularization', 'semi-structured data']

boosting VERB advcl is
boosting NOUN dobj graph
boosting VERB compound algorithm
boosting NOUN compound formulation

optimizing web traffic via the media scheduling problem website traffic varies through time in consistent and predictable ways , with highest traffic in the middle of the day . when providing media content to visitors , it is important to present repeat visitors with new content so that they keep coming back . in this paper we present an algorithm to balance the need to keep a website fresh with new content with the desire to present the best content to the most visitors at times of peak traffic . we formulate this as the media scheduling problem , where we attempt to maximize total clicks , given the overall traffic pattern and the time varying clickthrough rates of available media content . we present an efficient algorithm to perform this scheduling under certain conditions and apply this algorithm to real data obtained from server logs , showing evidence of significant improvements in traffic from our algorithmic schedules . finally , we analyze the click data , presenting models for why and how the clickthrough rate for new content declines as it ages .

['human response', 'media scheduling', 'user interaction']


extracting collective probabilistic forecasts from web games game sites on the world wide web draw people from around the world with specialized interests , skills , and knowledge . data from the games often reflects the players ' expertise and will to win . we extract probabilistic forecasts from data obtained from three online games : the hollywood stock exchange ( hsx ) , the foresight exchange ( fx ) , and the formula one pick six ( f1p6 ) competition . we find that all three yield accurate forecasts of uncertain future events . in particular , prices of so-called `` movie stocks '' on hsx are good indicators of actual box office returns . prices of hsx securities in oscar , emmy , and grammy awards correlate well with observed frequencies of winning . fx prices are reliable indicators of future developments in science and technology . collective predictions from players in the f1 competition serve as good forecasts of true race outcomes . in some cases , forecasts induced from game data are more reliable than expert opinions . we argue that web games naturally attract well-informed and well-motivated players , and thus offer a valuable and oft-overlooked source of high-quality data with significant predictive value .

['artificial markets', 'collective probabilistic forecasts', 'foresight exchange', 'formula one pick six competition', 'hollywood stock exchange', 'knowledge discovery', 'world wide web games']


experimental design for solicitation campaigns data mining techniques are routinely used by fundraisers to select those prospects from a large pool of candidates who are most likely to make a financial contribution . these techniques often rely on statistical models based on trial performance data . this trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool . collecting this trial data involves a cost ; therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospects . we describe an experimental design approach to optimally choose the trial prospects from an existing large pool of prospects . prospects are clustered to render the problem practically tractable . we modify the standard d-optimality algorithm to prevent repeated selection of the same prospect cluster , since each prospect can only be solicited at most once . we assess the benefits of this approach on the kdd-98 data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size .

['data collection', 'experimental design', 'probability and statistics', 'solicitation campaign']


generative model-based clustering of directional data high dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data . a natural model for multi-variate directional data is provided by the von mises-fisher ( vmf ) distribution on the unit hypersphere that is analogous to the multi-variate gaussian distribution in rd. . in this paper , we propose modeling complex directional data as a mixture of vmf distributions . we derive and analyze two variants of the expectation maximization ( em ) framework for estimating the parameters of this mixture . we also propose two clustering algorithms corresponding to these variants . an interesting aspect of our methodology is that the spherical kmeans algorithm ( kmeans with cosine similarity ) can be shown to be a special case of both our algorithms . thus , modeling text data by vmf distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community . as part of experimental validation , we present results on modeling high-dimensional text and gene-expression data as a mixture of vmf distributions . the results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces .

['clustering', 'clustering', 'directional data', 'em', 'information search and retrieval', 'mixtures', 'von mises-fisher']

clustering NOUN nsubj becoming
em NOUN appos maximization
clustering VERB compound algorithms
clustering NOUN compound tasks

combining partitions by probabilistic label aggregation data clustering represents an important tool in exploratory data analysis . the lack of objective criteria render model selection as well as the identification of robust solutions particularly difficult . the use of a stability assessment and the combination of multiple clustering solutions represents an important ingredient to achieve the goal of finding useful partitions . in this work , we propose a novel way of combining multiple clustering solutions for both , hard and soft partitions : the approach is based on modeling the probability that two objects are grouped together . an efficient em optimization strategy is employed in order to estimate the model parameters . our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solutions according to their consistency with the prediction for previously unseen objects . in addition to that , the probabilistic model supports an out-of-sample extension that ( i ) makes it possible to assign previously unseen objects to classes of the combined solution and ( ii ) renders the efficient aggregation of solutions possible . in this work , we also shed some light on the usefulness of such combination approaches . in the experimental result section , we demonstrate the competitive performance of our proposal in comparison with other recently proposed methods for combining multiple classifications of a finite data set .

['clustering', 'consensus partition', 'learning', 're-sampling']

clustering NOUN compound solutions
clustering NOUN compound solutions
clustering NOUN compound solutions

query chains : learning to rank from implicit feedback this paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results . we observe that users searching the web often perform a sequence , or chain , of queries with a similar information need . using query chains , we generate new types of preference judgments from search engine logs , thus taking advantage of user intelligence in reformulating queries . to validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments . we also implemented a real-world search engine to test our approach , using a modified ranking svm to learn an improved ranking function from preference data . our results demonstrate significant improvements in the ranking given by the search engine . the learned rankings outperform both a static ranking function , as well as one trained without considering query chains .

['clickthrough data', 'implicit feedback', 'information search and retrieval', 'machine learning', 'search engines', 'support vector machines']


group formation in large social networks : membership , growth , and evolution the processes by which communities come together , attract new members , and develop over time is a central research issue in the social sciences - political movements , professional organizations , and religious denominations all provide fundamental examples of such communities . in the digital domain , on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as myspace and livejournal . however , the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved : what are the structural features that influence whether individuals will join communities , which communities will grow rapidly , and how do the overlaps among pairs of communities change over time . here we address these questions using two large sources of data : friendship links and community membership on livejournal , and co-authorship and conference publications in dblp . both of these datasets provide explicit user-defined communities , where conferences serve as proxies for communities in dblp . we study how the evolution of these communities relates to properties such as the structure of the underlying social networks . we find that the propensity of individuals to join communities , and of communities to grow rapidly , depends in subtle ways on the underlying network structure . for example , the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community , but also crucially by how those friends are connected to one another . we use decision-tree techniques to identify the most significant structural determinants of these properties . we also develop a novel methodology for measuring movement of individuals between communities , and show how such movements are closely aligned with changes in the topics of interest within the communities .

['diffusion of innovations', 'on-line communities', 'social networks']


generalized additive neural networks

['additive models', 'financial', 'partial residuals', 'predictive modeling']


aggregating time partitions partitions of sequential data exist either per se or as a result of sequence segmentation algorithms . it is often the case that the same timeline is partitioned in many different ways . for example , different segmentation algorithms produce different partitions of the same underlying data points . in such cases , we are interested in producing an aggregate partition , i.e. , a segmentation that agrees as much as possible with the input segmentations . each partition is defined as a set of continuous non-overlapping segments of the timeline . we show that this problem can be solved optimally in polynomial time using dynamic programming . we also propose faster greedy heuristics that work well in practice . we experiment with our algorithms and we demonstrate their utility in clustering the behavior of mobile-phone users and combining the results of different segmentation algorithms on genomic sequences .

['nonnumerical algorithms and problems']


programming the k-means clustering algorithm in sql using sql has not been considered an efficient and feasible way to implement data mining algorithms . although this is true for many data mining , machine learning and statistical algorithms , this work shows it is feasible to get an efficient sql implementation of the well-known k-means clustering algorithm that can work on top of a relational dbms . the article emphasizes both correctness and performance . from a correctness point of view the article explains how to compute euclidean distance , nearest-cluster queries and updating clustering results in sql . from a performance point of view it is explained how to cluster large data sets defining and indexing tables to store and retrieve intermediate and final results , optimizing and avoiding joins , optimizing and simplifying clustering aggregations , and taking advantage of sufficient statistics . experiments evaluate scalability with synthetic data sets varying size and dimensionality . the proposed k-means implementation can cluster large data sets and exhibits linear scalability .

['clustering', 'k-means', 'sql']

clustering NOUN compound algorithm
sql NOUN pobj in
sql NOUN dobj using
sql NOUN compound implementation
clustering NOUN amod algorithm
clustering NOUN compound results
sql NOUN pobj in
clustering NOUN compound aggregations

nomograms for visualizing support vector machines we propose a simple yet potentially very effective way of visualizing trained support vector machines . nomograms are an established model visualization technique that can graphically encode the complete model on a single page . the dimensionality of the visualization does not depend on the number of attributes , but merely on the properties of the kernel . to represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms , we employ logistic regression to convert the distance from the separating hyperplane into a probability . case studies on selected data sets show that for a technique thought to be a black-box , nomograms can clearly expose its internal structure . by providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors .

['machine learning', 'nomogram', 'support vector machines', 'visualization']

visualization NOUN compound technique
visualization NOUN pobj of
visualization NOUN dobj providing

probabilistic author-topic models for information discovery we propose a new unsupervised learning technique for extracting information from large text collections . we model documents as if they were generated by a two-stage stochastic process . each author is represented by a probability distribution over topics , and each topic is represented as a probability distribution over words for that topic . the words in a multi-author paper are assumed to be the result of a mixture of each authors ' topic mixture . the topic-word and author-topic distributions are learned from data in an unsupervised manner using a markov chain monte carlo algorithm . we apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known citeseer digital library , and learn a model with 300 topics . we discuss in detail the interpretation of the results discovered by the system including specific topic and author models , ranking of authors by topic and topics by author , significant trends in the computer science literature between 1990 and 2002 , parsing of abstracts by topics and authors and detection of unusual papers by specific authors . an online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as citeseer .

['gibbs sampling', 'learning', 'text modeling', 'unsupervised learning']

learning NOUN compound technique

algorithms for estimating relative importance in networks large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis -- examples include social networks , web graphs , telecommunication networks , and biological networks . in interactive analysis of such data a natural query is `` which entities are most important in the network relative to a particular individual or set of individuals ? '' we investigate the problem of answering such queries in this paper , focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes . we define a general framework and a number of different algorithms , building on ideas from social networks , graph theory , markov models , and web graph analysis . we experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among september 11th terrorists , a network of collaborative research in biotechnology among companies and universities , and a network of co-authorship relationships among computer science researchers .

['graphs', 'information search and retrieval', 'markov chains', 'pagerank', 'relative importance', 'social networks']

graphs NOUN conj networks
graphs NOUN pobj on

mining scale-free networks using geodesic clustering many real-world graphs have been shown to be scale-free -- vertex degrees follow power law distributions , vertices tend to cluster , and the average length of all shortest paths is small . we present a new model for understanding scale-free networks based on multilevel geodesic approximation , using a new data structure called a multilevel mesh . using this multilevel framework , we propose a new kind of graph clustering for data reduction of very large graph systems such as social , biological , or electronic networks . finally , we apply our algorithms to real-world social networks and protein interaction graphs to show that they can reveal knowledge embedded in underlying graph structures . we also demonstrate how our data structures can be used to quickly answer approximate distance and shortest path queries on scale-free networks .

['clustering', 'graphs', 'scale-free networks', 'social networks']

clustering VERB xcomp using
graphs NOUN nsubjpass shown
clustering NOUN acl kind
graphs NOUN conj networks

the distributed boosting algorithm in this paper , we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that can not be merged at a single location . our distributed boosting algorithm can also be used as a parallel classification technique , where a massive database that can not fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis . in the proposed method , at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites . finally the classifiers are combined into a weighted voting ensemble on each disjoint data set . the ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites . in experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time . in addition , the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases .

['boosting', 'classifier ensembles', 'distributed learning']

boosting VERB compound algorithm
boosting VERB pobj for
boosting VERB compound algorithm
boosting VERB pcomp at
boosting VERB amod method
boosting NOUN compound algorithm
boosting VERB compound algorithm
boosting NOUN pobj to

sequential cost-sensitive decision making with reinforcement learning recently , there has been increasing interest in the issues of cost-sensitive learning and decision making in a variety of applications of data mining . a number of approaches have been developed that are effective at optimizing cost-sensitive decisions when each decision is considered in isolation . however , the issue of sequential decision making , with the goal of maximizing total benefits accrued over a period of time instead of immediate benefits , has rarely been addressed . in the present paper , we propose a novel approach to sequential decision making based on the reinforcement learning framework . our approach attempts to learn decision rules that optimize a sequence of cost-sensitive decisions so as to maximize the total benefits accrued over time . we use the domain of targeted ' marketing as a testbed for empirical evaluation of the proposed method . we conducted experiments using approximately two years of monthly promotion data derived from the well-known kdd cup 1998 donation data set . the experimental results show that the proposed method for optimizing total accrued benefits out performs the usual targeted-marketing methodology of optimizing each promotion in isolation . we also analyze the behavior of the targeting rules that were obtained and discuss their appropriateness to the application domain .

['learning']

learning VERB pobj with
learning NOUN pobj of
learning VERB compound framework

explicitly representing expected cost : an alternative to roc representation

['cost sensitive learning', 'roc analysis']


learning spatially variant dissimilarity ( svad ) measures clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed ( dis ) similarity measure between the data points in feature space . this makes the type of clusters identified highly dependent on the assumed similarity measure . building on recent work in this area , we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data . the idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure . our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature .

['clustering', 'learning dissimilarity measures']

clustering VERB acl measures

information awareness : a prospective technical assessment recent proposals to apply data mining systems to problems in law enforcement , national security , and fraud detection have attracted both media attention and technical critiques of their expected accuracy and impact on privacy . unfortunately , the majority of technical critiques have been based on simplistic assumptions about data , classifiers , inference procedures , and the overall architecture of such systems . we consider these critiques in detail , and we construct a simulation model that more closely matches realistic systems . we show how both the accuracy and privacy impact of a hypothetical system could be substantially improved , and we discuss the necessary and sufficient conditions for this improvement to be achieved . this analysis is neither a defense nor a critique of any particular system concept . rather , our model suggests alternative technical designs that could mitigate some concerns , but also raises more specific conditions that must be met for such systems to be both accurate and socially desirable .

['collective classification', 'database applications', 'information awareness', 'iterative classification', 'privacy', 'ranking classifiers', 'relational data mining', 'social network analysis', 'technology assessment', 'tia']

privacy NOUN pobj on
privacy NOUN conj accuracy

finding surprising patterns in a time series database in linear time and space the problem of finding a specified pattern in a time series database ( i.e. query by content ) has received much attention and is now a relatively mature field . in contrast , the important problem of enumerating all surprising or interesting patterns has received far less attention . this problem requires a meaningful definition of `` surprise '' , and an efficient search technique . all previous attempts at finding surprising patterns in time series use a very limited notion of surprise , and\/or do not scale to massive datasets . to overcome these limitations we introduce a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance , given some previously seen data .

['anomaly detection', 'feature extraction', 'markov model', 'novelty detection', 'suffix tree', 'time series']


clustering spatial data using random walks discovering significant patterns that exist implicitly in huge spatial databases is an important computational task . a common approach to this problem is to use cluster analysis . we propose a novel approach to clustering , based on the deterministic analysis of random walks on a weighted graph generated from the data . our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities , overcoming noise and outliers that may blur the natural decomposition of the data . the method requires only o ( n log n ) time , and one of its variants needs only constant space .

['probabilistic algorithms']


can we learn a template-independent wrapper for news article extraction from a single training site ? automatic news extraction from news pages is important in many web applications such as news aggregation . however , the existing news extraction methods based on template-level wrapper induction have three serious limitations . first , the existing methods can not correctly extract pages belonging to an unseen template . second , it is costly to maintain up-to-date wrappers for a large amount of news websites , because any change of a template may invalidate the corresponding wrapper . last , the existing methods can merely extract unformatted plain texts , and thus are not user friendly . in this paper , we tackle the problem of template-independent web news extraction in a user-friendly way . we formalize web news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site . novel features dedicated to news titles and bodies are developed . correlations between news titles and news bodies are exploited . our template-independent wrapper can extract news pages from different sites regardless of templates . moreover , our approach can extract not only texts , but also images and animates within the news bodies and the extracted news articles are in the same visual style as in the original pages . in our experiments , a wrapper learned from 40 pages from a single news site achieved an accuracy of 98.1 % on 3,973 news pages from 12 news sites .

['classification', 'data extraction', 'miscellaneous', 'web mining']


divrank : the interplay of prestige and diversity in information networks information networks are widely used to characterize the relationships between data items such as text documents . many important retrieval and mining tasks rely on ranking the data items based on their centrality or prestige in the network . beyond prestige , diversity has been recognized as a crucial objective in ranking , aiming at providing a non-redundant and high coverage piece of information in the top ranked results . nevertheless , existing network-based ranking approaches either disregard the concern of diversity , or handle it with non-optimized heuristics , usually based on greedy vertex selection . we propose a novel ranking algorithm , divrank , based on a reinforced random walk in an information network . this model automatically balances the prestige and the diversity of the top ranked vertices in a principled way . divrank not only has a clear optimization explanation , but also well connects to classical models in mathematics and network science . we evaluate divrank using empirical experiments on three different networks as well as a text summarization task . divrank outperforms existing network-based ranking methods in terms of enhancing diversity in prestige .

['diversity', 'information networks', 'ranking', 'reinforced random walk']

diversity NOUN conj prestige
ranking VERB pcomp on
diversity NOUN nsubjpass recognized
ranking VERB pobj in
ranking VERB amod approaches
diversity NOUN pobj of
ranking VERB amod algorithm
diversity NOUN conj prestige
ranking VERB amod methods
diversity NOUN dobj enhancing

universal multi-dimensional scaling in this paper , we propose a unified algorithmic framework for solving many known variants of mds . our algorithm is a simple iterative scheme with guaranteed convergence , and is modular ; by changing the internals of a single subroutine in the algorithm , we can switch cost functions and target spaces easily . in addition to the formal guarantees of convergence , our algorithms are accurate ; in most cases , they converge to better quality solutions than existing methods in comparable time . moreover , they have a small memory footprint and scale effectively for large data sets . we expect that this framework will be useful for a number of mds variants that have not yet been studied . our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere , preserving geodesic distances . as a complement to this result , we also extend the johnson-lindenstrauss lemma to this spherical setting , by showing that projecting to a random o ( ( 1 \/ âµ2 ) log n ) - dimensional sphere causes only an eps-distortion in the geodesic distances .

['dimensionality reduction', 'multi-dimensional scaling']


fast nearest-neighbor search in disk-resident graphs link prediction , personalized graph search , fraud detection , and many such graph mining problems revolve around the computation of the most `` similar '' k nodes to a given query node . one widely used class of similarity measures is based on random walks on graphs , e.g. , personalized pagerank , hitting and commute times , and simrank . there are two fundamental problems associated with these measures . first , existing online algorithms typically examine the local neighborhood of the query node which can become significantly slower whenever high-degree nodes are encountered ( a common phenomenon in real-world graphs ) . we prove that turning high degree nodes into sinks results in only a small approximation error , while greatly improving running times . the second problem is that of computing similarities at query time when the graph is too large to be memory-resident . the obvious solution is to split the graph into clusters of nodes and store each cluster on a disk page ; ideally random walks will rarely cross cluster boundaries and cause page-faults . our contributions here are twofold : ( a ) we present an efficient deterministic algorithm to find the k closest neighbors ( in terms of personalized pagerank ) of any query node in such a clustered graph , and ( b ) we develop a clustering algorithm ( rwdisk ) that uses only sequential sweeps over data files . empirical results on several large publicly available graphs like dblp , citeseer and live-journal ( ~ 90 m edges ) demonstrate that turning high degree nodes into sinks not only improves running time of rwdisk by a factor of 3 but also boosts link prediction accuracy by a factor of 4 on average . we also show that rwdisk returns more desirable ( high conductance and small size ) clusters than the popular clustering algorithm metis , while requiring much less memory . finally our deterministic algorithm for computing nearest neighbors incurs far fewer page-faults ( factor of 5 ) than actually simulating random walks .

['external memory', 'graphs', 'link prediction', 'random walks']

graphs NOUN compound link
graphs NOUN pobj on
graphs NOUN pobj in
graphs NOUN pobj on

a model for discovering customer value for e-content there exists a huge demand for multimedia goods and services in the internet . currently available bandwidth speeds can support sale of downloadable content like cds , e-books , etc. as well as services like video-on-demand . in the future , such services will be prevalent in the internet . since costs are typically fixed , maximizing revenue can maximize profits . a primary determinant of revenue in such e-content markets is how much value the customers associate with the content . though marketing surveys are useful , they can not adapt to the dynamic nature of the internet market . in this work , we examine how to learn customer valuations in close to real-time . our contributions in this paper are threefold : ( 1 ) we develop a probabilistic model to describe customer behavior , ( 2 ) we develop a framework for pricing e-content based on basic economic principles , and ( 3 ) we propose a price discovering algorithm that learns customer behavior parameters and suggests prices to an e-content provider . we validate our algorithm using simulations . our simulations indicate that our algorithm generates revenue close to the maximum expectation . further , they also indicate that the algorithm is robust to transient customer behavior .

['probabilistic algorithms']


unsupervised feature selection for multi-cluster data in many data analysis tasks , one is often confronted with very high dimensional data . feature selection techniques are designed to find the relevant feature subset of the original features which can facilitate clustering , classification and retrieval . in this paper , we consider the feature selection problem in unsupervised learning scenario , which is particularly difficult due to the absence of class labels that would guide the search for relevant information . the feature selection problem is essentially a combinatorial optimization problem which is computationally expensive . traditional unsupervised feature selection methods address this issue by selecting the top ranked features based on certain scores computed independently for each feature . these approaches neglect the possible correlation between different features and thus can not produce an optimal feature subset . inspired from the recent developments on manifold learning and l1-regularized models for subset selection , we propose in this paper a new approach , called multi-cluster feature selection ( mcfs ) , for unsupervised feature selection . specifically , we select those features such that the multi-cluster structure of the data can be best preserved . the corresponding optimization problem can be efficiently solved since it only involves a sparse eigen-problem and a l1-regularized least squares problem . extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm .

['clustering', 'feature selection', 'unsupervised']

clustering NOUN dobj facilitate
unsupervised ADJ amod scenario
unsupervised ADJ amod methods
unsupervised ADJ amod selection

gbase : a scalable and general graph management system graphs appear in numerous applications including cyber-security , the internet , social networks , protein networks , recommendation systems , and many more . graphs with millions or even billions of nodes and edges are common-place . how to store such large graphs efficiently ? what are the core operations\/queries on those graph ? how to answer the graph queries quickly ? we propose gbase , a scalable and general graph management and mining system . the key novelties lie in 1 ) our storage and compression scheme for a parallel setting and 2 ) the carefully chosen graph operations and their efficient implementation . we designed and implemented an instance of gbase using mapreduce\/hadoop . gbase provides a parallel indexing mechanism for graph mining operations that both saves storage space , as well as accelerates queries . we ran numerous experiments on real graphs , spanning billions of nodes and edges , and we show that our proposed gbase is indeed fast , scalable and nimble , with significant savings in space and time .

['compression', 'distributed computing', 'graph', 'indexing']

graph NOUN compound management
graph NOUN pobj on
graph NOUN compound queries
graph NOUN conj scalable
compression NOUN compound scheme
graph NOUN compound operations
indexing NOUN compound mechanism
graph NOUN compound mining

interpretable nonnegative matrix decompositions a matrix decomposition expresses a matrix as a product of at least two factor matrices . equivalently , it expresses each column of the input matrix as a linear combination of the columns in the first factor matrix . the interpretability of the decompositions is a key issue in many data-analysis tasks . we propose two new matrix-decomposition problems : the nonnegative cx and nonnegative cur problems , that give naturally interpretable factors . they extend the recently-proposed column and column-row based decompositions , and are aimed to be used with nonnegative matrices . our decompositions represent the input matrix as a nonnegative linear combination of a subset of its columns ( or columns and rows ) . we present two algorithms to solve these problems and provide an extensive experimental evaluation where we assess the quality of our algorithms ' results as well as the intuitiveness of nonnegative cx and cur decompositions . we show that our algorithms return intuitive answers with smaller reconstruction errors than the previously-proposed methods for column and column-row decompositions .

['alternating least squares', 'column-row decompositions', 'local search', 'matrix decompositions']


apolo : interactive large graph sensemaking by combining machine learning and visualization we present apolo , a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets . it combines visualization , rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small , rather than starting big and drilling down . apolo helps users find relevant information by specifying exemplars , and then using a machine learning method called belief propagation to infer which other nodes may be of interest . we demonstrate apolo 's usage and benefits using a google scholar citation graph , consisting of 83,000 articles ( nodes ) and 150,000 citations relationships . a demo video of apolo is available at http:\/\/www.cs.cmu.edu\/~dchau\/apolo\/apolo.mp4 .

['belief propagation', 'large network', 'sensemaking', 'user interfaces']

sensemaking VERB ROOT sensemaking
sensemaking VERB advcl combines

diversified ranking on large graphs : an optimization viewpoint diversified ranking on graphs is a fundamental mining task and has a variety of high-impact applications . there are two important open questions here . the first challenge is the measure - how to quantify the goodness of a given top-k ranking list that captures both the relevance and the diversity ? the second challenge lies in the algorithmic aspect - how to find an optimal , or near-optimal , top-k ranking list that maximizes the measure we defined in a scalable way ? in this paper , we address these challenges from an optimization point of view . firstly , we propose a goodness measure for a given top-k ranking list . the proposed goodness measure intuitively captures both ( a ) the relevance between each individual node in the ranking list and the query ; and ( b ) the diversity among different nodes in the ranking list . moreover , we propose a scalable algorithm ( linear wrt the size of the graph ) that generates a provably near-optimal solution . the experimental evaluations on real graphs demonstrate its effectiveness and efficiency .

['diversity', 'graph mining', 'ranking', 'scalability']

ranking VERB ccomp is
ranking VERB amod viewpoint
ranking NOUN amod list
diversity NOUN conj relevance
ranking NOUN amod list
ranking NOUN amod list
ranking ADJ amod list
diversity NOUN conj relevance
ranking ADJ amod list

an energy-efficient mobile recommender system the increasing availability of large-scale location traces creates unprecedent opportunities to change the paradigm for knowledge discovery in transportation systems . a particularly promising area is to extract energy-efficient transportation patterns ( green knowledge ) , which can be used as guidance for reducing inefficiencies in energy consumption of transportation sectors . however , extracting green knowledge from location traces is not a trivial task . conventional data analysis tools are usually not customized for handling the massive quantity , complex , dynamic , and distributed nature of location traces . to that end , in this paper , we provide a focused study of extracting energy-efficient transportation patterns from location traces . specifically , we have the initial focus on a sequence of mobile recommendations . as a case study , we develop a mobile recommender system which has the ability in recommending a sequence of pick-up points for taxi drivers or a sequence of potential parking positions . the goal of this mobile recommendation system is to maximize the probability of business success . along this line , we provide a potential travel distance ( ptd ) function for evaluating each candidate sequence . this ptd function possesses a monotone property which can be used to effectively prune the search space . based on this ptd function , we develop two algorithms , lcp and skyroute , for finding the recommended routes . finally , experimental results show that the proposed system can provide effective mobile sequential recommendation and the knowledge extracted from location traces can be used for coaching drivers and leading to the efficient use of energy .

['mobile recommender system', 'trajectory data analysis']


multiple domain user personalization content personalization is a key tool in creating attractive websites . synergies can be obtained by integrating personalization between several internet properties . in this paper we propose a hierarchical bayesian model to address these issues . our model allows the integration of multiple properties without changing the overall structure , which makes it easily extensible across large internet portals . it relies at its lowest level on latent dirichlet allocation , while making use of latent side features for cross-property integration . we demonstrate the efficiency of our approach by analyzing data from several properties of a major internet portal .

['domain integration', 'latent dirichlet allocation', 'learning', 'parallel statistical inference', 'user profiling']


discriminative topic modeling based on manifold learning topic modeling has been popularly used for data analysis in various domains including text documents . previous topic models , such as probabilistic latent semantic analysis ( plsa ) and latent dirichlet allocation ( lda ) , have shown impressive success in discovering low-rank hidden structures for modeling text documents . these models , however , do not take into account the manifold structure of data , which is generally informative for the non-linear dimensionality reduction mapping . more recent models , namely laplacian plsi ( lapplsi ) and locally-consistent topic model ( ltm ) , have incorporated the local manifold structure into topic models and have shown the resulting benefits . but these approaches fall short of the full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs . in this paper , we propose discriminative topic model ( dtm ) that separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together , thereby preserving the global manifold structure as well as improving the local consistency . we also present a novel model fitting algorithm based on the generalized em and the concept of pareto improvement . as a result , dtm achieves higher classification performance in a semi-supervised setting by effectively exposing the manifold structure of data . we provide empirical evidence on text corpora to demonstrate the success of dtm in terms of classification accuracy and robustness to parameters compared to state-of-the-art techniques .

['dimensionality reduction', 'document classification', 'general', 'semi-supervised learning', 'topic modeling']


single-shot detection of multiple categories of text using parametric mixture models in this paper , we address the problem of detecting multiple topics or categories of text where each text is not assumed to belong to one of a number of mutually exclusive categories . conventionally , the binary classification approach has been employed , in which whether or not text belongs to a category is judged by the binary classifier for every category . in this paper , we propose a more sophisticated approach to simultaneously detect multiple categories of text using parametric mixture models ( pmms ) , newly presented in this paper . pmms are probabilistic generative models for text that has multiple categories . our pmms are essentially different from the conventional mixture of multinomial distributions in the sense that in the former several basis multinomial parameters are mixed in the parameter space , while in the latter several multinomial components are mixed . we derive efficient learning algorithms for pmms within the framework of the maximum a posteriori estimate . we also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of world wide web pages , focusing on those from the `` yahoo.com '' domain .

['probabilistic algorithms']


querying multiple sets of discovered rules rule mining is an important data mining task that has been applied to numerous real-world applications . often a rule mining system generates a large number of rules and only a small subset of them is really useful in applications . although there exist some systems allowing the user to query the discovered rules , they are less suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rules . in this paper , we propose a new powerful rule query language rule-ql for querying multiple rulebases that is modeled after sql and has rigorous theoretical foundations of a rule-based calculus . in particular , we first propose a rule-based calculus rc based on the first-order logic , and then present the language rule-ql that is at least as expressive as the safe fragment of rc . we also propose a number of efficient query evaluation techniques for rule-ql and test them experimentally on some representative queries to demonstrate the feasibility of rule-ql .

['association rules', 'data mining queries', 'query evaluation', 'rulebases']

rulebases NOUN pobj of
rulebases NOUN pobj for

a generalized co-hits algorithm and its application to bipartite graphs recently many data types arising from data mining and web search applications can be modeled as bipartite graphs . examples include queries and urls in query logs , and authors and papers in scientific literature . however , one of the issues is that previous algorithms only consider the content and link information from one side of the bipartite graph . there is a lack of constraints to make sure the final relevance of the score propagation on the graph , as there are many noisy edges within the bipartite graph . in this paper , we propose a novel and general co-hits algorithm to incorporate the bipartite graph with the content information from both sides as well as the constraints of relevance . moreover , we investigate the algorithm based on two frameworks , including the iterative and the regularization frameworks , and illustrate the generalized co-hits algorithm from different views . for the iterative framework , it contains hits and personalized pagerank as special cases . in the regularization framework , we successfully build a connection with hits , and develop a new cost function to consider the direct relationship between two entity sets , which leads to a significant improvement over the baseline method . to illustrate our methodology , we apply the co-hits algorithm , with many different settings , to the application of query suggestion by mining the aol query log data . experimental results demonstrate that coregu-0 .5 ( i.e. , a model of the regularization framework ) achieves the best performance with consistent and promising improvements .

['bipartite graphs', 'co-hits', 'mutual reinforcement', 'regularization', 'score propagation']

regularization NOUN compound frameworks
regularization NOUN compound framework
regularization NOUN compound framework

enabling analysts in managed services for crm analytics data analytics tools and frameworks abound , yet rapid deployment of analytics solutions that deliver actionable insights from business data remains a challenge . the primary reason is that on-field practitioners are required to be both technically proficient and knowledgeable about the business . the recent abundance of unstructured business data has thrown up new opportunities for analytics , but has also multiplied the deployment challenge , since interpretation of concepts derived from textual sources require a deep understanding of the business . in such a scenario , a managed service for analytics comes up as the best alternative . a managed analytics service is centered around a business analyst who acts as a liaison between the business and the technology . this calls for new tools that assist the analyst to be efficient in the tasks that she needs to execute . also , the analytics needs to be repeatable , in that the delivered insights should not depend heavily on the expertise of specific analysts . these factors lead us to identify new areas that open up for kdd research in terms of ` time-to-insight ' and repeatability for these analysts . we present our analytics framework in the form of a managed service offering for crm analytics . we describe different analyst-centric tools using a case study from real-life engagements and demonstrate their effectiveness .

['analytics service', 'general', 'general', 'text mining']


a sequential sampling algorithm for a general class of utility criteria

['learning']


toward autonomic grids : analyzing the job flow with affinity streaming the affinity propagation ( ap ) clustering algorithm proposed by frey and dueck ( 2007 ) provides an understandable , nearly optimal summary of a dataset , albeit with quadratic computational complexity . this paper , motivated by autonomic computing , extends ap to the data streaming framework . firstly a hierarchical strategy is used to reduce the complexity to o ( n1 + îµ ) ; the distortion loss incurred is analyzed in relation with the dimension of the data items . secondly , a coupling with a change detection test is used to cope with non-stationary data distribution , and rebuild the model as needed . the presented approach strap is applied to the stream of jobs submitted to the egee grid , providing an understandable description of the job flow and enabling the system administrator to spot online some sources of failures .

['affinity propagation', 'autonomic computing', 'online clustering']


construct robust rule sets for classification we study the problem of computing classification rule sets from relational databases so that accurate predictions can be made on test data with missing attribute values . traditional classifiers perform badly when test data are not as complete as the training data because they tailor a training database too much . we introduce the concept of one rule set being more robust than another , that is , able to make more accurate predictions on test data with missing attribute values . we show that the optimal class association rule set is as robust as the complete class association rule set . we then introduce the k-optimal rule set , which provides predictions exactly the same as the optimal class association rule set on test data with up to k missing attribute values . this leads to a hierarchy of k-optimal rule sets in which decreasing size corresponds to decreasing robustness , and they all more robust than a traditional classification rule set . we introduce two methods to find k-optimal rule sets , i.e. an optimal association rule mining approach and a heuristic approximate approach . we show experimentally that a k-optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule sets perform significantly better than a typical classification rule set ( c4 .5 rules ) on incomplete test data .

['association rule', 'classification rule', 'deduction']


partial example acquisition in cost-sensitive learning it is often expensive to acquire data in real-world data mining applications . most previous data mining and machine learning research , however , assumes that a fixed set of training examples is given . in this paper , we propose an online cost-sensitive framework that allows a learner to dynamically acquire examples as it learns , and to decide the ideal number of examples needed to minimize the total cost . we also propose a new strategy for partial example acquisition ( pas ) , in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost . experiments on uci datasets show that the new pas strategy is an effective method in reducing the total cost for data acquisition .

['active cost-sensitive learning', 'active learning', 'cost-sensitive learning', 'data acquisition', 'data mining', 'interactive and online data mining', 'machine learning']


clustering based large margin classification : a scalable approach using socp formulation this paper presents a novel second order cone programming ( socp ) formulation for large scale binary classification tasks . assuming that the class conditional densities are mixture distributions , where each component of the mixture has a spherical covariance , the second order statistics of the components can be estimated efficiently using clustering algorithms like birch . for each cluster , the second order moments are used to derive a second order cone constraint via a chebyshev-cantelli inequality . this constraint ensures that any data point in the cluster is classified correctly with a high probability . this leads to a large margin socp formulation whose size depends on the number of clusters rather than the number of training data points . hence , the proposed formulation scales well for large datasets when compared to the state-of-the-art classifiers , support vector machines ( svms ) . experiments on real world and synthetic datasets show that the proposed algorithm outperforms svm solvers in terms of training time and achieves similar accuracies .

['birch', 'gaussian mixture models', 'large margin classification', 'scalability']

birch NOUN pobj like

on effective classification of strings with wavelets in recent years , the technological advances in mapping genes have made it increasingly easy to store and use a wide variety of biological data . such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task . for example , a typical dna string may be millions of characters long , and there may be thousands of such strings in a database . in many cases , the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which can not be easily determined apriori . another problem which complicates the classification task is that in some cases the classification behavior is reflected in global behavior of the string , whereas in others it is reflected in local patterns . given the enormous variation in the behavior of the strings over different data sets , it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification . for this purpose , we will exploit the multi-resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristics at different levels of granularity . the resulting scheme turns out to be very effective in practice on a wide range of problems .

['deduction', 'representations']


from run-time behavior to usage scenarios : an interaction-pattern mining approach a key challenge facing it organizations today is their evolution towards adopting e-business practices that gives rise to the need for reengineering their underlying software systems . any reengineering effort has to be aware of the functional requirements of the subject system , in order not to violate the integrity of its intended uses . however , as software systems get regularly maintained throughout their lifecycle , the documentation of their requirements often become obsolete or get lost . to address this problem of `` software requirements loss '' , we have developed an interaction-pattern mining method for the recovery of functional requirements as usage scenarios . our method analyzes traces of the run-time system-user interaction to discover frequently recurring patterns ; these patterns correspond to the functionality currently exercised by the system users , represented as usage scenarios . the discovered scenarios provide the basis for reengineering the software system into web-accessible components , each one supporting one of the discovered scenarios . in this paper , we describe ipm2 , our interaction-pattern discovery algorithm , we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed .

['interaction reengineering', 'run-time behavior analysis', 'sequential pattern mining', 'software engineering', 'software requirements recovery', 'usage scenarios']


systematic data selection to mine concept-drifting data streams one major problem of existing methods to mine data streams is that it makes ad hoc choices to combine most recent data with some amount of old data to search the new hypothesis . the assumption is that the additional old data always helps produce a more accurate hypothesis than using the most recent data only . we first criticize this notion and point out that using old data blindly is not better than `` gambling '' ; in other words , it helps increase the accuracy only if we are `` lucky . '' we discuss and analyze the situations where old data will help and what kind of old data will help . the practical problem on choosing the right example from old data is due to the formidable cost to compare different possibilities and models . this problem will go away if we have an algorithm that is extremely efficient to compare all sensible choices with little extra cost . based on this observation , we propose a simple , efficient and accurate cross-validation decision tree ensemble method .

['concept-drift', 'data streams', 'decision trees']


mining heterogeneous gene expression data with time lagged recurrent neural networks heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment , disease development and drug effect at the molecular level . in this paper for both exploring and prediction purposes a time lagged recurrent neural network with trajectory learning is proposed for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments . the proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time . also , the trajectory learning with back-propagation through time algorithm can recognize gene expression patterns vary over time . this may reveal much more information about the regulatory network underlying gene expressions . the analyzed data were extracted from spotted dna microarrays in the budding yeast expression measurements , produced by eisen et al. . the gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions . the number of recognized gene patterns in our study ranged from two to ten and were divided into three cases . optimal network architectures with different memory structures were selected based on akaike and bayesian information statistical criteria using two-way factorial design . the optimal model performance was compared to other popular gene classification algorithms such as nearest neighbor , support vector machine , and self-organized map . the reliability of the performance was verified with multiple iterated runs .

['backpropagation through time', 'gene expression', 'heterogeneous', 'probabilistic algorithms', 'self-modifying machines', 'time lagged neural network', 'trajectory learning']

heterogeneous ADJ amod data
heterogeneous ADJ amod types
heterogeneous ADJ amod series
heterogeneous ADJ amod time
heterogeneous ADJ amod conditions

graphs over time : densification laws , shrinking diameters and possible explanations how do real graphs evolve over time ? what are `` normal '' growth patterns in social , technological , and information networks ? many studies have discovered patterns in static graphs , identifying properties in a single snapshot of a large network , or in a very small number of snapshots ; these include heavy tails for in - and out-degree distributions , communities , small-world phenomena , and others . however , given the lack of information about network evolution over long periods , it has been hard to convert these findings into statements about trends over time . here we study a wide range of real graphs , and we observe some surprising phenomena . first , most of these graphs densify over time , with the number of edges growing super-linearly in the number of nodes . second , the average distance between nodes often shrinks over time , in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes ( like o ( log n ) or o ( log ( log n ) ) . existing graph generation models do not exhibit these types of behavior , even at a qualitative level . we provide a new graph generator , based on a `` forest fire '' spreading process , that has a simple , intuitive justification , requires very few parameters ( like the `` flammability '' of nodes ) , and produces graphs exhibiting the full range of properties observed both in prior work and in the present study .

['densification power laws', 'graph generators', 'graph mining', 'heavy-tailed distributions', 'small-world phenomena']


reasoning about sets using redescription mining redescription mining is a newly introduced data mining problem that seeks to find subsets of data that afford multiple definitions . it can be viewed as a generalization of association rule mining , from finding implications to equivalences ; as a form of conceptual clustering , where the goal is to identify clusters that afford dual characterizations ; and as a form of constructive induction , to build features based on given descriptors that mutually reinforce each other . in this paper , we present the use of redescription mining as an important tool to reason about a collection of sets , especially their overlaps , similarities , and differences . we outline algorithms to mine all minimal ( non-redundant ) redescriptions underlying a dataset using notions of minimal generators of closed itemsets . we also show the use of these algorithms in an interactive context , supporting constraint-based exploration and querying . specifically , we showcase a bioinformatics application that empowers the biologist to define a vocabulary of sets underlying a domain of genes and to reason about these sets , yielding significant biological insight .

['closed itemsets', 'learning', 'minimal generators', 'redescription']

redescription NOUN compound mining

mining a stream of transactions for customer patterns transaction data can arrive at a ferocious rate in the order that transactions are completed . the data contain an enormous amount of information about customers , not just transactions , but extracting up-to-date customer information from an ever changing stream of data and mining it in real-time is a challenge . this paper describes a statistically principled approach to designing short , accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions . a signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately , as an empirical study of the calling patterns of 96,000 wireless customers who made about 18 million wireless calls over a three month period shows .

['approximate queries', 'customer profiles', 'dynamic database', 'histograms', 'incremental updates', 'massive data', 'signatures']

signatures NOUN conj summaries

similarity measure based on partial information of time series similarity measure of time series is an important subroutine in many kdd applications . previous similarity models mainly focus on the prominent series behaviors by considering the whole information of time series . in this paper , we address the problem : which portion of information is more suitable for similarity measure for the data collected from a certain field . we propose a model for the retrieval and representation of the partial information in time series data , and a methodology for evaluating the similarity measurements based on partial information . the methodology is to retrieve various portions of information from the raw data and represent it in a concise form , then cluster the time series using the partial information and evaluate the similarity measurements through comparing the results with a standard classification . experiments on data set from stock market give some interesting observations and justify the usefulness of our approach .

['partial information', 'similarity measure', 'time series']


pet : a statistical model for popular events tracking in social communities user generated information in online communities has been characterized with the mixture of a text stream and a network structure both changing over time . a good example is a web-blogging community with the daily blog posts and a social network of bloggers . an important task of analyzing an online community is to observe and track the popular events , or topics that evolve over time in the community . existing approaches usually focus on either the burstiness of topics or the evolution of networks , but ignoring the interplay between textual topics and network structures . in this paper , we formally define the problem of popular event tracking in online communities ( pet ) , focusing on the interplay between texts and networks . we propose a novel statistical method that models the the popularity of events over time , taking into consideration the burstiness of user interest , information diffusion on the network structure , and the evolution of textual topics . specifically , a gibbs random field is defined to model the influence of historic status and the dependency relationships in the graph ; thereafter a topic model generates the words in text content of the event , regularized by the gibbs random field . we prove that two classic models in information diffusion and text burstiness are special cases of our model under certain situations . empirical experiments with two different communities and datasets ( i.e. , twitter and dblp ) show that our approach is effective and outperforms existing approaches .

['pet', 'popular events tracking', 'social communities', 'topic modeling']

pet NOUN nsubjpass characterized
pet PROPN appos communities

on community outliers and their efficient detection in information networks linked or networked data are ubiquitous in many applications . examples include web data or hypertext documents connected via hyperlinks , social networks or user profiles connected via friend links , co-authorship and citation information , blog data , movie reviews and so on . in these datasets ( called `` information networks '' ) , closely related objects that share the same properties or interests form a community . for example , a community in blogsphere could be users mostly interested in cell phone reviews and news . outlier detection in information networks can reveal important anomalous and interesting behaviors that are not obvious if community information is ignored . an example could be a low-income person being friends with many rich people even though his income is not anomalously low when considered over the entire population . this paper first introduces the concept of community outliers ( interesting points or rising stars for a more positive sense ) , and then shows that well-known baseline approaches without considering links or community information can not find these community outliers . we propose an efficient solution by modeling networked data as a mixture model composed of multiple normal communities and a set of randomly generated outliers . the probabilistic model characterizes both data and links simultaneously by defining their joint distribution based on hidden markov random fields ( hmrf ) . maximizing the data likelihood and the posterior of the model gives the solution to the outlier inference problem . we apply the model on both synthetic data and dblp data sets , and the results demonstrate importance of this concept , as well as the effectiveness and efficiency of the proposed approach .

['community discovery', 'information networks', 'outlier detection']


ranking-based classification of heterogeneous information networks it has been recently recognized that heterogeneous information networks composed of multiple types of nodes and links are prevalent in the real world . both classification and ranking of the nodes ( or data objects ) in such networks are essential for network analysis . however , so far these approaches have generally been performed separately . in this paper , we combine ranking and classification in order to perform more accurate analysis of a heterogeneous information network . our intuition is that highly ranked objects within a class should play more important roles in classification . on the other hand , class membership information is important for determining a quality ranking over a dataset . we believe it is therefore beneficial to integrate classification and ranking in a simultaneous , mutually enhancing process , and to this end , propose a novel ranking-based iterative classification framework , called rankclass . specifically , we build a graph-based ranking model to iteratively compute the ranking distribution of the objects within each class . at each iteration , according to the current ranking results , the graph structure used in the ranking algorithm is adjusted so that the sub-network corresponding to the specific class is emphasized , while the rest of the network is weakened . as our experiments show , integrating ranking with classification not only generates more accurate classes than the state-of-art classification methods on networked data , but also provides meaningful ranking of objects within each class , serving as a more informative view of the data than traditional classification .

['classification', 'heterogeneous information network', 'ranking']

classification NOUN nsubjpass recognized
classification NOUN nsubj are
ranking NOUN conj classification
ranking VERB dobj combine
classification NOUN conj ranking
classification NOUN pobj in
ranking VERB acl quality
classification NOUN dobj integrate
ranking VERB conj classification
ranking NOUN npadvmod based
classification NOUN compound framework
ranking ADJ amod model
ranking ADJ amod distribution
ranking VERB amod results
ranking VERB amod algorithm
ranking VERB xcomp integrating
classification NOUN pobj with
classification NOUN compound methods
ranking NOUN dobj provides
classification NOUN pobj than

probabilistic topic models with biased propagation on heterogeneous information networks with the development of web applications , textual documents are not only getting richer , but also ubiquitously interconnected with users and other objects in various ways , which brings about text-rich heterogeneous information networks . topic models have been proposed and shown to be useful for document analysis , and the interactions among multi-typed objects play a key role at disclosing the rich semantics of the network . however , most of topic models only consider the textual information while ignore the network structures or can merely integrate with homogeneous networks . none of them can handle heterogeneous information network well . in this paper , we propose a novel topic model with biased propagation ( tmbp ) algorithm to directly incorporate heterogeneous information network with topic modeling in a unified way . the underlying intuition is that multi-typed objects should be treated differently along with their inherent textual information and the rich semantics of the heterogeneous information network . a simple and unbiased topic propagation across such a heterogeneous network does not make much sense . consequently , we investigate and develop two biased propagation frameworks , the biased random walk framework and the biased regularization framework , for the tmbp algorithm from different perspectives , which can discover latent topics and identify clusters of multi-typed objects simultaneously . we extensively evaluate the proposed approach and compare to the state-of-the-art techniques on several datasets . experimental results demonstrate that the improvement in our proposed approach is consistent and promising .

['biased propagation', 'heterogeneous information network', 'topic modeling']


k-nn as an implementation of situation testing for discrimination discovery and prevention with the support of the legally-grounded methodology of situation testing , we tackle the problems of discrimination discovery and prevention from a dataset of historical decisions by adopting a variant of k-nn classification . a tuple is labeled as discriminated if we can observe a significant difference of treatment among its neighbors belonging to a protected-by-law group and its neighbors not belonging to it . discrimination discovery boils down to extracting a classification model from the labeled tuples . discrimination prevention is tackled by changing the decision value for tuples labeled as discriminated before training a classifier . the approach of this paper overcomes legal weaknesses and technical limitations of existing proposals .

['discrimination discovery and prevention', 'k-nn classification']


trading representability for scalability : adaptive multi-hyperplane machine for nonlinear classification support vector machines ( svms ) are among the most popular and successful classification algorithms . kernel svms often reach state-of-the-art accuracies , but suffer from the curse of kernelization due to linear model growth with data size on noisy data . linear svms have the ability to efficiently learn from truly large data , but they are applicable to a limited number of domains due to low representational power . to fill the representability and scalability gap between linear and nonlinear svms , we propose the adaptive multi-hyperplane machine ( amm ) algorithm that accomplishes fast training and prediction and has capability to solve nonlinear classification problems . amm model consists of a set of hyperplanes ( weights ) , each assigned to one of the multiple classes , and predicts based on the associated class of the weight that provides the largest prediction . the number of weights is automatically determined through an iterative algorithm based on the stochastic gradient descent algorithm which is guaranteed to converge to a local optimum . since the generalization bound decreases with the number of weights , a weight pruning mechanism is proposed and analyzed . the experiments on several large data sets show that amm is nearly as fast during training and prediction as the state-of-the-art linear svm solver and that it can be orders of magnitude faster than kernel svm . in accuracy , amm is somewhere between linear and kernel svms . for example , on an ocr task with 8 million highly dimensional training examples , amm trained in 300 seconds on a single-core processor had 0.54 % error rate , which was significantly lower than 2.03 % error rate of a linear svm trained in the same time and comparable to 0.43 % error rate of a kernel svm trained in 2 days on 512 processors . the results indicate that amm could be an attractive option when solving large-scale classification problems . the software is available at www.dabi.temple.edu\/~vucetic\/amm.html .

['large-scale learning', 'nonlinear classification', 'stochastic gradient descent', 'support vector machines']


spatially regularized logistic regression for disease mapping on large moving populations spatial analysis of disease risk , or disease mapping , typically relies on information about the residence and health status of individuals from population under study . however , residence information has its limitations because people are exposed to numerous disease risks as they spend time outside of their residences . thanks to the wide-spread use of mobile phones and gps-enabled devices , it is becoming possible to obtain a detailed record about the movement of human populations . availability of movement information opens up an opportunity to improve the accuracy of disease mapping . starting with an assumption that an individual 's disease risk is a weighted average of risks at the locations which were visited , we show that disease mapping can be accomplished by spatially regularized logistic regression . due to the inherent sparsity of movement data , the proposed approach can be applied to large populations and over large spatial grids . in our experiments , we were able to map disease for a simulated population with 1.6 million people and a spatial grid with 65 thousand locations in several minutes . the results indicate that movement information can improve the accuracy of disease mapping as compared to residential data only . we also studied a privacy-preserving scenario in which only the aggregate statistics are available about the movement of the overall population , while detailed movement information is available only for individuals with disease . the results indicate that the accuracy of disease mapping remains satisfactory when learning from movement data sanitized in this way .

['disease mapping', 'movement trajectories', 'privacy', 'regularization', 'spatial epidemiology', 'spatial-temporal data mining']

privacy NOUN npadvmod preserving

compression of weighted graphs we propose to compress weighted graphs ( networks ) , motivated by the observation that large networks of social , biological , or other relations can be complex to handle and visualize . in the process also known as graph simplification , nodes and ( unweighted ) edges are grouped to supernodes and superedges , respectively , to obtain a smaller graph . we propose models and algorithms for weighted graphs . the interpretation ( i.e. decompression ) of a compressed , weighted graph is that a pair of original nodes is connected by an edge if their supernodes are connected by one , and that the weight of an edge is approximated to be the weight of the superedge . the compression problem now consists of choosing supernodes , superedges , and superedge weights so that the approximation error is minimized while the amount of compression is maximized . in this paper , we formulate this task as the ` simple weighted graph compression problem ' . we then propose a much wider class of tasks under the name of ` generalized weighted graph compression problem ' . the generalized task extends the optimization to preserve longer-range connectivities between nodes , not just individual edge weights . we study the properties of these problems and propose a range of algorithms to solve them , with different balances between complexity and quality of the result . we evaluate the problems and algorithms experimentally on real networks . the results indicate that weighted graphs can be compressed efficiently with relatively little compression error .

['compression', 'graph mining', 'network', 'weighted graph']

compression NOUN ROOT compression
compression NOUN compound problem
compression NOUN pobj of
compression NOUN compound problem
compression NOUN compound problem
compression NOUN compound error

exploiting vulnerability to secure user privacy on a social networking site as ( one 's ) social network expands , a user 's privacy protection goes beyond his privacy settings and becomes a social networking problem . in this research , we aim to address some critical issues related to privacy protection : would the highest privacy settings guarantee a secure protection ? given the open nature of social networking sites , is it possible to manage one 's privacy protection ? with the diversity of one 's social media friends , how can one figure out an effective approach to balance between vulnerability and privacy ? we present a novel way to define a vulnerable friend from an individual user 's perspective is dependent on whether or not the user 's friends ' privacy settings protect the friend and the individual 's network of friends ( which includes the user ) . as a single vulnerable friend in a user 's social network might place all friends at risk , we resort to experiments and observe how much security an individual user can improve by unfriending a vulnerable friend . we also show how privacy weakens if newly accepted friends are unguarded or unprotected . this work provides a large-scale evaluation of new security and privacy indexes using a facebook dataset . we present and discuss a new perspective for reasoning about social networking security . when a user accepts a new friend , the user should ensure that the new friend is not an increased security risk with the potential of negatively impacting the entire friend network . additionally , by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends , it is possible to further improve security and privacy without changing the social networking site 's existing architecture .

['privacy', 'social network', 'vulnerability']

vulnerability NOUN dobj exploiting
privacy NOUN dobj secure
privacy NOUN compound protection
privacy NOUN compound settings
privacy NOUN compound protection
privacy NOUN compound settings
privacy NOUN compound protection
vulnerability NOUN pobj between
privacy NOUN conj vulnerability
privacy NOUN compound settings
privacy NOUN nsubj weakens
privacy NOUN compound indexes
privacy NOUN conj security

friendship and mobility : user movement in location-based social networks even though human movement and mobility patterns have a high degree of freedom and variation , they also exhibit structural patterns due to geographic and social constraints . using cell phone location data , as well as data from two online location-based social networks , we aim to understand what basic laws govern human motion and dynamics . we find that humans experience a combination of periodic movement that is geographically limited and seemingly random jumps correlated with their social networks . short-ranged travel is periodic both spatially and temporally and not effected by the social network structure , while long-distance travel is more influenced by social network ties . we show that social relationships can explain about 10 % to 30 % of all human movement , while periodic behavior explains 50 % to 70 % . based on our findings , we develop a model of human mobility that combines periodic short range movements with travel due to the social network structure . we show that our model reliably predicts the locations and dynamics of future human movement and gives an order of magnitude better performance than present models of human mobility .

['communication networks', 'human mobility', 'social networks']


an integrated machine learning approach to stroke prediction stroke is the third leading cause of death and the principal cause of serious long-term disability in the united states . accurate prediction of stroke is highly valuable for early intervention and treatment . in this study , we compare the cox proportional hazards model with a machine learning approach for stroke prediction on the cardiovascular health study ( chs ) dataset . specifically , we consider the common problems of data imputation , feature selection , and prediction in medical datasets . we propose a novel automatic feature selection algorithm that selects robust features based on our proposed heuristic : conservative mean . combined with support vector machines ( svms ) , our proposed feature selection algorithm achieves a greater area under the roc curve ( auc ) as compared to the cox proportional hazards model and l1 regularized cox feature selection algorithm . furthermore , we present a margin-based censored regression algorithm that combines the concept of margin-based classifiers with censored regression to achieve a better concordance index than the cox model . overall , our approach outperforms the current state-of-the-art in both metrics of auc and concordance index . in addition , our work has also identified potential risk factors that have not been discovered by traditional approaches . our method can be applied to clinical prediction of other diseases , where missing data are common and risk factors are not well understood .

['benchmark', 'classification', 'concordance index', 'data analysis', 'feature selection', 'healthcare', 'medical data analysis', 'prediction', 'roc', 'stroke', 'stroke prediction', 'svm']

stroke VERB compound stroke
prediction NOUN compound stroke
stroke NOUN pobj to
prediction NOUN nsubj is
stroke NOUN pobj of
stroke NOUN compound prediction
prediction NOUN pobj for
prediction NOUN conj selection
roc NOUN compound curve
prediction NOUN pobj to

clustering with relative constraints recent studies have suggested using relative distance comparisons as constraints to represent domain knowledge . a natural extension to relative comparisons is the combination of two comparisons defined on the same set of three instances . constraints in this form , termed relative constraints , provide a unified knowledge representation for both partitional and hierarchical clusterings . but many key properties of relative constraints remain unknown . in this paper , we answer the following important questions that enable the broader application of relative constraints in general clustering problems : '' feasibility : does there exist a clustering that satisfies a given set of relative constraints ? ( consistency of constraints ) `` completeness : given a set of consistent relative constraints , how can one derive a complete clustering without running into dead-ends ? '' informativeness : how can one extract the most informative relative constraints from given knowledge sources ? we show that any hierarchical domain knowledge can be easily represented by relative constraints . we further present a hierarchical algorithm that finds a clustering satisfying all given constraints in polynomial time . experiments showed that our algorithm achieves significantly higher accuracy than the existing metric learning approach based on relative comparisons .

['constrained clustering', 'hierarchical clustering', 'relative constraints']


logical-shapelets : an expressive primitive for time series classification time series shapelets are small , local patterns in a time series that are highly predictive of a class and are thus very useful features for building classifiers and for certain visualization and summarization tasks . while shapelets were introduced only recently , they have already seen significant adoption and extension in the community . despite their immense potential as a data mining primitive , there are two important limitations of shapelets . first , their expressiveness is limited to simple binary presence\/absence questions . second , even though shapelets are computed offline , the time taken to compute them is significant . in this work , we address the latter problem by introducing a novel algorithm that finds shapelets in less time than current methods by an order of magnitude . our algorithm is based on intelligent caching and reuse of computations , and the admissible pruning of the search space . because our algorithm is so fast , it creates an opportunity to consider more expressive shapelet queries . in particular , we show for the first time an augmented shapelet representation that distinguishes the data based on conjunctions or disjunctions of shapelets . we call our novel representation logical-shapelets . we demonstrate the efficiency of our approach on the classic benchmark datasets used for these problems , and show several case studies where logical shapelets significantly outperform the original shapelet representation and other time series classification techniques . we demonstrate the utility of our ideas in domains as diverse as gesture recognition , robotics , and biometrics .

['classification', 'decision tree', 'information gain', 'time series']

classification NOUN compound time
classification NOUN compound techniques

on the privacy of anonymized networks the proliferation of online social networks , and the concomitant accumulation of user data , give rise to hotly debated issues of privacy , security , and control . one specific challenge is the sharing or public release of anonymized data without accidentally leaking personally identifiable information ( pii ) . unfortunately , it is often difficult to ascertain that sophisticated statistical techniques , potentially employing additional external data sources , are unable to break anonymity . in this paper , we consider an instance of this problem , where the object of interest is the structure of a social network , i.e. , a graph describing users and their links . recent work demonstrates that anonymizing node identities may not be sufficient to keep the network private : the availability of node and link data from another domain , which is correlated with the anonymized network , has been used to re-identify the anonymized nodes . this paper is about conditions under which such a de-anonymization process is possible . we attempt to shed light on the following question : can we assume that a sufficiently sparse network is inherently anonymous , in the sense that even with unlimited computational power , de-anonymization is impossible ? our approach is to introduce a random graph model for a version of the de-anonymization problem , which is parameterized by the expected node degree and a similarity parameter that controls the correlation between two graphs over the same vertex set . we find simple conditions on these parameters delineating the boundary of privacy , and show that the mean node degree need only grow slightly faster than log n with network size n for nodes to be identifiable . our results have policy implications for sharing of anonymized network information .

['de-anonymization', 'general', 'graph sampling', 'network privacy', 'random graphs', 'social networks']


mining top-k frequent items in a data stream with flexible sliding windows we study the problem of finding the k most frequent items in a stream of items for the recently proposed max-frequency measure . based on the properties of an item , the max-frequency of an item is counted over a sliding window of which the length changes dynamically . besides being parameterless , this way of measuring the support of items was shown to have the advantage of a faster detection of bursts in a stream , especially if the set of items is heterogeneous . the algorithm that was proposed for maintaining all frequent items , however , scales poorly when the number of items becomes large . therefore , in this paper we propose , instead of reporting all frequent items , to only mine the top-k most frequent ones . first we prove that in order to solve this problem exactly , we still need a prohibitive amount of memory ( at least linear in the number of items ) . yet , under some reasonable conditions , we show both theoretically and empirically that a memory-efficient algorithm exists . a prototype of this algorithm is implemented and we present its performance w.r.t. memory-efficiency on real-life data and in controlled experiments with synthetic data .

['data stream mining', 'top-k frequent items']


smoothing techniques for adaptive online language models : topic tracking in tweet streams we are interested in the problem of tracking broad topics such as `` baseball '' and `` fashion '' in continuous streams of short texts , exemplified by tweets from the microblogging service twitter . the task is conceived as a language modeling problem where per-topic models are trained using hashtags in the tweet stream , which serve as proxies for topic labels . simple perplexity-based classifiers are then applied to filter the tweet stream for topics of interest . within this framework , we evaluate , both intrinsically and extrinsically , smoothing techniques for integrating `` foreground '' models ( to capture recency ) and `` background '' models ( to combat sparsity ) , as well as different techniques for retaining history . experiments show that unigram language models smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task .

['general', 'stream processing', 'tdt', 'twitter']

twitter ADV advmod exemplified

online allocation of display ads with smooth delivery display ads on the internet are often sold in bundles of thousands or millions of impressions over a particular time period , typically weeks or months . ad serving systems that assign ads to pages on behalf of publishers must satisfy these contracts , but at the same time try to maximize overall quality of placement . this is usually modeled in the literature as an online allocation problem , where contracts are represented by overall delivery constraints over a finite time horizon . however this model misses an important aspect of ad delivery : time homogeneity . advertisers who buy these packages expect their ad to be shown smoothly throughout the purchased time period , in order to reach a wider audience , to have a sustained impact , and to support the ads they are running on other media ( e.g. , television ) . in this paper we formalize this problem using several nested packing constraints , and develop a tight ( 1-1 \/ e ) - competitive online algorithm for this problem . our algorithms and analysis require novel techniques as they involve online computation of multiple dual variables per ad . we then show the effectiveness of our algorithms through exhaustive simulation studies on real data sets .

['ad allocation', 'display ads', 'general', 'nonnumerical algorithms and problems', 'online matching', 'smooth delivery']


applying data mining techniques to address disaster information management challenges on mobile devices the improvement of crisis management and disaster recovery techniques are national priorities in the wake of man-made and nature inflicted calamities of the last decade . our prior work has demonstrated that the efficiency of sharing and managing information plays an important role in business recovery efforts after disaster event . with the proliferation of smart phones and wireless tablets , professionals who have an operational responsibility in disaster situations are relying on such devices to maintain communication . further , with the rise of social media , technology savvy consumers are also using these devices extensively for situational updates . in this paper , we address several critical tasks which can facilitate information sharing and collaboration between both private and public sector participants for major disaster recovery planning and management . we design and implement an all-hazard disaster situation browser ( adsb ) system that runs on apple 's mobile operating system ( ios ) and iphone and ipad mobile devices . our proposed techniques create a collaborative solution on a mobile platform using advanced data mining and information retrieval techniques for disaster preparedness and recovery that helps impacted communities better understand the current disaster situation and how the community is recovering . specifically , hierarchical summarization techniques are used to generate brief reviews from a large collection of reports at different granularities ; probabilistic models are proposed to dynamically generate query forms based on user 's feedback ; and recommendation techniques are adapted to help users identify potential contacts for report sharing and community organization . furthermore , the developed techniques are designed to be all-hazard capable so that they can be used in earthquake , terrorism , or other unanticipated disaster situations .

['disaster information management', 'dynamic query form', 'hierarchical summarization', 'user recommendation']


mining and summarizing customer reviews merchants selling products on the web often ask their customers to review the products that they have purchased and the associated services . as e-commerce is becoming more and more popular , the number of customer reviews that a product receives grows rapidly . for a popular product , the number of reviews can be in hundreds or even thousands . this makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product . it also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions . for the manufacturer , there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products . in this research , we aim to mine and to summarize all the customer reviews of a product . this summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative . we do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization . our task is performed in three steps : ( 1 ) mining product features that have been commented on by customers ; ( 2 ) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative ; ( 3 ) summarizing the results . this paper proposes several novel techniques to perform these tasks . our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques .

['reviews', 'sentiment classification', 'summarization', 'text mining']

reviews NOUN pobj of
reviews NOUN pobj of
reviews NOUN dobj summarize
summarization NOUN compound task
summarization NOUN pobj from
reviews NOUN dobj summarize
reviews NOUN pobj from
summarization NOUN pobj in
reviews NOUN dobj using

mining uncertain data with probabilistic guarantees data uncertainty is inherent in applications such as sensor monitoring systems , location-based services , and biological databases . to manage this vast amount of imprecise information , probabilistic databases have been recently developed . in this paper , we study the discovery of frequent patterns and association rules from probabilistic data under the possible world semantics . this is technically challenging , since a probabilistic database can have an exponential number of possible worlds . we propose two effcient algorithms , which discover frequent patterns in bottom-up and top-down manners . both algorithms can be easily extended to discover maximal frequent patterns . we also explain how to use these patterns to generate association rules . extensive experiments , using real and synthetic datasets , were conducted to validate the performance of our methods .

['association rule', 'frequent pattern', 'uncertain data']


predictive client-side profiles for personalized advertising personalization is ubiquitous in modern online applications as it provides significant improvements in user experience by adapting it to inferred user preferences . however , there are increasing concerns related to issues of privacy and control of the user data that is aggregated by online systems to power personalized experiences . these concerns are particularly significant for user profile aggregation in online advertising . this paper describes a practical , learning-driven client-side personalization approach for keyword advertising platforms , an emerging application previously not addressed in literature . our approach relies on storing user-specific information entirely within the user 's control ( in a browser cookie or browser local storage ) , thus allowing the user to view , edit or purge it at any time ( e.g. , via a dedicated webpage ) . we develop a principled , utility-based formulation for the problem of iteratively updating user profiles stored client-side , which relies on calibrated prediction of future user activity . while optimal profile construction is np-hard for pay-per-click advertising with bid increments , it can be efficiently solved via a greedy approximation algorithm guaranteed to provide a near-optimal solution due to the fact that keyword profile utility is submodular : it exhibits the property of diminishing returns with increasing profile size . we empirically evaluate client-side keyword profiles for keyword advertising on a large-scale dataset from a major search engine . experiments demonstrate that predictive client-side personalization allows ad platforms to retain almost all of the revenue gains from personalization even if they give users the freedom to opt out of behavior tracking backed by server-side storage . additionally , we show that advertisers can potentially increase their return on investment significantly by utilizing bid increments for keyword profiles in their ad campaigns .

['client-side personalization', 'online advertising']


high-precision phrase-based document classification on a modern scale we present a document classification system that employs lazy learning from labeled phrases , and argue that the system can be highly effective whenever the following property holds : most of information on document labels is captured in phrases . we call this property near sufficiency . our research contribution is twofold : ( a ) we quantify the near sufficiency property using the information bottleneck principle and show that it is easy to check on a given dataset ; ( b ) we reveal that in all practical cases -- from small-scale to very large-scale -- manual labeling of phrases is feasible : the natural language constrains the number of common phrases composed of a vocabulary to grow linearly with the size of the vocabulary . both these contributions provide firm foundation to applicability of the phrase-based classification ( pbc ) framework to a variety of large-scale tasks . we deployed the pbc system on the task of job title classification , as a part of linkedin 's data standardization effort . the system significantly outperforms its predecessor both in terms of precision and coverage . it is currently being used in linkedin 's ad targeting product , and more applications are being developed . we argue that pbc excels in high explainability of the classification results , as well as in low development and low maintenance costs . we benchmark pbc against existing high-precision document classification algorithms and conclude that it is most useful in multilabel classification .

['high-precision classification', 'large-scale classification', 'multilabel text classification']


serendipitous learning : learning beyond the predefined label space most traditional supervised learning methods are developed to learn a model from labeled examples and use this model to classify the unlabeled ones into the same label space predefined by the models . however , in many real world applications , the label spaces for both the labeled\/training and unlabeled\/testing examples can be different . to solve this problem , this paper proposes a novel notion of serendipitous learning ( sl ) , which is defined to address the learning scenarios in which the label space can be enlarged during the testing phase . in particular , a large margin approach is proposed to solve sl . the basic idea is to leverage the knowledge in the labeled examples to help identify novel\/unknown classes , and the large margin formulation is proposed to incorporate both the classification loss on the examples within the known categories , as well as the clustering loss on the examples in unknown categories . an efficient optimization algorithm based on cccp and the bundle method is proposed to solve the optimization problem of the large margin formulation of sl . moreover , an efficient online learning method is proposed to address the issue of large scale data in online learning scenario , which has been shown to have a guaranteed learning regret . an extensive set of experimental results on two synthetic datasets and two datasets from real world applications demonstrate the advantages of the proposed method over several other baseline algorithms . one limitation of the proposed method is that the number of unknown classes is given in advance . it may be possible to remove this constraint if we model it by using a non-parametric way . we also plan to do experiments on more real world applications in the future .

['label space', 'maximum margin classification', 'serendipitous learning']


experiences with mining temporal event sequences from electronic medical records : initial successes and some challenges the standardization and wider use of electronic medical records ( emr ) creates opportunities for better understanding patterns of illness and care within and across medical systems . our interest is in the temporal history of event codes embedded in patients ' records , specifically investigating frequently occurring sequences of event codes across patients . in studying data from more than 1.6 million patient histories at the university of michigan health system we quickly realized that frequent sequences , while providing one level of data reduction , still constitute a serious analytical challenge as many involve alternate serializations of the same sets of codes . to further analyze these sequences , we designed an approach where a partial order is mined from frequent sequences of codes . we demonstrate an emr mining system called emrview that enables exploration of the precedence relationships to quickly identify and visualize partial order information encoded in key classes of patients . we demonstrate some important nuggets learned through our approach and also outline key challenges for future research based on our experiences .

['medical informatics', 'partial orders', 'temporal data mining']


improving predictions using aggregate information in domains such as consumer products or manufacturing amongst others , we have problems that warrant the prediction of a continuous target . besides the usual set of explanatory attributes we may also have exact ( or approximate ) estimates of aggregated targets , which are the sums of disjoint sets of individual targets that we are trying to predict . hence , the question now becomes can we use these aggregated targets , which are a coarser piece of information , to improve the quality of predictions of the individual targets ? in this paper , we provide a simple yet provable way of accomplishing this . in particular , given predictions from any regression model of the target on the test data , we elucidate a provable method for improving these predictions in terms of mean squared error , given exact ( or accurate enough ) information of the aggregated targets . these estimates of the aggregated targets may be readily available or obtained -- through multilevel regression -- at different levels of granularity . based on the proof of our method we suggest a criterion for choosing the appropriate level . moreover , in addition to estimates of the aggregated targets , if we have exact ( or approximate ) estimates of the mean and variance of the target distribution , then based on our general strategy we provide an optimal way of incorporating this information so as to further improve the quality of predictions of the individual targets . we then validate the results and our claims by conducting experiments on synthetic and real industrial data obtained from diverse domains .

['coarse to fine', 'general', 'hierarchical', 'regression']

regression NOUN compound model
regression NOUN pobj through
general ADJ amod strategy

prominent streak discovery in sequence data this paper studies the problem of prominent streak discovery in sequence data . given a sequence of values , a prominent streak is a long consecutive subsequence consisting of only large ( small ) values . for finding prominent streaks , we make the observation that prominent streaks are skyline points in two dimensions - streak interval length and minimum value in the interval . our solution thus hinges upon the idea to separate the two steps in prominent streak discovery ' candidate streak generation and skyline operation over candidate streaks . for candidate generation , we propose the concept of local prominent streak ( lps ) . we prove that prominent streaks are a subset of lpss and the number of lpss is less than the length of a data sequence , in comparison with the quadratic number of candidates produced by a brute-force baseline method . we develop efficient algorithms based on the concept of lps . the non-linear lps-based method ( nlps ) considers a superset of lpss as candidates , and the linear lps-based method ( llps ) further guarantees to consider only lpss . the results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method .

['sequence database', 'skyline query', 'time-series database']


mining closed episodes with simultaneous events sequential pattern discovery is a well-studied field in data mining . episodes are sequential patterns describing events that often occur in the vicinity of each other . episodes can impose restrictions to the order of the events , which makes them a versatile technique for describing complex patterns in the sequence . most of the research on episodes deals with special cases such as serial , parallel , and injective episodes , while discovering general episodes is understudied . in this paper we extend the definition of an episode in order to be able to represent cases where events often occur simultaneously . we present an efficient and novel miner for discovering frequent and closed general episodes . such a task presents unique challenges . firstly , we can not define closure based on frequency . we solve this by computing a more conservative closure that we use to reduce the search space and discover the closed episodes as a postprocessing step . secondly , episodes are traditionally presented as directed acyclic graphs . we argue that this representation has drawbacks leading to redundancy in the output . we solve these drawbacks by defining a subset relationship in such a way that allows us to remove the redundant episodes . we demonstrate the efficiency of our algorithm and the need for using closed episodes empirically on synthetic and real-world datasets .

['closed episodes', 'depth-first search', 'frequent episodes']


sparsification of influence networks we present spine , an efficient algorithm for finding the `` backbone '' of an influence network . given a social graph and a log of past propagations , we build an instance of the independent-cascade model that describes the propagations . we aim at reducing the complexity of that model , while preserving most of its accuracy in describing the data . we show that the problem is inapproximable and we present an optimal , dynamic-programming algorithm , whose search space , albeit exponential , is typically much smaller than that of the brute force , exhaustive-search approach . seeking a practical , scalable approach to sparsification , we devise spine , a greedy , efficient algorithm with practically little compromise in quality . we claim that sparsification is a fundamental data-reduction operation with many applications , ranging from visualization to exploratory and descriptive data analysis . as a proof of concept , we use spine on real-world datasets , revealing the backbone of their influence-propagation networks . moreover , we apply spine as a pre-processing step for the influence-maximization problem , showing that computations on sparsified models give up little accuracy , but yield significant improvements in terms of scalability .

['influence', 'propagation', 'social networks']

influence NOUN compound networks
influence NOUN compound network
influence NOUN compound propagation
propagation NOUN compound networks
influence NOUN compound maximization

making every bit count : fast nonlinear axis scaling existing axis scaling and dimensionality methods focus on preserving structure , usually determined via the euclidean distance . in other words , they inherently assume that the euclidean distance is already correct . we instead propose a novel nonlinear approach driven by an information-theoretic viewpoint , which we show is also strongly linked to intrinsic dimensionality , or degrees of freedom ; and uniformity . nonlinear transformations based on common probability distributions , combined with information-driven selection , simultaneously reduce the number of dimensions required and increase the value of those we retain . experiments on real data confirm that this approach reveals correlations , finds novel attributes , and scales well .

['computation of transforms']


exploiting place features in link prediction on location-based social networks link prediction systems have been largely adopted to recommend new friends in online social networks using data about social interactions . with the soaring adoption of location-based social services it becomes possible to take advantage of an additional source of information : the places people visit . in this paper we study the problem of designing a link prediction system for online location-based social networks . we have gathered extensive data about one of these services , gowalla , with periodic snapshots to capture its temporal evolution . we study the link prediction space , finding that about 30 % of new links are added among `` place-friends '' , i.e. , among users who visit the same places . we show how this prediction space can be made 15 times smaller , while still 66 % of future connections can be discovered . thus , we define new prediction features based on the properties of the places visited by users which are able to discriminate potential future links among them . building on these findings , we describe a supervised learning framework which exploits these prediction features to predict new links among friends-of-friends and place-friends . our evaluation shows how the inclusion of information about places and related user activity offers high link prediction performance . these results open new directions for real-world link recommendation systems on location-based social networks .

['database applications', 'link prediction', 'location-based services', 'social networks']


mining sequential patterns from probabilistic databases we consider sequential pattern mining in situations where there is uncertainty about which source an event is associated with . we model this in the probabilistic database framework and consider the problem of enumerating all sequences whose expected support is sufficiently large . unlike frequent itemset mining in probabilistic databases ( c. aggarwal et al. . kdd ' 09 ; chui et al. , pakdd ' 07 ; chui and kao , pakdd ' 08 ) , we use dynamic programming ( dp ) to compute the probability that a source supports a sequence , and show that this suffices to compute the expected support of a sequential pattern . next , we embed this dp algorithm into candidate generate-and-test approaches , and explore the pattern lattice both in a breadth-first ( similar to gsp ) and a depth-first ( similar to spam ) manner . we propose optimizations for efficiently computing the frequent 1-sequences , for re-using previously-computed results through incremental support computation , and for elmiminating candidate sequences without computing their support via probabilistic pruning . preliminary experiments show that our optimizations are effective in improving the cpu cost .

['mining complex sequential data', 'mining uncertain data', 'novel models and algorithms', 'probabilistic databases']


large-scale matrix factorization with distributed stochastic gradient descent we provide a novel algorithm to approximately factor large matrices with millions of rows , millions of columns , and billions of nonzero elements . our approach rests on stochastic gradient descent ( sgd ) , an iterative stochastic optimization algorithm . we first develop a novel `` stratified '' sgd variant ( ssgd ) that applies to general loss-minimization problems in which the loss function can be expressed as a weighted sum of `` stratum losses . '' we establish sufficient conditions for convergence of ssgd using results from stochastic approximation theory and regenerative process theory . we then specialize ssgd to obtain a new matrix-factorization algorithm , called dsgd , that can be fully distributed and run on web-scale datasets using , e.g. , mapreduce . dsgd can handle a wide variety of matrix factorizations . we describe the practical techniques used to optimize performance in our dsgd implementation . experiments suggest that dsgd converges significantly faster and has better scalability properties than alternative algorithms .

['distributed matrix factorization', 'mapreduce', 'recommendation system', 'stochastic gradient descent']

mapreduce VERB appos algorithm

a new two-phase sampling based algorithm for discovering association rules this paper introduces fast , a novel two-phase sampling-based algorithm for discovering association rules in large databases . in phase i a large initial sample of transactions is collected and used to quickly and accurately estimate the support of each individual item in the database . in phase ii these estimated supports are used to either trim `` outlier '' transactions or select `` representative '' transactions from the initial sample , thereby forming a small final sample that more accurately reflects the statistical characteristics ( i.e. , itemset supports ) of the entire database . the expensive operation of discovering association rules is then performed on the final sample . in an empirical study , fast was able to achieve 90 -- 95 % accuracy using a final sample having a size of only 15 -- 33 % of that of a comparable random sample . this efficiency gain resulted in a speedup by roughly a factor of 10 over previous algorithms that require expensive processing of the entire database -- even efficient algorithms that exploit sampling . our new sampling technique can be used in conjunction with almost any standard association-rule algorithm , and can potentially render scalable other algorithms that mine `` count '' data .

['fast fourier transforms']


fast coordinate descent methods with variable selection for non-negative matrix factorization nonnegative matrix factorization ( nmf ) is an effective dimension reduction method for non-negative dyadic data , and has proven to be useful in many areas , such as text mining , bioinformatics and image processing . nmf is usually formulated as a constrained non-convex optimization problem , and many algorithms have been developed for solving it . recently , a coordinate descent method , called fasthals , has been proposed to solve least squares nmf and is regarded as one of the state-of-the-art techniques for the problem . in this paper , we first show that fasthals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus , performs unneeded descent steps on unimportant variables . we then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method . our new method is considerably faster in practice and we show that it has theoretical convergence guarantees . moreover when the solution is sparse , as is often the case in real applications , our new method benefits by selecting important variables to update more often , thus resulting in higher speed . as an example , on a text dataset rcv1 , our method is 7 times faster than fasthals , and more than 15 times faster when the sparsity is increased by adding an l1 penalty . we also develop new coordinate descent methods when error in nmf is measured by kl-divergence by applying the newton method to solve the one-variable sub-problems . experiments indicate that our algorithm for minimizing the kl-divergence is faster than the lee & seung multiplicative rule by a factor of 10 on the cbcl image dataset .

['convergence', 'coordinate descent method', 'learning', 'non-negative matrix factorization']

convergence NOUN compound guarantees

mime : a framework for interactive visual pattern mining we present a framework for interactive visual pattern mining . our system enables the user to browse through the data and patterns easily and intuitively , using a toolbox consisting of interestingness measures , mining algorithms and post-processing algorithms to assist in identifying interesting patterns . by mining interactively , we enable the user to combine their subjective interestingness measure and background knowledge with a wide variety of objective measures to easily and quickly mine the most important and interesting patterns . basically , we enable the user to become an essential part of the mining algorithm . our demo currently applies to mining interesting itemsets and association rules , and its extension to episodes and decision trees is ongoing .

['interactive visual mining', 'mime', 'pattern exploration']

mime NOUN ROOT mime

balanced allocation with succinct representation motivated by applications in guaranteed delivery in computational advertising , we consider the general problem of balanced allocation in a bipartite supply-demand setting . our formulation captures the notion of deviation from being balanced by a convex penalty function . while this formulation admits a convex programming solution , we strive for more robust and scalable algorithms . for the case of l1 penalty functions we obtain a simple combinatorial algorithm based on min-cost flow in graphs and show how to precompute a linear amount of information such that the allocation along any edge can be approximated in constant time . we then extend our combinatorial solution to any convex function by solving a convex cost flow . these scalable methods may have applications in other contexts stipulating balanced allocation . we study the performance of our algorithms on large real-world graphs and show that they are efficient , scalable , and robust in practice .

['balanced allocation', 'convex flow', 'maximum flow']


latent aspect rating analysis without aspect keyword supervision mining detailed opinions buried in the vast amount of review text data is an important , yet quite challenging task with widespread applications in multiple domains . latent aspect rating analysis ( lara ) refers to the task of inferring both opinion ratings on topical aspects ( e.g. , location , service of a hotel ) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings . a major limitation of previous work on lara is the assumption of pre-specified aspects by keywords . however , the aspect information is not always available , and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews . in this paper , we propose a unified generative model for lara , which does not need pre-specified aspect keywords and simultaneously mines 1 ) latent topical aspects , 2 ) ratings on each identified aspect , and 3 ) weights placed on different aspects by a reviewer . experiment results on two different review data sets demonstrate that the proposed model can effectively perform the latent aspect rating analysis task without the supervision of aspect keywords . because of its generality , the proposed model can be applied to explore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interesting application tasks , such as aspect-based opinion summarization , personalized entity ranking and recommendation , and reviewer behavior analysis .

['aspect identification', 'information search and retrieval', 'latent rating analysis', 'review mining']


a gpu-tailored approach for training kernelized svms we present a method for efficiently training binary and multiclass kernelized svms on a graphics processing unit ( gpu ) . our methods apply to a broad range of kernels , including the popular gaus - sian kernel , on datasets as large as the amount of available memory on the graphics card . our approach is distinguished from earlier work in that it cleanly and efficiently handles sparse datasets through the use of a novel clustering technique . our optimization algorithm is also specifically designed to take advantage of the graphics hardware . this leads to different algorithmic choices then those preferred in serial implementations . our easy-to-use library is orders of magnitude faster then existing cpu libraries , and several times faster than prior gpu approaches .

['concurrent programming', 'gpgpu']


personal privacy vs population privacy : learning to attack anonymization over the last decade great strides have been made in developing techniques to compute functions privately . in particular , differential privacy gives strong promises about conclusions that can be drawn about an individual . in contrast , various syntactic methods for providing privacy ( criteria such as k-anonymity and l-diversity ) have been criticized for still allowing private information of an individual to be inferred . in this paper , we consider the ability of an attacker to use data meeting privacy definitions to build an accurate classifier . we demonstrate that even under differential privacy , such classifiers can be used to infer `` private '' attributes accurately in realistic data . we compare this to similar approaches for inference-based attacks on other forms of anonymized data . we show how the efficacy of all these attacks can be measured on the same scale , based on the probability of successfully inferring a private attribute . we observe that the accuracy of inference of private attributes for differentially private data and $ l $ - diverse data can be quite similar .

['anonymization', 'differential privacy', 'miscellaneous']

anonymization NOUN dobj attack

model order selection for boolean matrix factorization matrix factorizations -- where a given data matrix is approximated by a product of two or more factor matrices -- are powerful data mining tools . among other tasks , matrix factorizations are often used to separate global structure from noise . this , however , requires solving the ` model order selection problem ' of determining where fine-grained structure stops , and noise starts , i.e. , what is the proper size of the factor matrices . boolean matrix factorization ( bmf ) -- where data , factors , and matrix product are boolean -- has received increased attention from the data mining community in recent years . the technique has desirable properties , such as high interpretability and natural sparsity . but so far no method for selecting the correct model order for bmf has been available . in this paper we propose to use the minimum description length ( mdl ) principle for this task . besides solving the problem , this well-founded approach has numerous benefits , e.g. , it is automatic , does not require a likelihood function , is fast , and , as experiments show , is highly accurate . we formulate the description length function for bmf in general -- making it applicable for any bmf algorithm . we extend an existing algorithm for bmf to use mdl to identify the best boolean matrix factorization , analyze the complexity of the problem , and perform an extensive experimental evaluation to study its behavior .

['boolean matrix factorizations', 'matrix decompositions', 'matrix factorizations', 'minimum description length principle', 'model order selection', 'model selection']


query , analysis , and visualization of hierarchically structured data using polaris in the last several years , large olap databases have become common in a variety of applications such as corporate data warehouses and scientific computing . to support interactive analysis , many of these databases are augmented with hierarchical structures that provide meaningful levels of abstraction that can be leveraged by both the computer and analyst . this hierarchical structure generates many challenges and opportunities in the design of systems for the query , analysis , and visualization of these databases . in this paper , we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouses with rich hierarchical structure , such as might be stored in data cubes . we base this tool on polaris , a system for rapidly constructing table-based graphical displays of multidimensional databases . polaris builds visualizations using an algebraic formalism derived from the interface and interpreted as a set of queries to a database . we extend the user interface , algebraic formalism , and generation of data queries in polaris to expose and take advantage of hierarchical structure . in the resulting system , analysts can navigate through the hierarchical projections of a database , rapidly and incrementally generating visualizations for each projection .

['graphical user interfaces']


towards parameter-free data mining most data mining algorithms require the setting of many input parameters . two main dangers of working with parameter-laden algorithms are the following . first , incorrect settings may cause an algorithm to fail in finding the true patterns . second , a perhaps more insidious problem is that the algorithm may report spurious patterns that do not really exist , or greatly overestimate the significance of the reported patterns . this is especially likely when the user fails to understand the role of parameters in the data mining process . data mining algorithms should have as few parameters as possible , ideally none . a parameter-free algorithm would limit our ability to impose our prejudices , expectations , and presumptions on the problem at hand , and would let the data itself speak to us . in this work , we show that recent results in bioinformatics and computational theory hold great promise for a parameter-free data-mining paradigm . the results are motivated by observations in kolmogorov complexity theory . however , as a practical matter , they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen or so lines of code . we will show that this approach is competitive or superior to the state-of-the-art approaches in anomaly\/interestingness detection , classification , and clustering with empirical tests on time series\/dna\/text \/ video datasets .

['anomaly detection', 'clustering', 'parameter-free data mining']

clustering VERB conj classification

detecting adversarial advertisements in the wild in a large online advertising system , adversaries may attempt to profit from the creation of low quality or harmful advertisements . in this paper , we present a large scale data mining effort that detects and blocks such adversarial advertisements for the benefit and safety of our users . because both false positives and false negatives have high cost , our deployed system uses a tiered strategy combining automated and semi-automated methods to ensure reliable classification . we also employ strategies to address the challenges of learning from highly skewed data at scale , allocating the effort of human experts , leveraging domain expert knowledge , and independently assessing the effectiveness of our system .

['adversarial learning', 'applications', 'data mining', 'online advertisement']


real-time bidding algorithms for performance-based display ad allocation we describe a real-time bidding algorithm for performance-based display ad allocation . a central issue in performance display advertising is matching campaigns to ad impressions , which can be formulated as a constrained optimization problem that maximizes revenue subject to constraints such as budget limits and inventory availability . the current practice is to solve the optimization problem offline at a tractable level of impression granularity ( e.g. , the page level ) , and to serve ads online based on the precomputed static delivery scheme . although this offline approach takes a global view to achieve optimality , it fails to scale to ad allocation at the individual impression level . therefore , we propose a real-time bidding algorithm that enables fine-grained impression valuation ( e.g. , targeting users with real-time conversion data ) , and adjusts value-based bids according to real-time constraint snapshots ( e.g. , budget consumption levels ) . theoretically , we show that under a linear programming ( lp ) primal-dual formulation , the simple real-time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem as input . in other words , the online algorithm guarantees the offline optimality given the same level of knowledge an offline optimization would have . empirically , we develop and experiment with two real-time bid adjustment approaches to adapting to the non-stationary nature of the marketplace : one adjusts bids against real-time constraint satisfaction levels using control-theoretic methods , and the other adjusts bids also based on the statistically modeled historical bidding landscape . finally , we show experimental results with real-world ad delivery data that support our theoretical conclusions .

['ad exchange', 'combinatorial optimization', 'linear programming', 'performance display', 'real-time bidding']


towards bounding sequential patterns given a sequence database , can we have a non-trivial upper bound on the number of sequential patterns ? the problem of bounding sequential patterns is very challenging in theory due to the combinatorial complexity of sequences , even given some inspiring results on bounding itemsets in frequent itemset mining . moreover , the problem is highly meaningful in practice , since the upper bound can be used in many applications such as space allocation in building sequence data warehouses . in this paper , we tackle the problem of bounding sequential patterns by presenting , for the first time in the field of sequential pattern mining , strong combinatorial results on computing the number of possible sequential patterns that can be generated at a given length k. we introduce , as a case study , two novel techniques to estimate the number of candidate sequences . an extensive empirical study on both real data and synthetic data verifies the effectiveness of our methods .

['combinatorics', 'sequential pattern mining']


incremental context mining for adaptive document classification automatic document classification ( dc ) is essential for the management of information and knowledge . this paper explores two practical issues in dc : ( 1 ) each document has its context of discussion , and ( 2 ) both the content and vocabulary of the document database is intrinsically evolving . the issues call for adaptive document classification ( adc ) that adapts a dc system to the evolving contextual requirement of each document category , so that input documents may be classified based on their contexts of discussion . we present an incremental context mining technique to tackle the challenges of adc . theoretical analyses and empirical results show that , given a text hierarchy , the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category . based on the contextual requirements mined by the system , higher-precision dc may be achieved with better efficiency .

['adaptive document classification', 'context text mining', 'incremental mining']


learning to match and cluster large high-dimensional data sets for data integration part of the process of data integration is determining which sets of identifiers refer to the same real-world entities . in integrating databases found on the web or obtained by using information extraction methods , it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases . in this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive , in the sense that they can be trained to obtain better performance in a particular domain . an experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems , and is nearly always competitive with the best baseline system .

['large datasets', 'learning', 'text mining']

learning VERB csubj determining

mining concept-drifting data streams using ensemble classifiers recently , mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection , target marketing , network intrusion detection , etc. . conventional knowledge discovery tools are facing two challenges , the overwhelming volume of the streaming data , and the concept drifts . in this paper , we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers . we train an ensemble of classification models , such as c4 .5 , ripper , naive beyesian , etc. , from sequential chunks of the data stream . the classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment . thus , the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification . our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy , and the ensemble framework is effective for a variety of classification models .

['classifier', 'classifier ensemble', 'concept drift', 'data streams']

classifier NOUN compound approaches

adaptive duplicate detection using learnable string similarity measures the problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes . most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates . in this paper , we present a framework for improving duplicate detection using trainable measures of textual similarity . we propose to employ learnable text distance functions for each database field , and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field 's domain . we present two learnable text similarity measures suitable for this task : an extended variant of learnable string edit distance , and a novel vector-space based measure that employs a support vector machine ( svm ) for training . experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques .

['data cleaning', 'database applications', 'distance metric learning', 'learning', 'record linkage', 'string edit distance', 'svm applications', 'trained similarity measures']


ensemble-index : a new approach to indexing large databases the problem of similarity search ( query-by-content ) has attracted much research interest . it is a difficult problem because of the inherently high dimensionality of the data . the most promising solutions involve performing dimensionality reduction on the data , then indexing the reduced data with a multidimensional index structure . many dimensionality reduction techniques have been proposed , including singular value decomposition ( svd ) , the discrete fourier transform ( dft ) , the discrete wavelet transform ( dwt ) and piecewise polynomial approximation . in this work , we introduce a novel framework for using ensembles of two or more representations for more efficient indexing . the basic idea is that instead of committing to a single representation for an entire dataset , different representations are chosen for indexing different parts of the database . the representations are chosen based upon a local view of the database . for example , sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets , but highly spectral sections of the data are indexed using the fourier transform . at query time , it is necessary to search several small heterogeneous indices , rather than one large homogeneous index . as we will theoretically and empirically demonstrate this results in much faster query response times .

['content analysis and indexing', 'data mining', 'dimensionality reduction', 'indexing and retrieval', 'similarity search', 'time series']


style mining of electronic messages for multiple authorship discrimination : first results this paper considers the use of computational stylistics for performing authorship attribution of electronic messages , addressing categorization problems with as many as 20 different classes ( authors ) . effective stylistic characterization of text is potentially useful for a variety of tasks , as language style contains cues regarding the authorship , purpose , and mood of the text , all of which would be useful adjuncts to information retrieval or knowledge-management tasks . we focus here on the problem of determining the author of an anonymous message , based only on the message text . several multiclass variants of the winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating different authors . we present results comparing the classification accuracy of the different approaches . the results show that stylistic models can be accurately learned to determine an author 's identity .

['authorship attribution', 'computational stylistics', 'electronic communication', 'information search and retrieval', 'learning', 'text categorization', 'text mining']


understanding captions in biomedical publications from the standpoint of the automated extraction of scientific knowledge , an important but little-studied part of scientific publications are the figures and accompanying captions . captions are dense in information , but also contain many extra-grammatical constructs , making them awkward to process with standard information extraction methods . we propose a scheme for `` understanding '' captions in biomedical publications by extracting and classifying `` image pointers '' ( references to the accompanying image ) . we evaluate a number of automated methods for this task , including hand-coded methods , methods based on existing learning techniques , and methods based on novel learning techniques . the best of these methods leads to a usefully accurate tool for caption-understanding , with both recall and precision in excess of 94 % on the most important single class in a combined extraction\/classification task .

['bioinformatics', 'boosting', 'information extraction', 'information search and retrieval', 'learning']

learning NOUN compound techniques
learning NOUN compound techniques

efficient closed pattern mining in the presence of tough block constraints various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemset-based constraints that better capture the underlying application requirements and characteristics . in this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern 's items and its associated set of transactions . block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets . however , developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier , these block constraints are tough as they are neither anti-monotone , monotone , nor convertible . to overcome this problem , we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called cbminer to find the closed itemsets that satisfy the block constraints .

['block constraint', 'closed pattern', 'tough constraint']


accurate decision trees for mining high-speed data streams in this paper we study the problem of constructing accurate decision tree models from data streams . data streams are incremental tasks that require incremental , online , and any-time learning algorithms . one of the most successful algorithms for mining data streams is vfdt . in this paper we extend the vfdt system in two directions : the ability to deal with continuous data and the use of more powerful classification techniques at tree leaves . the proposed system , vfdtc , can incorporate and classify new information online , with a single scan of the data , in time constant per example . the most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets . this is relevant due to the any-time property . we study the behavior of vfdtc in different problems and demonstrate its utility in large and medium data sets . under a bias-variance analysis we observe that vfdtc in comparison to c4 .5 is able to reduce the variance component .

['decision support', 'decision trees', 'disk-based algorithms', 'hoeffding bounds', 'incremental learning', 'subsampling']


improving spatial locality of programs via data mining in most computer systems , page fault rate is currently minimized by generic page replacement algorithms which try to model the temporal locality inherent in programs . in this paper , we propose two algorithms , one greedy and the other stochastic , designed for program specific code restructuring as a means of increasing spatial locality within a program . both algorithms effectively decrease average working set size and hence the page fault rate . our methods are more effective than traditional approaches due to use of domain information . we illustrate the efficacy of our algorithms on actual data mining algorithms .

['code restructuring', 'database applications', 'page clustering', 'program locality']


bayesian networks for lossless dataset compression

['uncertainty, fuzzy, and probabilistic reasoning']


mining viewpoint patterns in image databases the increasing number of image repositories has made image mining an important task because of its potential in discovering useful image patterns from a large set of images . in this paper , we introduce the notion of viewpoint patterns for image databases . viewpoint patterns refer to patterns that capture the invariant relationships of one object from the point of view of another object . these patterns are unique and significant in images because the absolute positional information of objects for most images is not important , but rather , it is the relative distance and orientation of the objects from each other that is meaningful . we design a scalable and efficient algorithm to discover such viewpoint patterns . experiments results on various image sets demonstrate that viewpoint patterns are meaningful and interesting to human users .

['database applications', 'image database', 'image mining', 'spatial relationship']


web usage mining based on probabilistic latent semantic analysis the primary goal of web usage mining is the discovery of patterns in the navigational behavior of web users . standard approaches , such as clustering of user sessions and discovering association rules or frequent navigational paths , do not generally provide the ability to automatically characterize or quantify the unobservable factors that lead to common navigational patterns . it is , therefore , necessary to develop techniques that can automatically discover hidden semantic relationships among users as well as between users and web objects . probabilistic latent semantic analysis ( plsa ) is particularly useful in this context , since it can uncover latent semantic associations among users and pages based on the co-occurrence patterns of these pages in user sessions . in this paper , we develop a unified framework for the discovery and analysis of web navigational patterns based on plsa . we show the flexibility of this framework in characterizing various relationships among users and web objects . since these relationships are measured in terms of probabilities , we are able to use probabilistic inference to perform a variety of analysis tasks such as user segmentation , page classification , as well as predictive tasks such as collaborative recommendations . we demonstrate the effectiveness of our approach through experiments performed on real-world data sets .

['plsa', 'user profiling', 'web usage mining']

plsa PROPN appos analysis
plsa NOUN pobj on

relational markov models and their application to adaptive web navigation relational markov models ( rmms ) are a generalization of markov models where states can be of different types , with each type described by a different set of variables . the domain of each variable can be hierarchically structured , and shrinkage is carried out over the cross product of these hierarchies . rmms make effective learning possible in domains with very large and heterogeneous state spaces , given only sparse data . we apply them to modeling the behavior of web site users , improving prediction in our proteus architecture for personalizing web sites . we present experiments on an e-commerce and an academic web site showing that rmms are substantially more accurate than alternative methods , and make good predictions even when applied to previously-unvisited parts of the site .

['markov models', 'personalization', 'relational probabilistic models', 'shrinkage', 'web mining']

shrinkage NOUN nsubjpass carried

visualizing changes in the structure of data for exploratory feature selection using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays . several visualization techniques have been developed to study the cluster structure of data , i.e. , the existence of distinctive groups in the data and how these clusters are related to each other . however , only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed . understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase . in this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure . we demonstrate the application of our approach in two music related data mining projects .

['high-dimensional data', 'interactive data mining']


discovering word senses from text inventories of manually compiled dictionaries usually serve as a source for word senses . however , they often include many rare senses while missing corpus\/domain-specific senses . we present a clustering algorithm called cbc ( clustering by committee ) that automatically discovers word senses from text . it initially discovers a set of tight clusters called committees that are well scattered in the similarity space . the centroid of the members of a committee is used as the feature vector of the cluster . we proceed by assigning words to their most similar clusters . after assigning an element to a cluster , we remove their overlapping features from the element . this allows cbc to discover the less frequent senses of a word and to avoid discovering duplicate senses . each cluster that a word belongs to represents one of its senses . we also present an evaluation methodology for automatically measuring the precision and recall of discovered senses .

['evaluation', 'machine learning', 'word sense discovery']

evaluation NOUN compound methodology

gaining insights into support vector machine pattern classifiers using projection-based tour methods this paper discusses visual methods that can be used to understand and interpret the results of classification using support vector machines ( svm ) on data with continuous real-valued variables . svm induction algorithms build pattern classifiers by identifying a maximal margin separating hyperplane from training examples in high dimensional pattern spaces or spaces induced by suitable nonlinear kernel transformations over pattern spaces . svm have been demonstrated to be quite effective in a number of practical pattern classification tasks . since the separating hyperplane is defined in terms of more than two variables it is necessary to use visual techniques that can navigate the viewer through high-dimensional spaces . we demonstrate the use of projection-based tour methods to gain useful insights into svm classifiers with linear kernels on 8-dimensional data .

['classification', 'dynamic graphics', 'machine leaning', 'multivariate data', 'support vector machines', 'tours', 'visualization']

classification NOUN pobj of
classification NOUN compound tasks

learning to predict train wheel failures this paper describes a successful but challenging application of data mining in the railway industry . the objective is to optimize maintenance and operation of trains through prognostics of wheel failures . in addition to reducing maintenance costs , the proposed technology will help improve railway safety and augment throughput . building on established techniques from data mining and machine learning , we present a methodology to learn models to predict train wheel failures from readily available operational and maintenance data . this methodology addresses various data mining tasks such as automatic labeling , feature extraction , model building , model fusion , and evaluation . after a detailed description of the methodology , we report results from large-scale experiments . these results clearly show the great potential of this innovative application of data mining in the railway industry .

['decision support', 'machine learning', 'methodology', 'model building', 'model evaluation', 'model fusion', 'wheel failure prediction']

methodology NOUN dobj present
methodology NOUN nsubj addresses
methodology NOUN pobj of

robust space transformations for distance-based operations for many kdd operations , such as nearest neighbor search , distance-based clustering , and outlier detection , there is an underlying & kgr ;-d data space in which each tuple\/object is represented as a point in the space . in the presence of differing scales , variability , correlation , and\/or outliers , we may get unintuitive results if an inappropriate space is used . the fundamental question that this paper addresses is : `` what then is an appropriate space ? '' we propose using a robust space transformation called the donoho-stahel estimator . in the first half of the paper , we show the key properties of the estimator . of particular importance to kdd applications involving databases is the stability property , which says that in spite of frequent updates , the estimator does not : ( a ) change much , ( b ) lose its usefulness , or ( c ) require re-computation . in the second half , we focus on the computation of the estimator for high-dimensional databases . we develop randomized algorithms and evaluate how well they perform empirically . the novel algorithm we develop called the hybrid-random algorithm is , in most cases , at least an order of magnitude faster than the fixed-angle and subsampling algorithms .

['computation of transforms', 'data mining', 'distance-based operations', 'outliers', 'robust estimators', 'robust statistics', 'space transformations']

outliers NOUN conj correlation

pebl : positive example based learning for web page classification using svm web page classification is one of the essential techniques for web mining . specifically , classifying web pages of a user-interesting class is the first step of mining interesting information from the web . however , constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples . for instance , in order to construct a `` homepage '' classifier , one needs to collect a sample of homepages ( positive examples ) and a sample of non-homepages ( negative examples ) . in particular , collecting negative training examples requires arduous work and special caution to avoid biasing them . we introduce in this paper the positive example based learning ( pebl ) framework for web page classification which eliminates the need for manually collecting negative training examples in pre-processing . we present an algorithm called mapping-convergence ( m-c ) that achieves classification accuracy ( with positive and unlabeled data ) as high as that of traditional svm ( with positive and negative data ) . our experiments show that when the m-c algorithm uses the same amount of positive examples as that of traditional svm , the m-c algorithm performs as well as traditional svm .

['labeled data', 'mapping-convergence algorithm', 'svm', 'unlabeled data']

svm NOUN compound classification
svm NOUN pobj of
svm NOUN pobj of
svm NOUN conj performs

closet + : searching for the best strategies for mining frequent closed itemsets mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis . extensive studies have proposed various strategies for efficient frequent closed itemset mining , such as depth-first search vs. breadthfirst search , vertical formats vs. horizontal formats , tree-structure vs. other data structures , top-down vs. bottom-up traversal , pseudo projection vs. physical projection of conditional database , etc. . it is the right time to ask `` what are the pros and cons of the strategies ? '' and `` what and how can we pick and integrate the best strategies to achieve higher performance in general cases ? `` in this study , we answer the above questions by a systematic study of the search strategies and develop a winning algorithm closet + . closet + integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here . a thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of closet + over existing mining algorithms , including closet , charm and op , in terms of runtime , memory usage and scalability .

['association rules', 'frequent closed itemsets', 'mining methods and algorithms']


bayesian analysis of massive datasets via particle filters markov chain monte carlo ( mcmc ) techniques revolutionized statistical practice in the 1990s by providing an essential toolkit for making the rigor and flexibility of bayesian analysis computationally practical . at the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need to produce statistically sound methods that scale to these large problems . except for the most trivial examples , current mcmc methods require a complete scan of the dataset for each iteration eliminating their candidacy as feasible data mining techniques . in this article we present a method for making bayesian analysis of massive datasets computationally feasible . the algorithm simulates from a posterior distribution that conditions on a smaller , more manageable portion of the dataset . the remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling . computation of the importance weights requires a single scan of the remaining observations . while importance sampling increases efficiency in data access , it comes at the expense of estimation efficiency . a simple modification , based on the `` rejuvenation '' step used in particle filters for dynamic systems models , sidesteps the loss of efficiency with only a slight increase in the number of data accesses . to show proof-of-concept , we demonstrate the method on a mixture of transition models that has been used to model web traffic and robotics . for this example we show that estimation efficiency is not affected while offering a 95 % reduction in data accesses .

['probabilistic algorithms']


scaling up dynamic time warping for datamining applications

['dynamic time warping', 'time series']


indexing multi-dimensional time-series with support for multiple distance measures although most time-series data mining research has concentrated on providing solutions for a single distance function , in this work we motivate the need for a single index structure that can support multiple distance measures . our specific area of interest is the efficient retrieval and analysis of trajectory similarities . trajectory datasets are very common in environmental applications , mobility experiments , video surveillance and are especially important for the discovery of certain biological patterns . our primary similarity measure is based on the longest common subsequence ( lcss ) model , that offers enhanced robustness , particularly for noisy data , which are encountered very often in real world applications . however , our index is able to accommodate other distance measures as well , including the ubiquitous euclidean distance , and the increasingly popular dynamic time warping ( dtw ) . while other researchers have advocated one or other of these similarity measures , a major contribution of our work is the ability to support all these measures without the need to restructure the index . our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision\/recall . the experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the lcss and the dtw .

['database applications', 'dynamic time warping', 'longest common subsequence', 'trajectories']


inverted matrix : efficient discovery of frequent items in large datasets in the context of interactive mining existing association rule mining algorithms suffer from many problems when mining massive transactional datasets . one major problem is the high memory dependency : either the gigantic data structure built is assumed to fit in main memory , or the recursive mining process is too voracious in memory resources . another major impediment is the repetitive and interactive nature of any knowledge discovery process . to tune parameters , many runs of the same algorithms are necessary leading to the building of these huge data structures time and again . this paper proposes a new disk-based association rule mining algorithm called inverted matrix , which achieves its efficiency by applying three new ideas . first , transactional data is converted into a new database layout called inverted matrix that prevents multiple scanning of the database during the mining phase , in which finding frequent patterns could be achieved in less than a full scan with random access . second , for each frequent item , a relatively small independent tree is built summarizing co-occurrences . finally , a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed . experimental studies reveal that our inverted matrix approach outperform fp-tree especially in mining very large transactional databases with a very large number of unique items . our random access disk-based approach is particularly advantageous in a repetitive and interactive setting .

['association rules', 'cofi-tree', 'database applications', 'frequent patterns mining', 'inverted matrix']


efficient decision tree construction on streaming data decision tree construction is a well studied problem in data mining . recently , there has been much interest in mining streaming data . domingos and hulten have presented a one-pass algorithm for decision tree construction . their work uses hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed . in this paper , we revisit this problem . we make the following two contributions : 1 ) we present a numerical interval pruning ( nip ) approach for efficiently processing numerical attributes . our results show an average of 39 % reduction in execution times . 2 ) we exploit the properties of the gain function entropy ( and gini ) to reduce the sample size required for obtaining a given bound on the accuracy . our experimental results show a 37 % reduction in the number of data instances required .

['database applications', 'decision tree', 'learning', 'sampling', 'streaming data']


random projection in dimensionality reduction : applications to image and text data random projections have recently emerged as a powerful method for dimensionality reduction . theoretical results indicate that the method preserves distances quite nicely ; however , empirical results are sparse . we present experimental results on using random projection as a dimensionality reduction tool in a number of cases , where the high dimensionality of the data would otherwise lead to burden-some computations . our application areas are the processing of both noisy and noiseless images , and information retrieval in text documents . we show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis : the similarity of data vectors is preserved well under random projection . however , using random projections is computationally significantly less expensive than using , e.g. , principal component analysis . we also show experimentally that using a sparse random matrix gives additional computational savings in random projection .

['dimensionality reduction', 'high-dimensional data', 'image data', 'random projection', 'text document data']


frequent-subsequence-based prediction of outer membrane proteins a number of medically important disease-causing bacteria ( collectively called gram-negative bacteria ) are noted for the extra `` outer '' membrane that surrounds their cell . proteins resident in this membrane ( outer membrane proteins , or omps ) are of primary research interest for antibiotic and vaccine drug design as they are on the surface of the bacteria and so are the most accessible targets to develop new drugs against . with the development of genome sequencing technology and bioinformatics , biologists can now deduce all the proteins that are likely produced in a given bacteria and have attempted to classify where proteins are located in a bacterial cell . however such protein localization programs are currently least accurate when predicting omps , and so there is a current need for the development of a better omp classifier . data mining research suggests that the use of frequent patterns has good performance in aiding the development of accurate and efficient classification algorithms . in this paper , we present two methods to identify omps based on frequent subsequences and test them on all gram-negative bacterial proteins whose localizations have been determined by biological experiments . one classifier follows an association rule approach , while the other is based on support vector machines ( svms ) . we compare the proposed methods with the state-of-the-art methods in the biological domain . the results demonstrate that our methods are better both in terms of accurately identifying omps and providing biological insights that increase our understanding of the structures and functions of these important proteins .

['association rule', 'classification', 'database applications', 'outer membrane protein', 'subcellular localization', 'support vector machine']

classification NOUN compound algorithms

the ioc algorithm : efficient many-class non-parametric classification for high-dimensional data this paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets . k nearest neighbor remains a popular classification technique , especially in areas such as computer vision , drug activity prediction and astrophysics . furthermore , many more modern classifiers , such as kernel-based bayes classifiers or the prediction phase of svms , require computational regimes similar to k-nn . we believe that tractable k-nn algorithms therefore continue to be important . this paper relies on the insight that even with many classes , the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those k nearest neighbors . this insight was previously used in ( liu et al. , 2003 ) in two algorithms called kns2 and kns3 which dealt with fast classification in the case of two classes . in this paper we show how a different approach , ioc ( standing for the international olympic committee ) can apply to the case of n classes where n ) 2 . ioc assumes a slightly different processing of the datapoints in the neighborhood of the query . this allows it to search a set of metric trees , one for each class . during the searches it is possible to quickly prune away classes that can not possibly be the majority . we give experimental results on datasets of up to 5.8 x 105 records and 1.5 x 103 attributes , frequently showing an order of magnitude acceleration compared with each of ( i ) conventional linear scan , ( ii ) a well-known independent sr-tree implementation of conventional k-nn and ( iii ) a highly optimized conventional k-nn metric tree search .

['classification', 'high dimension', 'k nearest neighbor', 'learning', 'metric tree']

classification NOUN pobj of
classification NOUN compound technique
classification NOUN pobj with

detecting graph-based spatial outliers : algorithms and applications ( a summary of results ) identification of outliers can lead to the discovery of unexpected , interesting , and useful knowledge . existing methods are designed for detecting spatial outliers in multidimensional geometric data sets , where a distance metric is available . in this paper , we focus on detecting spatial outliers in graph structured data sets . we define statistical tests , analyze the statistical foundation underlying our approach , design several fast algorithms to detect spatial outliers , and provide a cost model for outlier detection procedures . in addition , we provide experimental results from the application of our algorithms on a minneapolis-st . paul ( twin cities ) traffic dataset to show their effectiveness and usefulness .

['outlier detection', 'spatial data mining', 'spatial graphs']


generating english summaries of time series data using the gricean maxims we are developing technology for generating english textual summaries of time-series data , in three domains : weather forecasts , gas-turbine sensor readings , and hospital intensive care data . our weather-forecast generator is currently operational and being used daily by a meteorological company . we generate summaries in three steps : ( a ) selecting the most important trends and patterns to communicate ; ( b ) mapping these patterns onto words and phrases ; and ( c ) generating actual texts based on these words and phrases . in this paper we focus on the first step , ( a ) , selecting the information to communicate , and describe how we perform this using modified versions of standard data analysis algorithms such as segmentation . the modifications arose out of empirical work with users and domain experts , and in fact can all be regarded as applications of the gricean maxims of quality , quantity , relevance , and manner , which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text . the gricean maxims are perhaps a key element of adapting data analysis algorithms for effective communication of information to human users , and should be considered by other researchers interested in communicating data to human users .

['gricean maxims', 'natural language processing', 'natural language processing', 'summarization', 'time series data']


segmentation-based modeling for advanced targeted marketing fingerhut business intelligence ( bi ) has a long and successful history of building statistical models to predict consumer behavior . the models constructed are typically segmentation-based models in which the target audience is split into subpopulations ( i.e. , customer segments ) and individually tailored statistical models are then developed for each segment . such models are commonly employed in the direct-mail industry ; however , segmentation is often performed on an ad-hoc basis without directly considering how segmentation affects the accuracy of the resulting segment models . fingerhut bi approached ibm research with the problem of how to build segmentation-based models more effectively so as to maximize predictive accuracy . the ibm advanced targeted marketing-single eventstm ( ibm atm-setm ) solution is the result of ibm research and fingerhut bi directing their efforts jointly towards solving this problem . this paper presents an evaluation of atm-se 's modeling capabilities using data from fingerhut 's catalog mailings .

['database applications', 'decision trees', 'feature selection', 'linear regression', 'logistic regression', 'segmentation-based models', 'targeted marketing']


discovering roll-up dependencies

['logical design']


treedt : gene mapping by tree disequilibrium test we introduce and evaluate treedt , a novel gene mapping method which is based on discovering and assessing tree-like patterns in genetic marker data . gene mapping aims at discovering a statistical connection from a particular disease or trait to a narrow region in the genome . in a typical case-control setting , data consists of genetic markers typed for a set of disease-associated chromosomes and a set of control chromosomes . a computer scientist would view this data as a set of strings . treedt extracts , essentially in the form of substrings and prefix trees , information about the historical recombinations in the population . this information is used to locate fragments potentially inherited from a common diseased founder , and to map the disease gene into the most likely such fragment . the method measures for each chromosomal location the disequilibrium of the prefix tree of marker strings starting from the location , to assess the distribution of disease-associated chromosomes . we evaluate experimentally the performance of treedt on realistic , simulated data sets , and comparisons to state of the art methods ( tdt , hpm ) show that treedt is very competitive .

['gene mapping', 'permutation tests', 'prefix trees']


using association rules for product assortment decisions : a case study

['association rules', 'decision support', 'frequent itemset', 'product assortment decisions']


deformable markov model templates for time-series pattern matching

['deformable templates', 'hidden markov models', 'pattern matching', 'segmental markov models', 'time series']


towards an effective cooperation of the user and the computer for classification

['decision support']


mining web logs for prediction models in www caching and prefetching web caching and prefetching are well known strategies for improving the performance of internet systems . when combined with web log mining , these strategies can decide to cache and prefetch web documents with higher accuracy . in this paper , we present an application of web log mining to obtain web-document access patterns and use these patterns to extend the well-known gdsf caching policies and prefetching policies . using real web logs , we show that this application of data mining can achieve dramatic improvement to web-access performance .

['application to caching and prefetching on the www', 'web log mining', 'world wide web']


frequent term-based text clustering text clustering methods can be used to structure large sets of text or hypertext documents . the well-known methods of text clustering , however , do not really address the special problems of text clustering : very high dimensionality of the data , very large size of the databases and understandability of the cluster description . in this paper , we introduce a novel approach which uses frequent item ( term ) sets for text clustering . such frequent sets can be efficiently discovered using algorithms for association rule mining . to cluster based on frequent term sets , we measure the mutual overlap of frequent sets with respect to the sets of supporting documents . we present two algorithms for frequent term-based text clustering , ftc which creates flat clusterings and hftc for hierarchical clustering . an experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the - art text clustering algorithms . furthermore , our methods provide an understandable description of the discovered clusters by their frequent term sets .

['frequent item sets', 'text documents']


application of neural networks to biological data mining : a case study in protein sequence classification

['bioinformatics', 'biological data mining', 'feature extraction from protein data', 'machine learning', 'neural networks', 'sequence alignment']


adversarial classification essentially all data mining algorithms assume that the data-generating process is independent of the data miner 's activities . however , in many domains , including spam detection , intrusion detection , fraud detection , surveillance and counter-terrorism , this is far from the case : the data is actively manipulated by an adversary seeking to make the classifier produce false negatives . in these domains , the performance of a classifier can degrade rapidly after it is deployed , as the adversary learns to defeat it . currently the only solution to this is repeated , manual , ad hoc reconstruction of the classifier . in this paper we develop a formal framework and algorithms for this problem . we view classification as a game between the classifier and the adversary , and produce a classifier that is optimal given the adversary 's optimal strategy . experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way , and ( within the parameters of the problem ) automatically adapt the classifier to the adversary 's evolving manipulations .

['cost-sensitive learning', 'game theory', 'integer linear programming', 'naive bayes', 'spam detection']


visualization of navigation patterns on a web site using model-based clustering

['data visualization', 'internet', 'internet', 'model-based clustering', 'sequence clustering', 'web']

web NOUN compound site

data mining with sparse grids using simplicial basis functions recently we presented a new approach ( 18 ) to the classification problem arising in data mining . it is based on the regularization network approach but , in contrast to other methods which employ ansatz functions associated to data points , we use a grid in the usually high-dimensional feature space for the minimization process . to cope with the curse of dimensionality , we employ sparse grids ( 49 ) . thus , only o ( hn-1nd-1 ) instead of o ( hn-d ) grid points and unknowns are involved . here d denotes the dimension of the feature space and hn = 2-n gives the mesh size . we use the sparse grid combination technique ( 28 ) where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension . the sparse grid solution is then obtained by linear combination . in contrast to our former work , where d-linear functions were used , we now apply linear basis functions based on a simplicial discretization . this allows to handle more dimensions and the algorithm needs less operations per data point . we describe the sparse grid combination technique for the classification problem , give implementational details and discuss the complexity of the algorithm . it turns out that the method scales linearly with the number of given data points . finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions . it turns out that our new method achieves correctness rates which are competitive to that of the best existing methods .

['approximation', 'classification', 'combination technique', 'simplicial discretization', 'sparse grids']

classification NOUN compound problem
classification NOUN compound problem
classification NOUN compound problem

mining ic test data to optimize vlsi testing

['belief networks', 'decision support', 'decision theory', 'em algorithm', 'ic test', 'real-time control', 'unsupervised learning', 'vlsi']

vlsi NOUN compound testing

handling concept drifts in incremental learning with support vector machines

['learning']


proximal support vector machine classifiers instead of a standard support vector machine ( svm ) that classifies points by assigning them to one of two disjoint half-spaces , points are classified by assigning them to the closest of two parallel planes ( in input or feature space ) that are pushed apart as far as possible . this formulation , which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks ( 8 , 9 ) , leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations . in contrast , standard svms solve a quadratic or a linear program that require considerably longer computational time . computational results on publicly available datasets indicate that the proposed proximal svm classifier has comparable test set correctness to that of standard svm classifiers , but with considerably faster computational time that can be an order of magnitude faster . the linear proximal svm can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds . all computational results are based on 6 lines of matlab code .

['data classification', 'linear equations', 'support vector machines']


shrinkage estimator generalizations of proximal support vector machines we give a statistical interpretation of proximal support vector machines ( psvm ) proposed at kdd2001 as linear approximaters to ( nonlinear ) support vector machines ( svm ) . we prove that psvm using a linear kernel is identical to ridge regression , a biased-regression method known in the statistical community for more than thirty years . techniques from the statistical literature to estimate the tuning constant that appears in the svm and psvm framework are discussed . better shrinkage strategies that incorporate more than one tuning constant are suggested . for nonlinear kernels , the minimization problem posed in the psvm framework is equivalent to finding the posterior mode of a bayesian model defined through a gaussian process on the predictor space . apart from providing new insights , these interpretations help us attach an estimate of uncertainty to our predictions and enable us to build richer classes of models . in particular , we propose a new algorithm called psvmmix which is a combination of ridge regression and a gaussian process model . extension to the case of continuous response is straightforward and illustrated with example datasets .

['bayesian models', 'bias-variance tradeoff', 'classification', 'correlation', 'kernel', 'probability and statistics', 'regression']

kernel NOUN dobj using
regression NOUN dobj ridge
regression NOUN compound method
regression NOUN pobj of

statistical modeling of large-scale simulation data with the advent of fast computer systems , scientists are now able to generate terabytes of simulation data . unfortunately , the sheer size of these data sets has made efficient exploration of them impossible . to aid scientists in gleaning insight from their simulation data , we have developed an ad-hoc query infrastructure . our system , called aqsim ( short for ad-hoc queries for simulation ) reduces the data storage requirements and query access times in two stages . first , it creates and stores mathematical and statistical models of the data at multiple resolutions . second , it evaluates queries on the models of the data instead of on the entire data set . in this paper , we present two simple but effective statistical modeling techniques for simulation data . our first modeling technique computes the `` true '' ( unbiased ) mean of systematic partitions of the data . it makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model . our second statistical modeling technique uses the andersen-darling goodness-of-fit method on systematic partitions of the data . this method evaluates a model by how well it passes the normality test on the data . both of our statistical models effectively answer range queries . at each resolution of the data , we compute the precision of our answer to the user 's query by scaling the one-sided chebyshev inequalities with the original mesh 's topology . we combine precisions at different resolutions by calculating their weighted average . our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets .

['approximate ad-hoc queries', 'large-scale scientific data sets', 'statistical modeling']


discovering evolutionary theme patterns from text : an exploration of temporal text mining temporal text mining ( ttm ) is concerned with discovering temporal patterns in text information collected over time . since most text information bears some time stamps , ttm has many applications in multiple domains , such as summarizing events in news articles and revealing research trends in scientific literature . in this paper , we study a particular ttm task -- discovering and summarizing the evolutionary patterns of themes in a text stream . we define this new text mining problem and present general probabilistic methods for solving this problem through ( 1 ) discovering latent themes from text ; ( 2 ) constructing an evolution graph of themes ; and ( 3 ) analyzing life cycles of themes . evaluation of the proposed methods on two different domains ( i.e. , news articles and literature ) shows that the proposed methods can discover interesting evolutionary theme patterns effectively .

['evolutionary theme patterns', 'temporal text mining', 'theme threads']


a generalized framework for mining spatio-temporal patterns in scientific data in this paper , we present a general framework to discover spatial associations and spatio-temporal episodes for scientific datasets . in contrast to previous work in this area , features are modeled as geometric objects rather than points . we define multiple distance metrics that take into account objects ' extent and thus are more robust in capturing the influence of an object on other objects in spatial neighborhood . we have developed algorithms to discover four different types of spatial object interaction ( association ) patterns . we also extend our approach to accommodate temporal information and propose a simple algorithm to derive spatio-temporal episodes . we show that such episodes can be used to reason about critical events . we evaluate our framework on real datasets to demonstrate its efficacy . the datasets originate from two different areas : computational molecular dynamics and computational fluid flow . we present results highlighting the importance of the identified patterns and episodes by using knowledge from the underlying domains . we also show that the proposed algorithms scale linearly with respect to the dataset size .

['scientific data', 'spatial object association', 'spatio-temporal association/episode']


multi-level organization and summarization of the discovered rules

['decision support']


transforming data to satisfy privacy constraints data on individuals and entities are being collected widely . these data can contain information that explicitly identifies the individual ( e.g. , social security number ) . data can also contain other kinds of personal information ( e.g. , date of birth , zip code , gender ) that are potentially identifying when linked with other available data sets . data are often shared for business or legal reasons . this paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process . we explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data . we extend earlier works in this area along various dimensions . first , satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated . this allows us to optimize the process of preserving privacy for the specified usage . in particular , we investigate the privacy transformation in the context of data mining applications like building classification and regression models . second , our work improves on previous approaches by allowing more flexible generalizations for the data . lastly , this is combined with a more thorough exploration of the solution space using the genetic algorithm framework . these extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints .

['data transformation', 'generalization', 'predictive modeling', 'suppression']


mining data records in web pages a large amount of information on the web is contained in regularly structured objects , which we call data records . such data records are important because they often present the essential information of their host pages , e.g. , lists of products or services . it is useful to mine such data records in order to extract information from them to provide value-added services . existing automatic techniques are not satisfactory because of their poor accuracies . in this paper , we propose a more effective technique to perform the task . the technique is based on two observations about data records on the web and a string matching algorithm . the proposed technique is able to mine both contiguous and non-contiguous data records . our experimental results show that the proposed technique outperforms existing techniques substantially .

['database applications', 'web data records', 'web information integration', 'web mining']


probabilistic modeling of transaction data with applications to profiling , visualization , and prediction transaction data is ubiquitous in data mining applications . examples include market basket data in retail commerce , telephone call records in telecommunications , and web logs of individual page-requests at web sites . profiling consists of using historical transaction data on individuals to construct a model of each individual 's behavior . simple profiling techniques such as histograms do not generalize well from sparse transaction data . in this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data . in effect , the mixture model represents each individual 's behavior as a linear combination of `` basis transactions . '' we evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques , as well as being relatively scalable , interpretable , and flexible . in addition we point to applications in outlier detection , customer ranking , interactive visualization , and so forth . the paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules .

['electronic commerce', 'em algorithm', 'mixture models', 'profiles', 'transaction data', 'types of simulation']

profiles NOUN dobj generate

mining knowledge-sharing sites for viral marketing viral marketing takes advantage of networks of influence among customers to inexpensively achieve large changes in behavior . our research seeks to put it on a firmer footing by mining these networks from data , building probabilistic models of them , and using these models to choose the best viral marketing plan . knowledge-sharing sites , where customers review products and advise each other , are a fertile source for this type of data mining . in this paper we extend our previous techniques , achieving a large reduction in computational cost , and apply them to data from a knowledge-sharing site . we optimize the amount of marketing funds spent on each customer , rather than just making a binary decision on whether to market to him . we take into account the fact that knowledge of the network is partial , and that gathering that knowledge can itself have a cost . our results show the robustness and utility of our approach .

['direct marketing', 'knowledge sharing', 'linear models', 'probabilistic models', 'social networks', 'viral marketing']


the uci kdd archive of large data sets for data mining research and experimentation

['data archive']


mining distance-based outliers in near linear time with randomization and a simple pruning rule defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set . recently , much work has been conducted with the goal of finding fast algorithms for this task . we show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used . we test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude . our average case analysis suggests that much of the efficiency is because the time to process non-outliers , which are the majority of examples , does not depend on the size of the data set .

['anomaly detection', 'diskbased algorithms', 'distance-based operations', 'outliers']

outliers NOUN nsubj is
outliers NOUN appos rule
outliers NOUN dobj process

mining the network value of customers one of the major applications of data mining is in helping companies determine which potential customers to market to . if the expected profit from a customer is greater than the cost of marketing to her , the marketing action for that customer is executed . so far , work in this area has considered only the intrinsic value of the customer ( i. e , the expected profit from sales to her ) . we propose to model also the customer 's network value : the expected profit from sales to other customers she may influence to buy , the customers those may influence , and so on recursively . instead of viewing a market as a set of independent entities , we view it as a social network and model it as a markov random field . we show the advantages of this approach using a social network mined from a collaborative filtering database . marketing that exploits the network value of customers -- also known as viral marketing -- can be extremely effective , but is still a black art . our work can be viewed as a step towards providing a more solid foundation for it , taking advantage of the availability of large relevant databases .

['collaborative filtering', 'dependency networks', 'direct marketing', 'markov random fields', 'social networks', 'viral marketing']


maximizing the spread of influence through a social network models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains , including the diffusion of medical and technological innovations , the sudden and widespread adoption of various strategies in game-theoretic settings , and the effects of `` word of mouth '' in the promotion of new products . recently , motivated by the design of viral marketing strategies , domingos and richardson posed a fundamental algorithmic problem for such social network processes : if we can try to convince a subset of individuals to adopt a new product or innovation , and the goal is to trigger a large cascade of further adoptions , which set of individuals should we target ? we consider this problem in several of the most widely studied models in social network analysis . the optimization problem of selecting the most influential nodes is np-hard here , and we provide the first provable approximation guarantees for efficient algorithms . using an analysis framework based on submodular functions , we show that a natural greedy strategy obtains a solution that is provably within 63 % of optimal for several classes of models ; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks . we also provide computational experiments on large collaboration networks , showing that in addition to their provable guarantees , our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks .

['approximation algorithms', 'diffusion of innovations', 'nonnumerical algorithms and problems', 'social networks', 'viral marketing']


using the fractal dimension to cluster datasets

['fractals']


rapid detection of significant spatial clusters given an n x n grid of squares , where each square has a count cij and an underlying population pij , our goal is to find the rectangular region with the highest density , and to calculate its significance by randomization . an arbitrary density function d , dependent on a region 's total count c and total population p , can be used . for example , if each count represents the number of disease cases occurring in that square , we can use kulldorff 's spatial scan statistic dk to find the most significant spatial disease cluster . a naive approach to finding the maximum density region requires o ( n4 ) time , and is generally computationally infeasible . we present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure , bounds the maximum score of subregions contained in each region , and prunes regions which can not contain the maximum density region . for sufficiently dense regions , this method finds the maximum density region in o ( ( n log n ) 2 ) time , in practice resulting in significant ( 20-2000x ) speedups on both real and simulated datasets .

['biosurveillance', 'cluster detection', 'spatial data mining algorithms', 'spatial scan statistics']


model-based overlapping clustering while the vast majority of clustering algorithms are partitional , many real world datasets have inherently overlapping clusters . several approaches to finding overlapping clusters have come from work on analysis of biological datasets . in this paper , we interpret an overlapping clustering model proposed by segal et al. ( 23 ) as a generalization of gaussian mixture models , and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding bregman divergence . we provide the necessary algorithm modifications for this extension , and present results on synthetic data as well as subsets of 20-newsgroups and eachmovie datasets .

['bregman divergences', 'exponential model', 'graphical model', 'high-dimensional clustering', 'learning', 'overlapping clustering']


assessment and pruning of hierarchical model based clustering the goal of clustering is to identify distinct groups in a dataset . the basic idea of model-based clustering is to approximate the data density by a mixture model , typically a mixture of gaussians , and to estimate the parameters of the component densities , the mixing fractions , and the number of components from the data . the number of distinct groups in the data is then taken to be the number of mixture components , and the observations are partitioned into clusters ( estimates of the groups ) using bayes ' rule . if the groups are well separated and look gaussian , then the resulting clusters will indeed tend to be `` distinct '' in the most common sense of the word - contiguous , densely populated areas of feature space , separated by contiguous , relatively empty regions . if the groups are not gaussian , however , this correspondence may break down ; an isolated group with a non-elliptical distribution , for example , may be modeled by not one , but several mixture components , and the corresponding clusters will no longer be well separated . we present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters . we also propose a new clustering method that can be regarded as a hybrid between model-based and nonparametric clustering . the hybrid clustering algorithm prunes the cluster tree generated by hierarchical model-based clustering . starting with the tree corresponding to the mixture model chosen by the bayesian information criterion , it progressively merges clusters that do not appear to correspond to different modes of the data density .

['clustering', 'density estimation', 'model-based clustering', 'nonparametric clustering', 'unimodality']

clustering VERB pcomp based
clustering NOUN pobj of
clustering NOUN pobj of
clustering NOUN compound method
clustering NOUN pobj between
clustering NOUN compound algorithm
clustering NOUN pobj by

carpenter : finding closed patterns in long biological datasets the growth of bioinformatics has resulted in datasets with new characteristics . these datasets typically contain a large number of columns and a small number of rows . for example , many gene expression datasets may contain 10,000-100 ,000 columns but only 100-1000 rows . such datasets pose a great challenge for existing ( closed ) frequent pattern discovery algorithms , since they have an exponential dependence on the average row length . in this paper , we describe a new algorithm called carpenter that is specially designed to handle datasets having a large number of attributes and relatively small number of rows . several experiments on real bioinformatics datasets show that carpenter is orders of magnitude better than previous closed pattern mining algorithms like closet and charm .

['closed pattern', 'database applications', 'frequent pattern', 'row enumeration']


eliminating noisy information in web pages for data mining a commercial web page typically contains many information blocks . apart from the main content blocks , it usually has such blocks as navigation panels , copyright and privacy notices , and advertisements ( for business purposes and for easy user access ) . we call these blocks that are not the main content blocks of the page the noisy blocks . we show that the information contained in these noisy blocks can seriously harm web data mining . eliminating these noises is thus of great importance . in this paper , we propose a noise elimination technique based on the following observation : in a given web site , noisy blocks usually share some common contents and presentation styles , while the main content blocks of the pages are often diverse in their actual contents and\/or presentation styles . based on this observation , we propose a tree structure , called style tree , to capture the common presentation styles and the actual contents of the pages in a given web site . by sampling the pages of the site , a style tree can be built for the site , which we call the site style tree ( sst ) . we then introduce an information based measure to determine which parts of the sst represent noises and which parts represent the main contents of the site . the sst is employed to detect and eliminate noises in any web page of the site by mapping this page to the sst . the proposed technique is evaluated with two data mining tasks , web page clustering and classification . experimental results show that our noise elimination technique is able to improve the mining results significantly .

['noise detection', 'noise elimination', 'web mining']


tensor-cur decompositions for tensor-based data motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices , we develop a tensor-based extension of the matrix cur decomposition . the tensor-cur decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others . in this case , the tensor-cur decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms ofthe processes generating the data . in order to demonstrate the general applicability of this tensor decomposition , we apply it to problems in two diverse domains of data analysis : hyperspectral medical image analysis and consumer recommendation system analysis . in the hyperspectral data application , the tensor-cur decomposition is used to compress the data , and we show that classification quality is not substantially reduced even after substantial data compression . in the recommendation system application , the tensor-cur decomposition is used to reconstruct missing entries in a user-product-product preference tensor , and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of product-product comparisons from a new user .

['cur decomposition', 'hyperspectral image analysis', 'miscellaneous', 'miscellaneous', 'recommendation system analysis', 'tensor cur']


discovering the set of fundamental rule changes the world around us changes constantly . knowing what has changed is an important part of our lives . for businesses , recognizing changes is also crucial . it allows businesses to adapt themselves to the changing market needs . in this paper , we study changes of association rules from one time period to another . one approach is to compare the supports and\/or confidences of each rule in the two time periods and report the differences . this technique , however , is too simplistic as it tends to report a huge number of rule changes , and many of them are , in fact , simply the snowball effect of a small subset of fundamental changes . here , we present a technique to highlight the small subset of fundamental changes . a change is fundamental if it can not be explained by some other changes . the proposed technique has been applied to a number of real-life datasets . experiments results show that the number of rules whose changes are unexplainable is quite small ( about 20 % of the total number of changes discovered ) , and many of these unexplainable changes reflect some fundamental shifts in the application domain .

['change mining']


mining quantitative correlated patterns using an information-theoretic approach existing research on mining quantitative databases mainly focuses on mining associations . however , mining associations is too expensive to be practical in many cases . in this paper , we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations . we propose a new notion of quantitative correlated patterns ( qcps ) , which is founded on two formal concepts , mutual information and all-confidence . we first devise a normalization on mutual information and apply it to qcp mining to capture the dependency between the attributes . we further adopt all-confidence as a quality measure to control , at a finer granularity , the dependency between the attributes with specific quantitative intervals . we also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information , such that the interval combining is guided by the dependency between the attributes . we develop an algorithm , qcomine , to efficiently mine qcps by utilizing normalized mutual information and all-confidence to perform a two-level pruning . our experiments verify the efficiency of qcomine and the quality of the qcps .

['correlated patterns', 'information-theoretic approach', 'mutual information', 'quantitative databases']


discovering associations with numeric variables this paper further develops aumann and lindell 's ( 3 ) proposal for a variant of association rules for which the consequent is a numeric variable . it is argued that these rules can discover useful interactions with numeric data that can not be discovered directly using traditional association rules with discretization . alternative measures for identifying interesting rules are proposed . efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of auman and lindell 's algorithm is infeasible .

['association rule', 'impact rule', 'information search and retrieval', 'learning', 'numeric data', 'search']


correlation search in graph databases correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects . however , the research of correlation mining from graph databases is still lacking despite the fact that graph data , especially in various scientific domains , proliferate in recent years . in this paper , we propose a new problem of correlation mining from graph databases , called correlated graph search ( cgs ) . cgs adopts pearson 's correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs . however , the problem poses significant challenges , since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential . we derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database . with this result , we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates . to further improve the efficiency , we develop three heuristic rules and apply them on the candidate set to further reduce the search space . our extensive experiments demonstrate the effectiveness of our method on candidate reduction . the results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets .

['correlation', 'graph databases', "pearson's correlation coefficient"]

correlation NOUN compound search
correlation NOUN compound mining
correlation NOUN compound mining
correlation NOUN compound coefficient
correlation NOUN compound measure

a quickstart in frequent structure mining can make a difference given a database , structure mining algorithms search for substructures that satisfy constraints such as minimum frequency , minimum confidence , minimum interest and maximum frequency . examples of substructures include graphs , trees and paths . for these substructures many mining algorithms have been proposed . in order to make graph mining more efficient , we investigate the use of the `` quickstart principle '' , which is based on the fact that these classes of structures are contained in each other , thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity . we introduce the graph\/sequence\/tree extraction ( gaston ) algorithm that implements this idea by searching first for frequent paths , then frequent free trees and finally cyclic graphs . we investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives .

['frequent item sets', 'graphs', 'semi-structures', 'structures']

graphs NOUN dobj include
structures NOUN pobj of
graphs NOUN conj trees
structures NOUN pobj of

automatic multimedia cross-modal correlation discovery given an image ( or video clip , or audio song ) , how do we automatically assign keywords to it ? the general problem is to find correlations across the media in a collection of multimedia objects like video clips , with colors , and\/or motion , and\/or audio , and\/or text scripts . we propose a novel , graph-based approach , `` mmg '' , to discover such cross-modal correlations . our `` mmg '' method requires no tuning , no clustering , no user-determined constants ; it can be applied to any multimedia collection , as long as we have a similarity function for each medium ; and it scales linearly with the database size . we report auto-captioning experiments on the `` standard '' corel image database of 680 mb , where it outperforms domain specific , fine-tuned methods by up to 10 percentage points in captioning accuracy ( 50 % relative improvement ) .

['automatic image captioning', 'cross-modal correlation', 'graph-based model']


adaptive query processing for time-series data

['time-series query processing']


natural communities in large linked networks we are interested in finding natural communities in large-scale linked networks . our ultimate goal is to track changes over time in such communities . for such temporal tracking , we require a clustering algorithm that is relatively stable under small perturbations of the input data . we have developed an efficient , scalable agglomerative strategy and applied it to the citation graph of the nec citeseer database ( 250,000 papers ; 4.5 million citations ) . agglomerative clustering techniques are known to be unstable on data in which the community structure is not strong . we find that some communities are essentially random and thus unstable while others are natural and will appear in most clusterings . these natural communities will enable us to track the evolution of communities over time .

['hierarchical agglomerative clustering', 'information search and retrieval', 'large linked networks', 'natural communities', 'stability']


a framework for specifying explicit bias for revision of approximate information extraction rules

['information extraction', 'text mining', 'theory revision', 'user guided revision']


co-clustering based classification for out-of-domain documents in many real world applications , labeled data are in short supply . it often happens that obtaining labeled data in a new domain is expensive and time consuming , while there may be plenty of labeled data from a related but different domain . traditional machine learning is not able to cope well with learning across different domains . in this paper , we address this problem for a text-mining task , where the labeled data are under one distribution in one domain known as in-domain data , while the unlabeled data are under a related but different domain known as out-of-domain data . our general goal is to learn from the in-domain and apply the learned knowledge to out-of-domain . we propose a co-clustering based classification ( cocc ) algorithm to tackle this problem . co-clustering is used as a bridge to propagate the class structure and knowledge from the in-domain to the out-of-domain . we present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results , even when the distributions between the two data are different . the experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms .

['classification', 'co-clustering', 'kullback-leibler divergence', 'out-of-domain']

co-clustering ADJ amod classification
classification NOUN nsubj are
classification NOUN dobj propose
classification NOUN compound results
classification NOUN compound performance

generating non-redundant association rules

['learning']


generalizing the notion of support the goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non-traditional types of patterns and non-binary data . to that end , we describe a framework for generalizing support that is based on the simple , but useful observation that support can be viewed as the composition of two functions : a function that evaluates the strength or presence of a pattern in each object ( transaction ) and a function that summarizes these evaluations with a single number . a key goal of any framework is to allow people to more easily express , explore , and communicate ideas , and hence , we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns , such as frequent itemsets , general boolean patterns , and error-tolerant itemsets . we also present two examples of the practical usefulness of generalized support . one example shows the usefulness of support functions for continuous data . another example shows how the hyperclique pattern -- an association pattern originally defined for binary data -- can be extended to continuous data by generalizing a support function .

['association analysis', 'hyperclique', 'support']

support NOUN pobj of
support NOUN pobj for
support NOUN nsubjpass viewed
support NOUN compound framework
support NOUN dobj describe
support NOUN pobj of
support NOUN compound functions
hyperclique ADJ amod pattern
support NOUN compound function

efficient discovery of error-tolerant frequent itemsets in high dimensions we present a generalization of frequent itemsets allowing for the notion of errors in the itemset definition . we motivate the problem and present an efficient algorithm that identifies error-tolerant frequent clusters of items in transactional data ( customer-purchase data , web browsing data , text , etc. ) . the algorithm exploits sparseness of the underlying data to find large groups of items that are correlated over database records ( rows ) . the notion of transaction coverage allows us to extend the algorithm and view it as a fast clustering algorithm for discovering segments of similar transactions in binary sparse data . we evaluate the new algorithm on three real-world applications : clustering high-dimensional data , query selectivity estimation and collaborative filtering . results show that the algorithm consistently uncovers structure in large sparse databases that other traditional clustering algorithms fail to find .

['collaborative filtering', 'error-tolerant frequent itemset', 'high dimensions', 'query selectivity estimation']


mining e-commerce data : the good , the bad , and the ugly organizations conducting electronic commerce ( e-commerce ) can greatly benefit from the insight that data mining of transactional and clickstream data provides . such insight helps not only to improve the electronic channel ( e.g. , a web site ) , but it is also a learning vehicle for the bigger organization conducting business at brick-and-mortar stores . the e-commerce site serves as an early alert system for emerging patterns and a laboratory for experimentation . for successful data mining , several ingredients are needed and e-commerce provides all the right ones ( the good ) . web server logs , which are commonly used as the source of data for mining e-commerce data , were designed to debug web servers , and the data they provide is insufficient , requiring the use of heuristics to reconstruct events . moreover , many events are never logged in web server logs , limiting the source of data for mining ( the bad ) . many of the problems of dealing with web server log data can be resolved by properly architecting the e-commerce sites to generate data needed for mining . even with a good architecture , however , there are challenging problems that remain hard to solve ( the ugly ) . lessons and metrics based on mining real e-commerce data are presented .

['application server', 'e-commerce', 'learning', 'web server', 'web site architecture']

learning NOUN amod vehicle

cross-training : learning probabilistic mappings between topics classification is a well-established operation in text mining . given a set of labels a and a set da of training documents tagged with these labels , a classifier learns to assign labels to unlabeled test documents . suppose we also had available a different set of labels b , together with a set of documents db marked with labels from b. if a and b have some semantic overlap , can the availability of db help us build a better classifier for a , and vice versa ? we answer this question in the affirmative by proposing cross-training : a new approach to semi-supervised learning in presence of multiple label sets . we give distributional and discriminative algorithms for cross-training and show , through extensive experiments , that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification .

['design methodology', 'document classification', 'em', 'learning', 'semi-supervised multi-task learning', 'support vector machines']

learning NOUN pobj to

efficiently handling feature redundancy in high-dimensional data high-dimensional data poses a severe challenge for data mining . feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining . traditionally , feature selection is focused on removing irrelevant features . however , for high-dimensional data , removing redundant features is equally critical . in this paper , we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model . the extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features .

['feature selection', 'high-dimensional data', 'redundancy']

redundancy NOUN dobj handling
redundancy NOUN pobj of

privacy-preserving bayesian network structure computation on distributed heterogeneous data as more and more activities are carried out using computers and computer networks , the amount of potentially sensitive data stored by business , governments , and other parties increases . different parties may wish to benefit from cooperative use of their data , but privacy regulations and other privacy concerns may prevent the parties from sharing their data . privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data is not revealed . in this paper , we present a privacy-preserving protocol for a particular data mining task : learning the bayesian network structure for distributed heterogeneous data . in this setting , two parties owning confidential databases wish to learn the structure of bayesian network on the combination of their databases without revealing anything about their data to each other . we give an efficient and privacy-preserving version of the k2 algorithm to construct the structure of a bayesian network for the parties ' joint data .

['bayesian network', 'distributed databases', 'privacy-preserving data mining']


hardening soft information sources

['data integration', 'miscellaneous']


efficient data reduction with ease a variety of mining and analysis problems -- ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes -- involve the extraction of knowledge from a set of categorical count data . such data can be viewed as a collection of `` transactions , '' where a transaction is a fixed-length vector of counts . classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow . one effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data . we present a new data-reduction algorithm , called ease , for producing such a sample . like the fast algorithm introduced by chen et al. , ease is especially designed for count data applications . both ease and fast take a relatively large initial random sample and then deterministically produce a subsample whose `` distance '' -- appropriately defined -- from the complete database is minimal . unlike fast , which obtains the final subsample by quasi-greedy descent , ease uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving . experiments both in the context of association rule mining and classical ï‡2 contingency-table analysis show that ease outperforms both fast and simple random sampling , sometimes dramatically .

['association rules', 'count dataset', 'data streams', 'database applications', 'frequency estimation', 'olap', 'sampling']

sampling NOUN advcl ease

a bayesian network framework for reject inference most learning methods assume that the training set is drawn randomly from the population to which the learned model is to be applied . however in many applications this assumption is invalid . for example , lending institutions create models of who is likely to repay a loan from training sets consisting of people in their records to whom loans were given in the past ; however , the institution approved loan applications previously based on who was thought unlikely to default . learning from only approved loans yields an incorrect model because the training set is a biased sample of the general population of applicants . the issue of including rejected samples in the learning process , or alternatively using rejected samples to adjust a model learned from accepted samples only , is called reject inference . the main contribution of this paper is a systematic analysis of different cases that arise in reject inference , with explanations of which cases arise in various real-world situations . we use bayesian networks to formalize each case as a set of conditional independence relationships and identify eight cases , including the familiar missing completely at random ( mcar ) , missing at random ( mar ) , and missing not at random ( mnar ) cases . for each case we present an overview of available learning algorithms . these algorithms have been published in separate fields of research , including epidemiology , econometrics , clinical trial evaluation , sociology , and credit scoring ; our second major contribution is to describe these algorithms in a common framework .

['bayesian networks', 'expectation-maximization', 'heckman estimator', 'propensity scores', 'reject inference', 'sample selection bias']


a rank sum test method for informative gene discovery finding informative genes from microarray data is an important research problem in bioinformatics research and applications . most of the existing methods rank features according to their discriminative capability and then find a subset of discriminative genes ( usually top k genes ) . in particular , t-statistic criterion and its variants have been adopted extensively . this kind of methods rely on the statistics principle of t-test , which requires that the data follows a normal distribution . however , according to our investigation , the normality condition often can not be met in real data sets . to avoid the assumption of the normality condition , in this paper , we propose a rank sum test method for informative gene discovery . the method uses a rank-sum statistic as the ranking criterion . moreover , we propose using the significance level threshold , instead of the number of informative genes , as the parameter . the significance level threshold as a parameter carries the quality specification in statistics . we follow the pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t-statistic method in theory . to verify the effectiveness of the rank sum method , we use support vector machine ( svm ) to construct classifiers based on the identified informative genes on two well known data sets , namely colon data and leukemia data . the prediction accuracy reaches 96.2 % on the colon data and 100 % on the leukemia data . the results are clearly better than those from the previous feature ranking methods . by experiments , we also verify that using significance level threshold is more effective than directly specifying an arbitrary k.

['classification', 'informative genes', 'rank sum test', 'ranking criterion', 'support vector machine']


extending naã¯ve bayes classifiers using long itemsets

['association mining', 'bayesian learning', 'classification', 'lazy learning']


scalable robust covariance and correlation estimates for data mining covariance and correlation estimates have important applications in data mining . in the presence of outliers , classical estimates of covariance and correlation matrices are not reliable . a small fraction of outliers , in some cases even a single outlier , can distort the classical covariance and correlation estimates making them virtually useless . that is , correlations for the vast majority of the data can be very erroneously reported ; principal components transformations can be misleading ; and multidimensional outlier detection via mahalanobis distances can fail to detect outliers . there is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases . all such estimators have unacceptable exponential complexity in the number of variables and quadratic complexity in the number of observations . in this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complexity in the number of variables and linear complexity in the number of observations . these estimators are based on several forms of pairwise robust covariance and correlation estimates . the estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by ( 14 ) . we show that the estimators have attractive robustness properties , and give an example that uses one of the estimators in the new insightful miner data mining product .

['outliers', 'robust estimators', 'robust statistics', 'scalable algorithm']

outliers NOUN pobj of
outliers NOUN pobj of
outliers NOUN dobj detect

constraint-driven clustering clustering methods can be either data-driven or need-driven . data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements . thus , need-driven ( e.g. constrained ) clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks , privacy preservation , and market segmentation . however , the existing methods of constrained clustering require users to provide the number of clusters , which is often unknown in advance , but has a crucial impact on the clustering result . in this paper , we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters . for this purpose , we introduce a novel cluster model , constraint-driven clustering ( cdc ) , which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints . two general types of constraints are considered , i.e. minimum significance constraints and minimum variance constraints , as well as combinations of these two types . we prove the np-hardness of the cdc problem with different constraints . we propose a novel dynamic data structure , the cd-tree , which organizes data points in leaf nodes such that each leaf node approximately satisfies the cdc constraints and minimizes the objective function . based on cd-trees , we develop an efficient algorithm to solve the new clustering problem . our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm .

['clustering', 'constraints', 'np-hardness']

clustering NOUN nsubj be
clustering NOUN compound methods
clustering NOUN nsubj is
clustering NOUN pobj of
clustering NOUN compound result
constraints NOUN nsubj decide
clustering NOUN dobj introduce
constraints NOUN dobj satisfy
constraints NOUN pobj of
constraints NOUN appos types
constraints NOUN conj constraints
constraints NOUN pobj with
constraints NOUN dobj satisfies
clustering NOUN compound problem

selection , combination , and evaluation of effective software sensors for detecting abnormal computer usage we present and empirically analyze a machine-learning approach for detecting intrusions on individual computers . our winnow-based algorithm continually monitors user and system behavior , recording such properties as the number of bytes transferred over the last 10 seconds , the programs that currently are running , and the load on the cpu . in all , hundreds of measurements are made and analyzed each second . using this data , our algorithm creates a model that represents each particular computer 's range of normal behavior . parameters that determine when an alarm should be raised , due to abnormal activity , are set on a per-computer basis , based on an analysis of training data . a major issue in intrusion-detection systems is the need for very low false-alarm rates . our empirical results suggest that it is possible to obtain high intrusion-detection rates ( 95 % ) and low false-alarm rates ( less than one per day per computer ) , without `` stealing '' too many cpu cycles ( less than 1 % ) . we also report which system measurements are the most valuable in terms of detecting intrusions . a surprisingly large number of different measurements prove significantly useful .

['anomaly detection', 'feature selection', 'intrusion detection', 'learning', 'machine learning', 'security and protection', 'user modeling', 'windows 2000', 'winnow algorithm']

learning VERB amod approach

correlating synchronous and asynchronous data streams in a variety of modern mining applications , data are commonly viewed as infinite time ordered data streams rather as finite data sets stored on disk . this view challenges fundamental assumptions commonly made in the context of several data mining algorithms . in this paper , we study the problem of identifying correlations between multiple data streams . in particular , we propose algorithms capable of capturing correlations between multiple continuous data streams in a highly efficient and accurate manner . our algorithms and techniques are applicable in the case of both synchronous and asynchronous data streaming environments . we capture correlations between multiple streams using the well known technique of singular value decomposition ( svd ) . correlations between data items , and the svd technique in particular , have been repeatedly utilized in an off-line ( non stream ) data mining problems , for example forecasting , approximate query answering , and data reduction . we propose a methodology based on a combination of dimensionality reduction and sampling to make the svd technique suitable for a data stream context . our techniques are approximate , trading accuracy with performance , and we analytically quantify this tradeoff . we present a through experimental evaluation , using both real and synthetic data sets , from a prototype implementation of our technique , investigating the impact of various parameters in the accuracy of the overall computation . our results indicate , that correlations between multiple data streams can be identified very efficiently and accurately . the algorithms proposed herein , are presented as generic tools , with a multitude of applications on data stream mining problems .

['approximate computation', 'data streams', 'singular value decomposition']


a classification-based methodology for planning audit strategies in fraud detection

['classification', 'decision trees', 'fraud detection', 'integration of querying and mining', 'knowledge discovery in databases', 'logic-based database languages']

classification NOUN npadvmod based

automatic mining of fruit fly embryo images we present femine , an automatic system for image-based gene expression analysis . we perform experiments on the largest publicly available collection of drosophila ish ( in situ hybridization ) images , showing that our femine system achieves excellent performance in classification , clustering , and content-based image retrieval . the major innovation of femine is the use of automatically discovered latent spatial `` themes '' of gene expressions , lges , in the whole-embryo context , as opposed to patterns in nearly disjoint portions of an embryo proposed in previous methods .

['drosophila', 'embryonic image analysis', 'gene expression', 'independent component analysis']

drosophila ADJ compound ish

a multinomial clustering model for fast simulation of computer architecture designs computer architects utilize simulation tools to evaluate the merits of a new design feature . the time needed to adequately evaluate the tradeoffs associated with adding any new feature has become a critical issue . recent work has found that by identifying execution phases present in common workloads used in simulation studies , we can apply clustering algorithms to significantly reduce the amount of time needed to complete the simulation . our goal in this paper is to demonstrate the value of this approach when applied to the set of industry-standard benchmarks most commonly used in computer architecture studies . we also look to improve upon prior work by applying more appropriate clustering algorithms to identify phases , and to further reduce simulation time . we find that the phase clustering in computer architecture simulation has many similarities to text clustering . in prior work on clustering techniques to reduce simulation time , k-means clustering was used to identify representative program phases . in this paper we apply a mixture of multinomials to the clustering problem and show its advantages over using k-means on simulation data . we have implemented these two clustering algorithms and evaluate how well they can characterize program behavior . by adopting a mixture of multinomials model , we find that we can maintain simulation result fidelity , while greatly reducing overall simulation time . we report results for a range of applications taken from the spec2000 benchmark suite .

['clustering', 'clustering', 'em', 'k-means', 'mixture of multinomials', 'program phase', 'simulation', 'single data stream architectures']

clustering NOUN compound model
simulation NOUN pobj for
simulation NOUN compound tools
simulation NOUN compound studies
clustering VERB compound algorithms
simulation NOUN dobj complete
clustering NOUN compound algorithms
simulation NOUN compound time
clustering NOUN acl phase
simulation NOUN pobj in
clustering NOUN dobj text
clustering NOUN pcomp on
simulation NOUN compound time
clustering NOUN nsubjpass used
clustering NOUN compound problem
simulation NOUN compound data
clustering VERB compound algorithms
simulation NOUN nsubj result
simulation NOUN compound time

using retrieval measures to assess similarity in mining dynamic web clickstreams while scalable data mining methods are expected to cope with massive web data , coping with evolving trends in noisy data in a continuous fashion , and without any unnecessary stoppages and reconfigurations is still an open challenge . this dynamic and single pass setting can be cast within the framework of mining evolving data streams . in this paper , we explore the task of mining mass user profiles by discovering evolving web session clusters in a single pass with a recently proposed scalable immune based clustering approach ( tecno-streams ) , and study the effect of the choice of different similarity measures on the mining process and on the interpretation of the mined patterns . we propose a simple similarity measure that has the advantage of explicitly coupling the precision and coverage criteria to the early learning stages , and furthermore requiring that the affinity of the data to the learned profiles or summaries be defined by the minimum of their coverage or precision , hence requiring that the learned profiles are simultaneously precise and complete , with no compromises . in our experiments , we study the task of mining evolving user profiles from web clickstream data ( web usage mining ) in a single pass , and under different trend sequencing scenarios , showing that compared oto the cosine similarity measure , the proposed similarity measure explicitly based on precision and coverage allows the discovery of more correct profiles at the same precision or recall quality levels .

['artificial immune systems', 'clustering', 'mining evolving data', 'personalization', 'stream data mining', 'web mining']

clustering NOUN amod approach

cryptographically private support vector machines we propose private protocols implementing the kernel adatron and kernel perceptron learning algorithms , give private classification protocols and private polynomial kernel computation protocols . the new protocols return their outputs - either the kernel value , the classifier or the classifications - in encrypted form so that they can be decrypted only by a common agreement by the protocol participants . we show how to use the encrypted classifications to privately estimate many properties of the data and the classifier . the new svm classifiers are the first to be proven private according to the standard cryptographic definitions .

['kernel methods', 'privacy preserving data mining']


recovering latent time-series from their observed sums : network tomography with particle filters . hidden variables , evolving over time , appear in multiple settings , where it is valuable to recover them , typically from observed sums . our driving application is ` network tomography ' , where we need to estimate the origin-destination ( od ) traffic flows to determine , e.g. , who is communicating with whom in a local area network . this information allows network engineers and managers to solve problems in design , routing , configuration debugging , monitoring and pricing . unfortunately the direct measurement of the od traffic is usually difficult , or even impossible ; instead , we can easily measure the loads on every link , that is , sums of desirable od flows . in this paper we propose i-filter , a method to solve this problem , which improves the state-of-the-art by ( a ) introducing explicit time dependence , and by ( b ) using realistic , non-gaussian marginals in the statistical models for the traffic flows , as never attempted before . we give experiments on real data , where i-filter scales linearly with new observations and out-performs the best existing solutions , in a wide variety of settings . specifically , on real network traffic measured at cmu , and at at&t , i-filter reduced the estimation errors between 15 % and 46 % in all cases .

['empirical bayes', 'informative priors', 'learning', 'link loads', 'mcmc', 'origin-destination traffic flows', 'particle filter', 'self-organizing bayesian dynamical system']


fast best-effort pattern matching in large attributed graphs we focus on large graphs where nodes have attributes , such as a social network where the nodes are labeled with each person 's job title . in such a setting , we want to find subgraphs that match a user query pattern . for example , a `` star '' query would be , `` find a ceo who has strong interactions with a manager , a lawyer , and an accountant , or another structure as close to that as possible '' . similarly , a `` loop '' query could help spot a money laundering ring . traditional sql-based methods , as well as more recent graph indexing methods , will return no answer when an exact match does not exist . this is the first main feature of our method . it can find exact - , as well as near-matches , and it will present them to the user in our proposed `` goodness '' order . for example , our method tolerates indirect paths between , say , the `` ceo '' and the `` accountant '' of the above sample query , when direct paths do n't exist . its second feature is scalability . in general , if the query has nq nodes and the data graph has n nodes , the problem needs polynomial time complexity o ( n n q ) , which is prohibitive . our g-ray ( `` graph x-ray '' ) method finds high-quality subgraphs in time linear on the size of the data graph . experimental results on the dlbp author-publication graph ( with 356k nodes and 1.9 m edges ) illustrate both the effectiveness and scalability of our approach . the results agree with our intuition , and the speed is excellent . it takes 4 seconds on average fora 4-node query on the dblp graph .

['attributed graph', 'miscellaneous', 'pattern match', 'random walk']


center-piece subgraphs : problem definition and fast solutions given q nodes in a social network ( say , authorship network ) , how can we find the node\/author that is the center-piece , and has direct or indirect connections to all , or most of them ? for example , this node could be the common advisor , or someone who started the research area that the q nodes belong to . isomorphic scenarios appear in law enforcement ( find the master-mind criminal , connected to all current suspects ) , gene regulatory networks ( find the protein that participates in pathways with all or most of the given q proteins ) , viral marketing and many more . connection subgraphs is an important first step , handling the case of q = 2 query nodes . then , the connection subgraph algorithm finds the b intermediate nodes , that provide a good connection between the two original query nodes . here we generalize the challenge in multiple dimensions : first , we allow more than two query nodes . second , we allow a whole family of queries , ranging from ` or ' to ` and ' , with ` softand ' in-between . finally , we design and compare a fast approximation , and study the quality\/speed trade-off . we also present experiments on the dblp dataset . the experiments confirm that our proposed method naturally deals with multi-source queries and that the resulting subgraphs agree with our intuition . wall-clock timing results on the dblp dataset show that our proposed approximation achieve good accuracy for about 6:1 speedup .

['center-piece subgraph', 'goodness score', 'kand']


single-pass online learning : performance , voting schemes and online feature selection to learn concepts over massive data streams , it is essential to design inference and learning methods that operate in real time with limited memory . online learning methods such as perceptron or winnow are naturally suited to stream processing ; however , in practice multiple passes over the same training data are required to achieve accuracy comparable to state-of-the-art batch learners . in the current work we address the problem of training an on-line learner with a single passover the data . we evaluate several existing methods , and also propose a new modification of margin balanced winnow , which has performance comparable to linear svm . we also explore the effect of averaging , a.k.a. voting , on online learning . finally , we describe how the new modified margin balanced winnow algorithm can be naturally adapted to perform feature selection . this scheme performs comparably to widely-used batch feature selection methods like information gain or chi-square , with the advantage of being able to select features on-the-fly . taken together , these techniques allow single-pass online learning to be competitive with batch techniques , and still maintain the advantages of on-line learning .

['averaging', 'learning', 'models', 'online learning', 'voting', 'winnow']

learning NOUN advcl is
voting VERB compound schemes
learning NOUN conj inference
learning VERB compound methods
winnow NOUN conj perceptron
winnow NOUN advmod balanced
averaging NOUN pobj of
voting NOUN conj averaging
learning NOUN pobj on
winnow NOUN prep balanced
learning VERB nsubj be
learning NOUN pobj of

interestingness of frequent itemsets using bayesian networks as background knowledge the paper presents a method for pruning frequent itemsets based on background knowledge represented by a bayesian network . the interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the bayesian network . efficient algorithms are presented for finding interestingness of a collection of frequent itemsets , and for finding all attribute sets with a given minimum interestingness . practical usefulness of the algorithms and their efficiency have been verified experimentally .

['association rule', 'background', 'bayesian network', 'frequent itemset', 'interestingness', 'knowledge']

interestingness ADJ nsubj presents
background NOUN compound knowledge
knowledge NOUN pobj as
background NOUN compound knowledge
knowledge NOUN pobj on
interestingness NOUN nsubjpass defined
interestingness NOUN dobj finding
interestingness NOUN pobj with

mining risk patterns in medical data in this paper , we discuss a problem of finding risk patterns in medical data . we define risk patterns by a statistical metric , relative risk , which has been widely used in epidemiological research . we characterise the problem of mining risk patterns as an optimal rule discovery problem . we study an anti-monotone property for mining optimal risk pattern sets and present an algorithm to make use of the property in risk pattern discovery . the method has been applied to a real world data set to find patterns associated with an allergic event for ace inhibitors . the algorithm has generated some useful results for medical researchers .

['medical application', 'optimal risk pattern set', 'relative risk', 'rule']

rule NOUN compound discovery

mining phenotypes and informative genes from gene expression data mining microarray gene expression data is an important research topic in bioinformatics with broad applications . while most of the previous studies focus on clustering either genes or samples , it is interesting to ask whether we can partition the complete set of samples into exclusive groups ( called phenotypes ) and find a set of informative genes that can manifest the phenotype structure . in this paper , we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data . some statistics-based metrics are proposed to measure the quality of the mining results . two interesting algorithms are developed : the heuristic search and the mutual reinforcing adjustment method . we present an extensive performance study on both real-world data sets and synthetic data sets . the mining results from the two proposed methods are clearly better than those from the previous methods . they are ready for the real-world applications . between the two methods , the mutual reinforcing adjustment method is in general more scalable , more effective and with better quality of the mining results .

['array data', 'bioinformatics', 'informative genes', 'phenotype']

bioinformatics NOUN pobj in
phenotype NOUN compound structure

information genealogy : uncovering the flow of ideas in non-hyperlinked document databases we now have incrementally-grown databases of text documents ranging back for over a decade in areas ranging from personal email , to news-articles and conference proceedings . while accessing individual documents is easy , methods for overviewing and understanding these collections as a whole are lacking in number and in scope . in this paper , we address one such global analysis task , namely the problem of automatically uncovering how ideas spread through the collection over time . we refer to this problem as information genealogy . in contrast to bibliometric methods that are limited to collections with explicit citation structure , we investigate content-based methods requiring only the text and timestamps of the documents . in particular , we propose a language-modeling approach and a likelihood ratio test to detect influence between documents in a statistically well-founded way . furthermore , we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection . experiments on the nips conference proceedings and the physics arxiv show that our method is more effective than methods based on document similarity .

['citation inference', 'flow of ideas', 'information genealogy', 'language models', 'miscellaneous', 'temporal data', 'text mining']


topics over time : a non-markov continuous-time model of topical trends this paper presents an lda-style topic model that captures not only the low-dimensional structure of data , but also how the structure changes over time . unlike other recent work that relies on markov assumptions or discretization of time , here each topic is associated with a continuous distribution over timestamps , and for each generated document , the mixture distribution over topics is influenced by both word co-occurrences and the document 's timestamp . thus , the meaning of a particular topic can be relied upon as constant , but the topics ' occurrence and correlations change significantly over time . we present results on nine months of personal email , 17 years of nips research papers and over 200 years of presidential state-of-the-union addresses , showing improved topics , better timestamp prediction , and interpretable trends .

['graphical models', 'learning', 'temporal analysis', 'topic modeling']


machine learning for online query relaxation in this paper we provide a fast , data-driven solution to the failing query problem : given a query that returns an empty answer , how can one relax the query 's constraints so that it returns a non-empty set of tuples ? we introduce a novel algorithm , loqr , which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes . loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query . in a first step , loqr uses a small , randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute 's value satisfies the constraints in the failing query ; this query-driven operation is performed online for each failing query . in the second step , loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query ; then it uses the attributes ' values from this rule to relax the failing query 's constraints . our experiments on six application domains show that loqr is both robust and fast : it successfully relaxes more than 95 % of the failing queries , and it takes under a second for processing queries that consist of up to 20 attributes ( larger queries of up to 93 attributes are processed in several seconds ) .

['failing query', 'learning', 'nearest neighbor', 'online query relaxation', 'rule learning', 'web-based information sources']

learning VERB acl machine

yale : rapid prototyping for complex data mining tasks kdd is a complex and demanding task . while a large number of methods has been established for numerous problems , many challenges remain to be solved . new tasks emerge requiring the development of new methods or processing schemes . like in software development , the development of such solutions demands for careful analysis , specification , implementation , and testing . rapid prototyping is an approach which allows crucial design decisions as early as possible . a rapid prototyping system should support maximal re-use and innovative combinations of existing methods , as well as simple and quick integration of new ones . this paper describes yale , a free open-source environment forkdd and machine learning . yale provides a rich variety of methods whichallows rapid prototyping for new applications and makes costlyre-implementations unnecessary . additionally , yale offers extensive functionality for process evaluation and optimization which is a crucial property for any kdd rapid prototyping tool . following the paradigm of visual programming eases the design of processing schemes . while the graphical user interface supports interactive design , the underlying xml representation enables automated applications after the prototyping phase . after a discussion of the key concepts of yale , we illustrate the advantages of rapid prototyping for kdd on case studies ranging from data pre-processing to result visualization . these case studies cover tasks like feature engineering , text mining , data stream mining and tracking drifting concepts , ensemble methods and distributed data mining . this variety of applications is also reflected in a broad user base , we counted more than 40,000 downloads during the last twelve months .

['audio and text mining', 'data pre-processing', 'data stream mining', 'design methodology', 'distributed data mining', 'feature construction', 'kdd system', 'multimedia mining', 'rapid prototyping', 'result visualization']


a framework for ontology-driven subspace clustering traditional clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes . while domain knowledge is always the best way to justify clustering , few clustering algorithms have ever take domain knowledge into consideration . in this paper , the domain knowledge is represented by hierarchical ontology . we develop a framework by directly incorporating domain knowledge into clustering process , yielding a set of clusters with strong ontology implication . during the clustering process , ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithms . meanwhile , the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace clusters with significant categorical enrichment onto the ontology hierarchy . our experiments on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality . meanwhile , many hierarchical organizations of gene clusters corresponding to a sub-hierarchies in gene ontology were also successfully captured .

['ontology', 'subspace clustering', 'tendency preserving']

ontology NOUN pobj by
ontology NOUN compound implication
ontology NOUN compound information
ontology NOUN compound hierarchy
ontology NOUN dobj using
ontology NOUN pobj by
ontology NOUN pobj in

sleeved coclustering a cocluster of a m x n matrix x is a submatrix determined by a subset of the rows and a subset of the columns . the problem of finding coclusters with specific properties is of interest , in particular , in the analysis of microarray experiments . in that case the entries of the matrix x are the expression levels of $ m $ genes in each of $ n $ tissue samples . one goal of the analysis is to extract a subset of the samples and a subset of the genes , such that the expression levels of the chosen genes behave similarly across the subset of the samples , presumably reflecting an underlying regulatory mechanism governing the expression level of the genes . we propose to base the similarity of the genes in a cocluster on a simple biological model , in which the strength of the regulatory mechanism in sample j is hj , and the response strength of gene i to the regulatory mechanism is gi . in other words , every two genes participating in a good cocluster should have expression values in each of the participating samples , whose ratio is a constant depending only on the two genes . noise in the expression levels of genes is taken into account by allowing a deviation from the model , measured by a relative error criterion . the sleeve-width of the cocluster reflects the extent to which entry i , j in the cocluster is allowed to deviate , relatively , from being expressed as the product gihj . we present a polynomial-time monte-carlo algorithm which outputs a list of coclusters whose sleeve-widths do not exceed a prespecified value . moreover , we prove that the list includes , with fixed probability , a cocluster which is near-optimal in its dimensions . extensive experimentation with synthetic data shows that the algorithm performs well .

['clustering', 'clustering', 'co-regulation', 'coclustering', 'gene expression data']

coclustering VERB xcomp sleeved

data mining criteria for tree-based regression and classification this paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees . to this end , we propose new splitting criteria for growing trees . conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit between the left and the right side . by contrast , we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data , as opposed to modeling all of the data equally well . the new criteria do not split based on a compromise between the left and the right bucket ; they effectively pick the more interesting bucket and ignore the other . as expected , the result is often a simpler characterization of interesting subsets of the data . less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions . surprisingly , it is a `` flaw '' that works to their advantage : the new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges . this so-called `` end-cut problem '' leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees .

['boston housing data', 'cart', 'pima indians diabetes data', 'splitting criteria']


rule extraction from linear support vector machines we describe an algorithm for converting linear support vector machines and any other arbitrary hyperplane-based linear classifiers into a set of non-overlapping rules that , unlike the original classifier , can be easily interpreted by humans . each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve . we discuss various properties of the algorithm and provide proof of convergence for two different optimization criteria we demonstrate the performance and the speed of the algorithm on linear classifiers learned from real-world datasets , including a medical dataset on detection of lung cancer from medical images . the ability to convert svm 's and other `` black-box '' classifiers into a set of human-understandable rules , is critical not only for physician acceptance , but also to reducing the regulatory barrier for medical-decision support systems based on such classifiers .

['linear classifiers', 'mathematical programming', 'medical decision-support', 'miscellaneous', 'rule extraction']


a scalable modular convex solver for regularized risk minimization a wide variety of machine learning problems can be described as minimizing a regularized risk functional , with different algorithms using different notions of risk and different regularizers . examples include linear support vector machines ( svms ) , logistic regression , conditional random fields ( crfs ) , and lasso amongst others . this paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems . it can be parallelized on a cluster of workstations , allows for data-locality , and can deal with regularizers such as l1 and l2 penalties . at present , our solver implements 20 different estimation problems , can be easily extended , scales to millions of observations , and is up to 10 times faster than specialized solvers for many applications . the open source code is freely available as part of the elefant toolbox .

['convexity', 'optimization']


mining comparable bilingual text corpora for cross-language information integration integrating information in multiple natural languages is a challenging task that often requires manually created linguistic resources such as a bilingual dictionary or examples of direct translations of text . in this paper , we propose a general cross-lingual text mining method that does not rely on any of these resources , but can exploit comparable bilingual text corpora to discover mappings between words and documents in different languages . comparable text corpora are collections of text documents in different languages that are about similar topics ; such text corpora are often naturally available ( e.g. , news articles in different languages published in the same time period ) . the main idea of our method is to exploit frequency correlations of words in different languages in the comparable corpora and discover mappings between words in different languages . such mappings can then be used to further discover mappings between documents in different languages , achieving cross-lingual information integration . evaluation of the proposed method on a 120mb chinese-english comparable news collection shows that the proposed method is effective for mapping words and documents in english and chinese . since our method only relies on naturally available comparable corpora , it is generally applicable to any language pairs as long as we have comparable corpora .

['comparable corpora', 'cross-lingual text mining', 'document alignment', 'frequency correlation', 'information search and retrieval']


data filtering for automatic classification of rocks from reflectance spectra the ability to identify the mineral composition of rocks and soils is an important tool for the exploration of geological sites . for instance , nasa intends to design robots that are sufficiently autonomous to perform this task on planetary missions . spectrometer readings provide one important source of data for identifying sites with minerals of interest . reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths . spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals . for some mineral classes , carbonates for example , specific short spectral intervals are known to carry a distinctive signature . finding similar distinctive spectral ranges for other mineral classes is not an easy problem . we propose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals . in one set of studies , we partition the whole interval of wavelengths available in our data into sub-intervals , or bins , and use a genetic algorithm to evaluate a candidate selection of subintervals . as alternatives to this computationally expensive search technique , we present an entropy-based heuristic that gives higher scores for wavelengths more likely to distinguish between classes , as well as other greedy search procedures . results are presented for four different classes , showing reasonable improvements in identifying some , but not all , of the mineral classes tested .

['applications', 'design methodology']


generating semantic annotations for frequent patterns with context analysis as a fundamental data mining task , frequent pattern mining has widespread applications in many different domains . research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns , but little attention has been paid to the important nextstep - interpreting the discovered frequent patterns . although some recent work has studied the compression and summarization of frequent patterns , the proposed techniques can only annotate a frequent pattern with non-semantical information ( e.g. support ) , which provides only limited help for a user to understand the patterns . in this paper , we propose the novel problem of generating semantic annotations for frequent patterns . the goal is to annotate a frequent pattern with in-depth , concise , and structured information that can better indicate the hidden meanings of the pattern . we propose a general approach to generate such anannotation for a frequent pattern by constructing its context model , selecting informative context indicators , and extracting representative transactions and semantically similar patterns . this general approach has potentially many applications such as generating a dictionary-like description for a pattern , finding synonym patterns , discovering semantic relations , and summarizing semantic classes of a set of frequent patterns . experiments on different datasets show that our approach is effective in generating semantic pattern annotations .

['frequent pattern', 'pattern annotation', 'pattern context', 'pattern semantic analysis']


supervised probabilistic principal component analysis principal component analysis ( pca ) has been extensively applied in data mining , pattern recognition and information retrieval for unsupervised dimensionality reduction . when labels of data are available , e.g. , in a classification or regression task , pca is however not able to use this information . the problem is more interesting if only part of the input data are labeled , i.e. , in a semi-supervised setting . in this paper we propose a supervised pca model called sppca and a semi-supervised pca model called s2ppca , both of which are extensions of a probabilistic pca model . the proposed models are able to incorporate the label information into the projection phase , and can naturally handle multiple outputs ( i.e. , in multi-task learning problems ) . we derive an efficient em learning algorithm for both models , and also provide theoretical justifications of the model behaviors . sppca and s2ppca are compared with other supervised projection methods on various learning tasks , and show not only promising performance but also good scalability .

['dimensionality reduction', 'principal component analysis', 'semi-supervised projection', 'supervised projection']


the data mining approach to automated software testing in today 's industry , the design of software tests is mostly based on the testers ' expertise , while test automation tools are limited to execution of pre-planned tests only . evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification . not surprisingly , this manual approach to software testing results in heavy losses to the world 's economy . the costs of the so-called `` catastrophic '' software failures ( such as mars polar lander shutdown in 1999 ) are even hard to measure . in this paper , we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data . the induced data mining models of tested software can be utilized for recovering missing and incomplete specifications , designing a minimal set of regression tests , and evaluating the correctness of software outputs when testing new , potentially flawed releases of the system . to study the feasibility of the proposed approach , we have applied a novel data mining algorithm called info-fuzzy network ( ifn ) to execution data of a general-purpose code for solving partial differential equations . after being trained on a relatively small number of randomly generated input-output examples , the model constructed by the ifn algorithm has shown a clear capability to discriminate between correct and faulty versions of the program .

['automated software testing', 'finite element solver', 'info-fuzzy networks', 'input-output analysis', 'regression testing', 'testing tools']


deriving quantitative models for correlation clusters correlation clustering aims at grouping the data set into correlation clusters such that the objects in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality . several algorithms for this task have been proposed recently . however , all algorithms only compute the partitioning of the data into clusters . this is only a first step in the pipeline of advanced data analysis and system modelling . the second ( post-clustering ) step of deriving a quantitative model for each correlation cluster has not been addressed so far . in this paper , we describe an original approach to handle this second step . we introduce a general method that can extract quantitative information on the linear dependencies within a correlation clustering . our concepts are independent of the clustering model and can thus be applied as a post-processing step to any correlation clustering algorithm . furthermore , we show how these quantitative models can be used to predict the probability distribution that an object is created by these models . our broad experimental evaluation demonstrates the beneficial impact of our method on several applications of significant practical importance .

['cluster description', 'cluster model', 'clustering', 'correlation clustering', 'data mining']

clustering ADJ amod step
clustering NOUN pobj within
clustering NOUN amod model
clustering NOUN acl correlation

v-miner : using enhanced parallel coordinates to mine product design and test data analyzing data to find trends , correlations , and stable patterns is an important task in many industrial applications . this paper proposes a new technique based on parallel coordinate visualization . previous work on parallel coordinate methods has shown that they are effective only when variables that are correlated and\/or show similar patterns are displayed adjacently . although current parallel coordinate tools allow the user to manually rearrange the order of variables , this process is very time-consuming when the number of variables is large . automated assistance is required . this paper introduces an edit-distance based technique to rearrange variables so that interesting change patterns can be easily detected visually . the visual miner ( v-miner ) software includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined or displayed by the system . in addition , the system can filter data according to rules sets imported from other data mining tools . this feature was found very helpful in practice , because it enables decision makers to visually identify interesting rules and data segments for further analysis or data mining . this paper begins with an introduction to the proposed techniques and the v-miner system . next , a case study illustrates how v-miner has been used at motorola to guide product design and test decisions .

['change patterns', 'parallel coordinate visualization', 'rules']

rules NOUN compound sets
rules NOUN dobj identify

critical event prediction for proactive management in large-scale computer clusters as the complexity of distributed computing systems increases , systems management tasks require significantly higher levels of automation ; examples include diagnosis and prediction based on real-time streams of computer events , setting alarms , and performing continuous monitoring . the core of autonomic computing , a recently proposed initiative towards next-generation it-systems capable of ` self-healing ' , is the ability to analyze data in real-time and to predict potential problems . the goal is to avoid catastrophic failures through prompt execution of remedial actions . this paper describes an attempt to build a proactive prediction and control system for large clusters . we collected event logs containing various system reliability , availability and serviceability ( ras ) events , and system activity reports ( sars ) from a 350-node cluster system for a period of one year . the ` raw ' system health measurements contain a great deal of redundant event data , which is either repetitive in nature or misaligned with respect to time . we applied a filtering technique and modeled the data into a set of primary and derived variables . these variables used probabilistic networks for establishing event correlations through prediction algorithms . we also evaluated the role of time-series methods , rule-based classification algorithms and bayesian network models in event prediction . based on historical data , our results suggest that it is feasible to predict system performance parameters ( sars ) with a high degree of accuracy using time-series models . rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70 % accuracy .

['critical event prediction', 'database applications', 'large-scale clusters', 'learning', 'system event log']


computer aided detection via asymmetric cascade of sparse hyperplane classifiers this paper describes a novel classification method for computer aided detection ( cad ) that identifies structures of interest from medical images . cad problems are challenging largely due to the following three characteristics . typical cad training data sets are large and extremely unbalanced between positive and negative classes . when searching for descriptive features , researchers often deploy a large set of experimental features , which consequently introduces irrelevant and redundant features . finally , a cad system has to satisfy stringent real-time requirements . this work is distinguished by three key contributions . the first is a cascade classification approach which is able to tackle all the above difficulties in a unified framework by employing an asymmetric cascade of sparse classifiers each trained to achieve high detection sensitivity and satisfactory false positive rates . the second is the incorporation of feature computational costs in a linear program formulation that allows the feature selection process to take into account different evaluation costs of various features . the third is a boosting algorithm derived from column generation optimization to effectively solve the proposed cascade linear programs . we apply the proposed approach to the problem of detecting lung nodules from helical multi-slice ct images . our approach demonstrates superior performance in comparison against support vector machines , linear discriminant analysis and cascade adaboost . especially , the resulting detection system is significantly sped up with our approach .

['cascading classification', 'computer aided detection', 'mathematical programming', 'miscellaneous', 'sparse solutions', 'support vector machines']


clustering pair-wise dissimilarity data into partially ordered sets ontologies represent data relationships as hierarchies of possibly overlapping classes . ontologies are closely related to clustering hierarchies , and in this article we explore this relationship in depth . in particular , we examine the space of ontologies that can be generated by pairwise dissimilarity matrices . we demonstrate that classical clustering algorithms , which take dissimilarity matrices as inputs , do not incorporate all available information . in fact , only special types of dissimilarity matrices can be exactly preserved by previous clustering methods . we model ontologies as a partially ordered set ( poset ) over the subset relation . in this paper , we propose a new clustering algorithm , that generates a partially ordered set of clusters from a dissimilarity matrix .

['clustering', 'dissimilarity', 'pocluster', 'poset']

clustering VERB csubj represent
dissimilarity NOUN compound data
clustering NOUN pcomp to
dissimilarity NOUN compound matrices
clustering NOUN compound algorithms
dissimilarity NOUN compound matrices
dissimilarity NOUN compound matrices
clustering NOUN compound methods
poset NOUN appos set
clustering NOUN compound algorithm
dissimilarity NOUN compound matrix

extracting key-substring-group features for text classification in many text classification applications , it is appealing to take every document as a string of characters rather than a bag of words . previous research studies in this area mostly focused on different variants of generative markov chain models . although discriminative machine learning methods like support vector machine ( svm ) have been quite successful in text classification with word features , it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features . in this paper , we propose to partition all substrings into statistical equivalence groups , and then pick those groups which are important ( in the statistical sense ) as features ( named key-substring-group features ) for text classification . in particular , we propose a suffix tree based algorithm that can extract such features in linear time ( with respect to the total number of characters in the corpus ) . our experiments on english , chinese and greek datasets show that svm with key-substring-group features can achieve outstanding performance for various text classification tasks .

['content analysis and indexing', 'feature extraction', 'machine learning', 'suffix tree', 'text classification', 'text mining']


formulating distance functions via the kernel trick tasks of data mining and information retrieval depend on a good distance function for measuring similarity between data instances . the most effective distance function must be formulated in a context-dependent ( also application - , data - , and user-dependent ) way . in this paper , we propose to learn a distance function by capturing the nonlinear relationships among contextual information provided by the application , data , or user . we show that through a process called the `` kernel trick , '' such nonlinear relationships can be learned efficiently in a projected space . theoretically , we substantiate that our method is both sound and optimal . empirically , using several datasets and applications , we demonstrate that our method is effective and useful .

['distance function', 'information storage and retrieval', 'kernel trick']


efficient mining of iterative patterns for software specification discovery studies have shown that program comprehension takes up to 45 % of software development costs . such high costs are caused by the lack-of documented specification and further aggravated by the phenomenon of software evolution . there is a need for automated tools to extract specifications to aid program comprehension . in this paper , a novel technique to efficiently mine common software temporal patterns from traces is proposed . these patterns shed light on program behaviors , and are termed iterative patterns . they capture unique characteristic of software traces , typically not found in arbitrary sequences . specifically , due to loops , interesting iterative patterns can occur multiple times within a trace . furthermore , an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length . since a program behavior can be manifested in numerous ways , analyzing a single trace will not be sufficient . iterative pattern mining extends sequential pattern and episode minings to discover frequent iterative patterns which occur repetitively both within a program trace and across multiple traces . in this paper , we present cliper ( closed iterative pattern miner ) to efficiently mine a closed set of iterative patterns . a performance study on several simulated and real datasets shows the efficiency of our mining algorithm and effectiveness of our pruning strategy . our case study on jboss application server confirms the usefulness of mined patterns in discovering interesting software behavioral specification .

['closed iterative patterns', 'software specification discovery']


fast window correlations over uncooperative time series data arriving in time order ( a data stream ) arises in fields including physics , finance , medicine , and music , to name a few . often the data comes from sensors ( in physics and medicine for example ) whose data rates continue to improve dramatically as sensor technology improves . further , the number of sensors is increasing , so correlating data between sensors becomes ever more critical in order to distill knowlege from the data . in many applications such as finance , recent correlations are of far more interest than long-term correlation , so correlation over sliding windows ( windowed correlation ) is the desired operation . fast response is desirable in many applications ( e.g. , to aim a telescope at an activity of interest or to perform a stock trade ) . these three factors -- data size , windowed correlation , and fast response -- motivate this work . previous work ( 10 , 14 ) showed how to compute pearson correlation using fast fourier transforms and wavelet transforms , but such techniques do n't work for time series in which the energy is spread over many frequency components , thus resembling white noise . for such `` uncooperative '' time series , this paper shows how to combine several simple techniques -- sketches ( random projections ) , convolution , structured random vectors , grid structures , and combinatorial design -- to achieve high performance windowed pearson correlation over a variety of data sets .

['correlation', 'data structures', 'randomized algorithms', 'time series']

correlation NOUN pobj than
correlation NOUN nsubj is
correlation NOUN appos windows
correlation NOUN dobj windowed
correlation NOUN dobj compute
correlation NOUN dobj achieve

utility-based anonymization using local recoding privacy becomes a more and more serious concern in applications involving microdata . recently , efficient anonymization has attracted much research work . most of the previous methods use global recoding , which maps the domains of the quasi-identifier attributes to generalized or changed values . however , global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data . moreover , anonymized data is often for analysis . as well accepted in many analytical applications , different attributes in a data set may have different utility in the analysis . the utility of attributes has not been considered in the previous methods . in this paper , we study the problem of utility-based anonymization . first , we propose a simple framework to specify utility of attributes . the framework covers both numeric and categorical data . second , we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization . our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy . furthermore , our utility-based method can boost the quality of analysis using the anonymized data .

['k-anonymity', 'local recoding', 'privacy preservation', 'utility']

utility NOUN dobj have
utility NOUN nsubjpass considered
utility NOUN npadvmod based
utility NOUN dobj specify
utility NOUN npadvmod based
utility NOUN npadvmod based

integration of profile hidden markov model output into association rule mining scientific models typically depend on parameters . preserving the parameter dependence of models in the pattern mining context opens up several applications . within association rule mining ( arm ) , the choice of parameters can be studied with more flexibly then in traditional model building . studying support , confidence , and other rule metrics as a function of model parameters allows conclusions on assumptions underlying the models . we present efficient techniques to handle multiple model output data sets at little more than the cost of one . we integrate output from hidden markov models into the association rule mining framework , demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation .

['association rule mining', 'model mining', 'profile hidden markov model']


frequent subgraph mining in outerplanar graphs in recent years there has been an increased interest in algorithms that can perform frequent pattern discovery in large databases of graph structured objects . while the frequent connected subgraph mining problem for tree datasets can be solved in incremental polynomial time , it becomes intractable for arbitrary graph databases . existing approaches have therefore resorted to various heuristic strategies and restrictions of the search space , but have not identified a practically relevant tractable graph class beyond trees . in this paper , we define the class of so called tenuous outerplanar graphs , a strict generalization of trees , develop a frequent subgraph mining algorithm for tenuous outerplanar graphs that works in incremental polynomial time , and evaluate the algorithm empirically on the nci molecular graph dataset .

['computational chemistry', 'frequent pattern discovery', 'graph mining']


a multiple tree algorithm for the efficient association of asteroid observations in this paper we examine the problem of efficiently finding sets of observations that conform to a given underlying motion model . while this problem is often phrased as a tracking problem , where it is called track initiation , it is useful in a variety of tasks where we want to find correspondences or patterns in spatial-temporal data . unfortunately , this problem often suffers from a combinatorial explosion in the number of potential sets that must be evaluated . we consider the problem with respect to large-scale asteroid observation data , where the goal is to find associations among the observations that correspond to the same underlying asteroid . in this domain , it is vital that we can efficiently extract the underlying associations . we introduce a new methodology for track initiation that exhaustively considers all possible linkages . we then introduce an exact tree-based algorithm for tractably finding all compatible sets of points . further , we extend this approach to use multiple trees , exploiting structure from several time steps at once . we compare this approach to a standard sequential approach and show how the use of multiple trees can provide a significant benefit .

['multiple tree algorithms', 'track initiation']


temporal causal modeling with graphical granger methods the need for mining causality , beyond mere statistical correlations , for real world problems has been recognized widely . many of these applications naturally involve temporal data , which raises the challenge of how best to leverage the temporal information for causal modeling . recently graphical modeling with the concept of `` granger causality '' , based on the intuition that a cause helps predict its effects in the future , has gained attention in many domains involving time series data analysis . with the surge of interest in model selection methodologies for regression , such as the lasso , as practical alternatives to solving structural learning of graphical models , the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling . in this paper , we examine a host of related algorithms that , loosely speaking , fall under the category of graphical granger methods , and characterize their relative performance from multiple viewpoints . our experiments show , for instance , that the lasso algorithm exhibits consistent gain over the canonical pairwise graphical granger method . we also characterize conditions under which these variants of graphical granger methods perform well in comparison to other benchmark methods . finally , we apply these methods to a real world data set involving key performance indicators of corporations , and present some concrete results .

['causal modeling', 'graphical models', 'time series data']


online novelty detection on temporal sequences in this paper , we present a new framework for online novelty detection on temporal sequences . this framework include a mechanism for associating each detection result with a confidence value . based on this framework , we develop a concrete online detection algorithm , by modeling the temporal sequence using an online support vector regression algorithm . experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm .

['anomaly detection', 'design methodology', 'novelty detection', 'online algorithm', 'support vector regression']


graph-based anomaly detection anomaly detection is an area that has received much attention in recent years . it has a wide variety of applications , including fraud detection and network intrusion detection . a good deal of research has been performed in this area , often using strings or attribute-value data as the medium from which anomalies are to be extracted . little work , however , has focused on anomaly detection in graph-based data . in this paper , we introduce two techniques for graph-based anomaly detection . in addition , we introduce a new method for calculating the regularity of a graph , with applications to anomaly detection . we hypothesize that these methods will prove useful both for finding anomalies , and for determining the likelihood of successful anomaly detection within graph-based data . we provide experimental results using both real-world network intrusion data and artificially-created data .

['anomaly detection', 'graph regularity']


mining , indexing , and querying historical spatiotemporal data in many applications that track and analyze spatiotemporal data , movements obey periodic patterns ; the objects follow the same routes ( approximately ) over regular time intervals . for example , people wake up at the same time and follow more or less the same route to their work everyday . the discovery of hidden periodic patterns in spatiotemporal data , apart from unveiling important information to the data analyst , can facilitate data management substantially . based on this observation , we propose a framework that analyzes , manages , and queries object movements that follow such patterns . we define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic patterns . we also devise a novel , specialized index structure that can benefit from the discovered patterns to support more efficient execution of spatiotemporal queries . we evaluate our methods experimentally using datasets with object trajectories that exhibit periodicity .

['indexing', 'pattern mining', 'spatiotemporal data', 'trajectories']

trajectories NOUN pobj with

mining optimal decision trees from itemset lattices we present dl8 , an exact algorithm for finding a decision tree that optimizes a ranking function under size , depth , accuracy and leaf constraints . because the discovery of optimal trees has high theoretical complexity , until now few efforts have been made to compute such trees for real-world datasets . an exact algorithm is of both scientific and practical interest . from a scientific point of view , it can be used as a gold standard to evaluate the performance of heuristic constraint-based decision tree learners and to gain new insight in traditional decision tree learners . from the application point of view , it can be used to discover trees that can not be found by heuristic decision tree learners . the key idea behind our algorithm is that there is a relation between constraints on decision trees and constraints on itemsets . we show that optimal decision trees can be extracted from lattices of itemsets in linear time . we give several strategies to efficiently build these lattices . experiments show that under the same constraints , dl8 obtains better results than c4 .5 , which confirms that exhaustive search does not always imply overfitting . the results also show that dl8 is a useful and interesting tool to learn decision trees under constraints .

['constraint-based mining', 'decision trees', 'formal concepts', 'frequent itemsets']


transforming classifier scores into accurate multiclass probability estimates class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making , such as example-dependent misclassification costs , the outputs of other classifiers , or domain knowledge . previous calibration methods apply only to two-class problems . here , we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates . we also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples . using naive bayes and support vector machine classifiers , we give experimental results from a variety of two-class and multiclass domains , including direct marketing , text categorization and digit recognition .

['probabilistic algorithms']


mining reference tables for automatic text segmentation automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legacy sources and text collections into a data warehouse for subsequent querying , analysis , mining and integration . in this paper , we mine tables present in data warehouses and relational databases to develop an automatic segmentation system . thus , we overcome limitations of existing supervised text segmentation approaches , which require comprehensive manually labeled training data . our segmentation system is robust , accurate , and efficient , and requires no additional manual effort . thorough evaluation on real datasets demonstrates the robustness and accuracy of our system , with segmentation accuracy exceeding state of the art supervised approaches .

['data cleaning', 'information extraction', 'learning', 'machine learning', 'text management', 'text segmentation']


mining rank-correlated sets of numerical attributes we study the mining of interesting patterns in the presence of numerical attributes . instead of the usual discretization methods , we propose the use of rank based measures to score the similarity of sets of numerical attributes . new support measures for numerical data are introduced , based on extensions of kendall 's tau , and spearman 's footrule and rho . we show how these support measures are related . furthermore , we introduce a novel type of pattern combining numerical and categorical attributes . we give efficient algorithms to find all frequent patterns for the proposed support measures , and evaluate their performance on real-life datasets .

['data mining', 'numerical', 'rank correlation', 'systems']

numerical ADJ amod attributes
numerical ADJ amod attributes
numerical ADJ amod attributes
numerical ADJ amod data
numerical ADJ amod attributes

improved robustness of signature-based near-replica detection via lexicon randomization detection of near duplicate documents is an important problem in many data mining and information filtering applications . when faced with massive quantities of data , traditional duplicate detection techniques relying on direct inter-document similarity computation ( e.g. , using the cosine measure ) are often not feasible given the time and memory performance constraints . on the other hand , fingerprint-based methods , such as i-match , are very attractive computationally but may be brittle with respect to small changes to document content . we focus on approaches to near-replica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization . in experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional i-match , with the relative improvement in duplicate-document recall reaching as high as 40-60 % . the large gains in detection accuracy are offset by only small increases in computational requirements .

['data cleaning', 'deduplication', 'spam filtering', 'web mining']


acclimatizing taxonomic semantics for hierarchical content classification hierarchical models have been shown to be effective in content classification . however , we observe through empirical study that the performance of a hierarchical model varies with given taxonomies ; even a semantically sound taxonomy has potential to change its structure for better classification . by scrutinizing typical cases , we elucidate why a given semantics-based hierarchy does not work well in content classification , and how it could be improved for accurate hierarchical classification . with these understandings , we propose effective localized solutions that modify the given taxonomy for accurate hierarchical classification . we conduct extensive experiments on both toy and real-world data sets , report improved performance and interesting findings , and provide further analysis of algorithmic issues such as time complexity , robustness , and sensitivity to the number of features .

['hierarchical classification', 'hierarchical modeling', 'taxonomy adjustment', 'text classification']


sewep : using site semantics and a taxonomy to enhance the web personalization process web personalization is the process of customizing a web site to the needs of each specific user or set of users , taking advantage of the knowledge acquired through the analysis of the user 's navigational behavior . integrating usage data with content , structure or user profile data enhances the results of the personalization process . in this paper , we present sewep , a system that makes use of both the usage logs and the semantics of a web site 's content in order to personalize it . web content is semantically annotated using a conceptual hierarchy ( taxonomy ) . we introduce c-logs , an extended form of web usage logs that encapsulates knowledge derived from the link semantics . c-logs are used as input to the web usage mining process , resulting in a broader yet semantically focused set of recommendations .

['concept hierarchies', 'database applications', 'on-line information services', 'semantic annotation of web content', 'web mining', 'web personalization']


fast discovery of unexpected patterns in data , relative to a bayesian network we consider a model in which background knowledge on a given domain of interest is available in terms of a bayesian network , in addition to a large database . the mining problem is to discover unexpected patterns : our goal is to find the strongest discrepancies between network and database . this problem is intrinsically difficult because it requires inference in a bayesian network and processing the entire , potentially very large , database . a sampling-based method that we introduce is efficient and yet provably finds the approximately most interesting unexpected patterns . we give a rigorous proof of the method 's correctness . experiments shed light on its efficiency and practicality for large-scale bayesian networks and databases .

['association rules', 'bayesian networks', 'sampling']

sampling NOUN npadvmod based

structural and temporal analysis of the blogosphere through community factorization the blogosphere has unique structural and temporal properties since blogs are typically used as communication media among human individuals . in this paper , we propose a novel technique that captures the structure and temporal dynamics of blog communities . in our framework , a community is a set of blogs that communicate with each other triggered by some events ( such as a news article ) . the community is represented by its structure and temporal dynamics : a community graph indicates how often one blog communicates with another , and a community intensity indicates the activity level of the community that varies over time . our method , community factorization , extracts such communities from the blogosphere , where the communication among blogs is observed as a set of subgraphs ( i.e. , threads of discussion ) . this community extraction is formulated as a factorization problem in the framework of constrained optimization , in which the objective is to best explain the observed interactions in the blogosphere over time . we further provide a scalable algorithm for computing solutions to the constrained optimization problems . extensive experimental studies on both synthetic and real blog data demonstrate that our technique is able to discover meaningful communities that are not detectable by traditional methods .

['blog', 'blogosphere', 'community factorization', 'iterative search', 'non-negative matrix factorization', 'regularization']

blogosphere NOUN pobj of
blogosphere NOUN nsubj has
blog NOUN compound communities
blog NOUN compound communicates
blogosphere NOUN pobj from
blogosphere NOUN pobj in
blog NOUN compound data

naã¯ve filterbots for robust cold-start recommendations the goal of a recommender system is to suggest items of interest to a user based on historical behavior of a community of users . given detailed enough history , item-based collaborative filtering ( cf ) often performs as well or better than almost any other recommendation method . however , in cold-start situations - where a user , an item , or the entire system is new - simple non-personalized recommendations often fare better . we improve the scalability and performance of a previous approach to handling cold-start situations that uses filterbots , or surrogate users that rate items based only on user or item attributes . we show that introducing a very small number of simple filterbots helps make cf algorithms more robust . in particular , adding just seven global filterbots improves both user-based and item-based cf in cold-start user , cold-start item , and cold-start system settings . performance is better when data is scarce , performance is no worse when data is plentiful , and algorithm efficiency is negligibly affected . we systematically compare a non-personalized baseline , user-based cf , item-based cf , and our bot-augmented user - and item-based cf algorithms using three data sets ( yahoo ! movies , movielens , and eachmovie ) with the normalized mae metric in three types of cold-start situations . the advantage of our `` naã¯ve filterbot '' approach is most pronounced for the yahoo ! data , the sparsest of the three data sets .

['cold start', 'collaborative filtering', 'hybrid content and collaborative filtering', 'learning', 'naïve filterbots', 'performance analysis', 'recommender systems', 'robustness']


dimension induced clustering it is commonly assumed that high-dimensional datasets contain points most of which are located in low-dimensional manifolds . detection of low-dimensional clusters is an extremely useful task for performing operations such as clustering and classification , however , it is a challenging computational problem . in this paper we study the problem of finding subsets of points with low intrinsic dimensionality . our main contribution is to extend the definition of fractal correlation dimension , which measures average volume growth rate , in order to estimate the intrinsic dimensionality of the data in local neighborhoods . we provide a careful analysis of several key examples in order to demonstrate the properties of our measure . based on our proposed measure , we introduce a novel approach to discover clusters with low dimensionality . the resulting algorithms extend previous density based measures , which have been successfully used for clustering . we demonstrate the effectiveness of our algorithms for discovering low-dimensional m-flats embedded in high dimensional spaces , and for detecting low-rank sub-matrices .

['clustering', 'fractal dimension', 'miscellaneous']

clustering NOUN xcomp induced
clustering NOUN pobj as
clustering NOUN pobj for

scalable mining of large disk-based graph databases mining frequent structural patterns from graph databases is an interesting problem with broad applications . most of the previous studies focus on pruning unfruitful search subspaces effectively , but few of them address the mining on large , disk-based databases . as many graph databases in applications can not be held into main memory , scalable mining of large , disk-based graph databases remains a challenging problem . in this paper , we develop an effective index structure , adi ( for ( u ) ad ( \/ u ) jacency ( u ) i ( \/ u ) ndex ) , to support mining various graph patterns over large databases that can not be held into main memory . the index is simple and efficient to build . moreover , the new index structure can be easily adopted in various existing graph pattern mining algorithms . as an example , we adapt the well-known gspan algorithm by using the adi structure . the experimental results show that the new index structure enables the scalable graph pattern mining over large databases . in one set of the experiments , the new disk-based method can mine graph databases with one million graphs , while the original gspan algorithm can only handle databases of up to 300 thousand graphs . moreover , our new method is faster than gspan when both can run in main memory .

['frequent graph pattern', 'graph database', 'graph mining', 'index']

index NOUN compound structure
index NOUN nsubj is
index NOUN compound structure
index NOUN compound structure

towards systematic design of distance functions for data mining applications distance function computation is a key subtask in many data mining algorithms and applications . the most effective form of the distance function can only be expressed in the context of a particular data domain . it is also often a challenging and non-trivial task to find the most effective form of the distance function . for example , in the text domain , distance function design has been considered such an important and complex issue that it has been the focus of intensive research over three decades . the final design of distance functions in this domain has been reached only by detailed empirical testing and consensus over the quality of results provided by the different variations . with the increasing ability to collect data in an automated way , the number of new kinds of data continues to increase rapidly . this makes it increasingly difficult to undertake such efforts for each and every new data type . the most important aspect of distance function design is that since a human is the end-user for any application , the design must satisfy the user requirements with regard to effectiveness . this creates the need for a systematic framework to design distance functions which are sensitive to the particular characteristics of the data domain . in this paper , we discuss such a framework . the goal is to create distance functions in an automated waywhile minimizing the work required from the user . we will show that this framework creates distance functions which are significantly more effective than popularly used functions such as the euclidean metric .

['distance functions', 'user interaction']


exploiting dictionaries in named entity extraction : combining semi-markov extraction processes and data integration methods we consider the problem of improving named entity recognition ( ner ) systems by using external dictionaries -- more specifically , the problem of extending state-of-the-art ner systems by incorporating information about the similarity of extracted entities to entities in an external dictionary . this is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name ; however , the most useful similarity measures score entire candidate names . to correct this mismatch we formalize a semi-markov extraction process , which is based on sequentially classifying segments of several adjacent words , rather than single words . in addition to allowing a natural way of coupling high-performance ner methods and high-performance similarity functions , this formalism also allows the direct use of other useful entity-level features , and provides a more natural formulation of the ner problem than sequential word classification . experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in ner .

['data integration', 'information extraction', 'learning', 'learning', 'named entity recognition', 'sequential learning']


robust information-theoretic clustering how do we find a natural clustering of a real world point set , which contains an unknown number of clusters with different shapes , and which may be contaminated by noise ? most clustering algorithms were designed with certain assumptions ( gaussianity ) , they often require the user to give input parameters , and they are sensitive to noise . in this paper , we propose a robust framework for determining a natural clustering of a given data set , based on the minimum description length ( mdl ) principle . the proposed framework , robust information-theoretic clustering ( ric ) , is orthogonal to any known clustering algorithm : given a preliminary clustering , ric purifies these clusters from noise , and adjusts the clusterings such that it simultaneously determines the most natural amount and shape ( subspace ) of the clusters . our ric method can be combined with any clustering technique ranging from k-means and k-medoids to advanced methods such as spectral clustering . in fact , ric is even able to purify and improve an initial coarse clustering , even if we start with very simple methods such as grid-based space partitioning . moreover , ric scales well with the data set size . extensive experiments on synthetic and real world data sets validate the proposed ric framework .

['clustering', 'data summarization', 'noise-robustness', 'parameter-free data mining']

clustering NOUN ROOT clustering
clustering NOUN dobj find
clustering ADJ amod algorithms
clustering NOUN dobj determining
clustering NOUN conj framework
clustering NOUN compound algorithm
clustering NOUN pobj given
clustering NOUN compound technique
clustering NOUN pobj as
clustering NOUN dobj improve

cancer genomics throughout life , the cells in every individual accumulate many changes in the dna inherited from his or her parents . certain combinations of changes lead to cancer . during the last decade , the cost of dna sequencing has been dropping by a factor of 10 every two years , making it now possible to read most of the three billion base genome from a patient 's cancer tumor , and to try to determine all of the thousands of dna changes in it . under the auspices of nci 's cancer genome atlas project , 10,000 tumors will be sequenced in this manner in the next few years . soon cancer genome sequencing will be a widespread clinical practice , and millions of tumors will be sequenced . a massive computational problem looms in interpreting these data . first , because we can only read short pieces of dna , we have the enormous problem of assembling a coherent and reliable representation of the tumor genome from massive amounts of incomplete and error-prone evidence . this is the first challenge . second , every human genome is unique from birth , and every tumor a unique variant . there is no single route to cancer . we must learn to read the varied signatures of cancer within the tumor genome and associate these with optimal treatments . already there are hundreds of molecularly targeted treatments for cancer available , each known to be more or less effective depending on specific genetic variants . however , targeting a single gene with one treatment rarely works . the second challenge is to tackle the combinatorics of personalized , targeted , combination therapy in cancer .

['cancer']

cancer NOUN compound genomics
cancer NOUN pobj to
cancer NOUN compound tumor
cancer NOUN compound project
cancer NOUN nsubj be
cancer NOUN pobj to
cancer NOUN pobj of
cancer NOUN pobj for
cancer NOUN pobj in

generalized component analysis for text with heterogeneous attributes we present a class of richly structured , undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities . our model generalizes techniques such as principal component analysis to heterogeneous data types . in contrast to other approaches , this framework allows modalities such as words , authors and timestamps to be captured in their natural , probabilistic encodings . a latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method . we demonstrate the effectiveness of our framework on the task of author prediction from 13 years of the nips conference proceedings and for a recipient prediction task using a 10-month academic email archive of a researcher . our approach should be more broadly applicable to many real-world applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework .

['author prediction', 'learning', 'multimodal heterogeneous data', 'recipient prediction', 'text mining', 'topic modeling', 'undirected graphical models']


density-based clustering of uncertain data in many different application areas , e.g. sensor databases , location based services or face recognition systems , distances between odjects have to be computed based on vague and uncertain data . commonly , the distances between these uncertain object descriptions are expressed by one numerical distance value . based on such single-valued distance functions standard data mining algorithms can work without any changes . in this paper , we propose to express the similarity between two fuzzy objects by distance probability functions . these fuzzy distance functions assign a probability value to each possible distance value . by integrating these fuzzy distance functions directly into data mining algorithms , the full information provided by these functions is exploited . in order to demonstrate the benefits of this general approach , we enhance the density-based clustering algorithm dbscan so that it can work directly on these fuzzy distance functions . in a detailed experimental evaluation based on artificial and real-world data sets , we show the characteristics and benefits of our new approach .

['density-based clustering', 'fuzzy distance functions', 'probabilistic algorithms', 'uncertain data']


trajectory pattern mining the increasing pervasiveness of location-acquisition technologies ( gps , gsm networks , etc. ) is leading to the collection of large spatio-temporal datasets and to the opportunity of discovering usable knowledge about movement behavior , which fosters novel applications and services . in this paper , we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects . we introduce trajectory patterns as concise descriptions of frequent behaviors , in terms of both space ( i.e. , the regions of space visited during movements ) and time ( i.e. , the duration of movements ) . in this setting , we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity . the various approaches are then empirically evaluated over real data and synthetic benchmarks , comparing their strengths and weaknesses .

['spatio-temporal data mining', 'trajectory patterns']


a dea approach for model combination this paper proposes a novel data envelopment analysis ( dea ) based approach for model combination . we first prove that for the 2-class classification problems dea models identify the same convex hull as the popular roc analysis used for model combination . for general k-class classifiers , we then develop a dea-based method to combine multiple classifiers . experiments show that the method outperforms other benchmark methods and suggest that dea can be a promising tool for model combination .

['data envelopment analysis', 'learning', 'model combination', 'roc']

roc NOUN compound analysis

cost-effective outbreak detection in networks given a water distribution network , where should we place sensors toquickly detect contaminants ? or , which blogs should we read to avoid missing important stories ? . these seemingly different problems share common structure : outbreak detection can be modeled as selecting nodes ( sensor locations , blogs ) in a network , in order to detect the spreading of a virus or information asquickly as possible . we present a general methodology for near optimal sensor placement in these and related problems . we demonstrate that many realistic outbreak detection objectives ( e.g. , detection likelihood , population affected ) exhibit the property of `` submodularity '' . we exploit submodularity to develop an efficient algorithm that scales to large problems , achieving near optimal placements , while being 700 times faster than a simple greedy algorithm . we also derive online bounds on the quality of the placements obtained by any algorithm . our algorithms and bounds also handle cases where nodes ( sensor locations , blogs ) have different costs . we evaluate our approach on several large real-world problems , including a model of a water distribution network from the epa , andreal blog data . the obtained sensor placements are provably near optimal , providing a constant fraction of the optimal solution . we show that the approach scales , achieving speedups and savings in storage of several orders of magnitude . we also show how the approach leads to deeper insights in both applications , answering multicriteria trade-off , cost-sensitivity and generalization questions .

['graphs', 'information cascades', 'sensor placement', 'submodular functions', 'virus propagation']


weighted association rule mining using weighted support and significance framework we address the issues of discovering significant binary relationships in transaction datasets in a weighted setting . traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight . the goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatornal explosion of insignificant relationships . we identify the challenge of using weights in the iterative process of generating large itemsets . the problem of invalidation of the `` downward closure property '' in the weighted setting is solved by using an improved model of weighted support measurements and exploiting a `` weighted downward closure property '' . a new algorithm called warm ( weighted association rule mining ) is developed based on the improved model . the algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on simulated datasets .

['significant relationship', 'warm algorithm', 'weighted association rule mining', 'weighted downward closure property', 'weighted support']


enhanced max margin learning on multimodal data mining in a multimedia database the problem of multimodal data mining in a multimedia database can be addressed as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables . in this paper , built upon the existing literature on the max margin based learning , we develop a new max margin learning approach called enhanced max margin learning ( emml ) framework . in addition , we apply emml framework to developing an effective and efficient solution to the multimodal data mining problem in a multimedia database . the main contributions include : ( 1 ) we have developed a new max margin learning approach - the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate , which is verified in empirical evaluations ; ( 2 ) we have applied this emml approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale , allowing facilitating a multimodal data mining querying to a very large scale multimedia database , and excelling many existing multimodal data mining methods in the literature that do not scale up at all ; this advantage is also supported through the complexity analysis as well as empirical evaluations against a state-of-the-art multimodal data mining method from the literature . while emml is a general framework , for the evaluation purpose , we apply it to the berkeley drosophila embryo image database , and report the performance comparison with a state-of-the-art multimodal data mining method .

['image annotation', 'image retrieval', 'max margin', 'multimodal data mining']


global distance-based segmentation of trajectories this work introduces distance-based criteria for segmentation of object trajectories . segmentation leads to simplification of the original objects into smaller , less complex primitives that are better suited for storage and retrieval purposes . previous work on trajectory segmentation attacked the problem locally , segmenting separately each trajectory of the database . therefore , they did not directly optimize the inter-object separability , which is necessary for mining operations such as searching , clustering , and classification on large databases . in this paper we analyze the trajectory segmentation problem from a global perspective , utilizing data aware distance-based optimization techniques , which optimize pairwise distance estimates hence leading to more efficient object pruning . we first derive exact solutions of the distance-based formulation . due to the intractable complexity of the exact solution , we present anapproximate , greedy solution that exploits forward searching of locally optimal solutions . since the greedy solution also imposes a prohibitive computational cost , we also put forward more light weight variance-based segmentation techniques , which intelligently `` relax '' the pairwise distance only in the areas that affect the least the mining operation .

['data simplification', 'dna visualization']


spin : mining maximal frequent subgraphs from graph databases one fundamental challenge for mining recurring subgraphs from semi-structured data sets is the overwhelming abundance of such patterns . in large graph databases , the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resources . in this paper , we propose a new algorithm that mines only maximal frequent subgraphs , i.e. subgraphs that are not a part of any other frequent subgraphs . this may exponentially decrease the size of the output set in the best case ; in our experiments on practical data sets , mining maximal frequent subgraphs reduces the total number of mined patterns by two to three orders of magnitude . our method first mines all frequent trees from a general graph database and then reconstructs all maximal subgraphs from the mined trees . using two chemical structure benchmarks and a set of synthetic graph data sets , we demonstrate that , in addition to decreasing the output size , our algorithm can achieve a five-fold speed up over the current state-of-the-art subgraph mining algorithms .

['spanning tree', 'subgraph mining']


the complexity of mining maximal frequent itemsets and maximal frequent patterns mining maximal frequent itemsets is one of the most fundamental problems in data mining . in this paper we study the complexity-theoretic aspects of maximal frequent itemset mining , from the perspective of counting the number of solutions . we present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions , given an arbitrary support threshold , is #p - complete , thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is np-hard . this result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in p. we also extend our complexity analysis to other similar data mining problems dealing with complex data structures , such as sequences , trees , and graphs , which have attracted intensive research interests in recent years . normally , in these problems a partial order among frequent patterns can be defined in such a way as to preserve the downward closure property , with maximal frequent patterns being those without any successor with respect to this partial order . we investigate several variants of these mining problems in which the patterns of interest are subsequences , subtrees , or subgraphs , and show that the associated problems of counting the number of maximal frequent patterns are all either #p - complete or #p - hard .

['complexity', 'data mining', 'maximal frequent itemset', 'maximal frequent pattern', 'nonnumerical algorithms and problems']

complexity NOUN nsubj is
complexity NOUN compound theoretic
complexity NOUN compound analysis

pattern lattice traversal by selective jumps regardless of the frequent patterns to discover , either the full frequent patterns or the condensed ones , either closed or maximal , the strategy always includes the traversal of the lattice of candidate patterns . we study the existing depth versus breadth traversal approaches for generating candidate patterns and propose in this paper a new traversal approach that jumps in the search space among only promising nodes . our leaping approach avoids nodes that would not participate in the answer set and reduce drastically the number of candidate patterns . we use this approach to efficiently pinpoint maximal patterns at the border of the frequent patterns in the lattice and collect enough information in the process to generate all subsequent patterns .

['nonnumerical algorithms and problems']


mining correlated bursty topic patterns from coordinated text streams previous work on text mining has almost exclusively focused on a single stream . however , we often have available multiple text streams indexed by the same set of time points ( called coordinated text streams ) , which offer new opportunities for text mining . for example , when a major event happens , all the news articles published by different agencies in different languages tend to cover the same event for a certain period , exhibiting a correlated bursty topic pattern in all the news article streams . in general , mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams . in this paper , we define and study this novel text mining problem . we propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies ( e.g. , english vs chinese ) . evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets : the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles ( in english and chinese , respectively ) , while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research . since the proposed method is general and does not require the streams to share vocabulary , it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period .

['coordinated streams', 'correlated bursty patterns', 'reinforcement']


finding tribes : identifying close-knit individuals from employment patterns we present a family of algorithms to uncover tribes-groups of individuals who share unusual sequences of affiliations . while much work inferring community structure describes large-scale trends , we instead search for small groups of tightly linked individuals who behave anomalously with respect to those trends . we apply the algorithms to a large temporal and relational data set consisting of millions of employment records from the national association of securities dealers . the resulting tribes contain individuals at higher risk for fraud , are homogenous with respect to risk scores , and are geographically mobile , all at significant levels compared to random or to other sets of individuals who share affiliations .

['anomaly detection', 'dynamic networks', 'social networks']


a framework for analysis of dynamic social networks finding patterns of social interaction within a population has wide-ranging applications including : disease modeling , cultural and information transmission , and behavioral ecology . social interactions are often modeled with networks . a key characteristic of social interactions is their continual change . however , most past analyses of social networks are essentially static in that all information about the time that social interactions take place is discarded . in this paper , we propose a new mathematical and computational framework that enables analysis of dynamic social networks and that explicitly makes use of information about when social interactions occur .

['disease spread', 'dynamic social networks', 'model development']


efficient anonymity-preserving data collection the output of a data mining algorithm is only as good as its inputs , and individuals are often unwilling to provide accurate data about sensitive topics such as medical history and personal finance . individuals maybe willing to share their data , but only if they are assured that it will be used in an aggregate study and that it can not be linked back to them . protocols for anonymity-preserving data collection provide this assurance , in the absence of trusted parties , by allowing a set of mutually distrustful respondents to anonymously contribute data to an untrusted data miner . to effectively provide anonymity , a data collection protocol must be collusion resistant , which means that even if all dishonest respondents collude with a dishonest data miner in an attempt to learn the associations between honest respondents and their responses , they will be unable to do so . to achieve collusion resistance , previously proposed protocols for anonymity-preserving data collection have quadratically many communication rounds in the number of respondents , and employ ( sometimes incorrectly ) complicated cryptographic techniques such as zero-knowledge proofs . we describe a new protocol for anonymity-preserving , collusion resistant data collection . our protocol has linearly many communication rounds , and achieves collusion resistance without relying on zero-knowledge proofs . this makes it especially suitable for data mining scenarios with a large number of respondents .

['anonymity', 'privacy']

anonymity NOUN npadvmod preserving
anonymity NOUN npadvmod preserving
anonymity NOUN dobj provide
anonymity NOUN npadvmod preserving
anonymity NOUN npadvmod preserving

local sparsity control for naive bayes with extreme misclassification costs in applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable . this limits the utility of a classifier to a range in which such constraints can be met . naive bayes , which has proven to be very useful in text mining applications due to high scalability , can be particularly affected . although its 0\/1 loss tends to be small , its misclassifications are often made with apparently high confidence . aside from efforts to better calibrate naive bayes scores , it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance . traditionally , sparsity is controlled globally , and the result for any particular document may vary . in this work we examine the merits of local sparsity control for naive bayes in the context of highly asymmetric misclassification costs . in experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection . in the extreme cost setting , multinomial naive bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the naive bayes classifier . there are also indications that local feature selection may be preferable in different cost settings .

['feature selection', 'high recall classification', 'naive bayes', 'text categorization']


discovering additive structure in black box functions many automated learning procedures lack interpretability , operating effectively as a black box : providing a prediction tool but no explanation of the underlying dynamics that drive it . a common approach to interpretation is to plot the dependence of a learned function on one or two predictors . we present a method that seeks not to display the behavior of a function , but to evaluate the importance of non-additive interactions within any set of variables . should the function be close to a sum of low dimensional components , these components can be viewed and even modeled parametrically . alternatively , the work here provides an indication of where intrinsically high-dimensional behavior takes place . the calculations used in this paper correspond closely with the functional anova decomposition ; a well-developed construction in statistics . in particular , the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models . the algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes .

['additive models', 'diagnostics', 'draphical models', 'feature selection', 'functional anova', 'interpretation', 'learning', 'visualization']

learning VERB compound procedures
interpretation NOUN pobj to
interpretation NOUN compound purposes

a refinement approach to handling model misfit in text categorization text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents . this problem has been studied in information retrieval , machine learning and data mining . so far , many effective techniques have been proposed . however , most techniques are based on some underlying models and\/or assumptions . when the data fits the model well , the classification accuracy will be high . however , when the data does not fit the model well , the classification accuracy can be very low . in this paper , we propose a refinement approach to dealing with this problem of model misfit . we show that we do not need to change the classification technique itself ( or its underlying model ) to make it more flexible . instead , we propose to use successive refinements of classification on the training data to correct the model misfit . we apply the proposed technique to improve the classification performance of two simple and efficient text classifiers , the rocchio classifier and the naã¯ve bayesian classifier . these techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier . extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically . in particular , our refined model is able to improve the naã¯ve bayesian or rocchio classifier 's prediction performance by 45 % on average .

['naïve bayesian classifier', 'probabilistic algorithms', 'rocchio algorithm', 'text categorization']


practical guide to controlled experiments on the web : listen to your customers not to the hippo the web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments , also called randomized experiments ( single factor or factorial designs ) , a\/b tests ( and their generalizations ) , split tests , control\/treatment tests , and parallel flights . controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior . we provide a practical guide to conducting online experiments , where end-users can help guide the development of features . our experience indicates that significant learning and return-on-investment ( roi ) are seen when development teams listen to their customers , not to the highest paid person 's opinion ( hippo ) . we provide several examples of controlled experiments with surprising results . we review the important ingredients of running controlled experiments , and discuss their limitations ( both technical and organizational ) . we focus on several areas that are critical to experimentation , including statistical power , sample size , and techniques for variance reduction . we describe common architectures for experimentation systems and analyze their advantages and disadvantages . we evaluate randomization and hashing techniques , which we show are not as simple in practice as is often assumed . controlled experiments typically generate large amounts of data , which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest , leading to new hypotheses and creating a virtuous cycle of improvements . organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses . based on our extensive practical experience with multiple systems and organizations , we share key lessons that will help practitioners in running trustworthy controlled experiments .

['a/b testing', 'controlled experiments', 'e-commerce', 'learning']

learning NOUN nsubjpass seen

time and sample efficient discovery of markov blankets and direct causal relations data mining with bayesian network learning has two important characteristics : under conditions learned edges between variables correspond to casual influences , and second , for every variable t in the network a special subset ( markov blanket ) identifiable by the network is the minimal variable set required to predict t. however , all known algorithms learning a complete bn do not scale up beyond a few hundred variables . on the other hand , all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region . the contribution of this paper is two-fold . we introduce a novel local algorithm that returns all variables with direct edges to and from a target variable t as well as a local algorithm that returns the markov blanket of t. both algorithms ( i ) are sound , ( ii ) can be run efficiently in datasets with thousands of variables , and ( iii ) significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods . a fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region ; this yields up to exponential savings in sample relative to previously known algorithms . the results presented here are promising not only for discovery of local causal structure , and variable selection for classification , but also for the induction of complete bns .

['bayesian networks', 'learning', 'novel data mining algorithms', 'robust and scalable statistical methods']

learning NOUN pobj with
learning VERB acl set
learning VERB csubj require

empirical comparisons of various voting methods in bagging finding effective methods for developing an ensemble of models has been an active research area of large-scale data mining in recent years . models learned from data are often subject to some degree of uncertainty , for a variety of resoans . in classification , ensembles of models provide a useful means of averaging out error introduced by individual classifiers , hence reducing the generalization error of prediction . the plurality voting method is often chosen for bagging , because of its simplicity of implementation . however , the plurality approach to model reconciliation is ad-hoc . there are many other voting methods to choose from , including the anti-plurality method , the plurality method with elimination , the borda count method , and condorcet 's method of pairwise comparisons . any of these could lead to a better method for reconciliation . in this paper , we analyze the use of these voting methods in model reconciliation . we present empirical results comparing performance of these voting methods when applied in bagging . these results include some surprises , and among other things suggest that ( 1 ) plurality is not always the best voting method ; ( 2 ) the number of classes can affect the performance of voting methods ; and ( 3 ) the degree of dataset noise can affect the performance of voting methods . while it is premature to make final judgments about specific voting methods , the results of this work raise interesting questions , and they open the door to the application of voting theory in classification theory .

['ensemble classification', 'learning', 'model reconciliation', 'voting']

voting NOUN compound method
voting NOUN compound methods
voting NOUN compound methods
voting NOUN compound methods
voting NOUN compound method
voting NOUN compound methods
voting NOUN compound methods
voting NOUN compound methods
voting VERB compound theory

from frequent itemsets to semantically meaningful visual patterns data mining techniques that are successful in transaction and text data may not be simply applied to image data that contain high-dimensional features and have spatial structures . it is not a trivial task to discover meaningful visual patterns in image databases , because the content variations and spatial dependency in the visual data greatly challenge most existing methods . this paper presents a novel approach to coping with these difficulties for mining meaningful visual patterns . specifically , the novelty of this work lies in the following new contributions : ( 1 ) a principled solution to the discovery of meaningful itemsets based on frequent itemset mining ; ( 2 ) a self-supervised clustering scheme of the high-dimensional visual features by feeding back discovered patterns to tune the similarity measure through metric learning ; and ( 3 ) a pattern summarization method that deals with the measurement noises brought by the image data . the experimental results in the real images show that our method can discover semantically meaningful patterns efficiently and effectively .

['image data mining', 'meaningful itemset mining', 'pattern summarization', 'self-supervised clustering']


mining asynchronous periodic patterns in time series data

['asynchronous periodic pattern', 'database applications', 'segment-based approach']


a hybrid unsupervised approach for document clustering we propose a hybrid , unsupervised document clustering approach that combines a hierarchical clustering algorithm with expectation maximization . we developed several heuristics to automatically select a subset of the clusters generated by the first algorithm as the initial points of the second one . furthermore , our initialization algorithm generates not only an initial model for the iterative refinement algorithm but also an estimate of the model dimension , thus eliminating another important element of human supervision . we have evaluated the proposed system on five real-world document collections . the results show that our approach generates clustering solutions of higher quality than both its individual components .

['em initialization', 'information search and retrieval', 'unsupervised clustering']


making holistic schema matching robust : an ensemble approach the web has been rapidly `` deepened '' by myriad searchable databases online , where data are hidden behind query interfaces . as an essential task toward integrating these massive `` deep web '' sources , large scale schema matching ( i.e. , discovering semantic correspondences of attributes across many query interfaces ) has been actively studied recently . in particular , many works have emerged to address this problem by `` holistically '' matching many schemas at the same time and thus pursuing `` mining '' approaches in nature . however , while holistic schema matching has built its promise upon the large quantity of input schemas , it also suffers the robustness problem caused by noisy data quality . such noises often inevitably arise in the automatic extraction of schema data , which is mandatory in large scale integration . for holistic matching to be viable , it is thus essential to make it robust against noisy schemas . to tackle this challenge , we propose a data-ensemble framework with sampling and voting techniques , which is inspired by bagging predictors . specifically , our approach creates an ensemble of matchers , by randomizing input schema data into many independently downsampled trials , executing the same matcher on each trial and then aggregating their ranked results by taking majority voting . as a principled basis , we provide analytic justification of the effectiveness of this data-ensemble framework . further , empirically , our experiments on real web data show that the `` ensemblization '' indeed significantly boosts the matching accuracy under noisy schema input , and thus maintains the desired robustness of a holistic matcher .

['bagging predictors', 'data integration', 'deep web', 'ensemble', 'heterogeneous databases', 'schema matching']

ensemble ADJ amod approach
ensemble ADJ amod framework
ensemble NOUN dobj creates
ensemble ADJ amod framework

assessing data mining results via swap randomization the problem of assessing the significance of data mining results on high-dimensional 0-1 data sets has been studied extensively in the literature . for problems such as mining frequent sets and finding correlations , significance testing can be done by , e.g. , chi-square tests , or many other methods . however , the results of such tests depend only on the specific attributes and not on the dataset as a whole . moreover , the tests are more difficult to apply to sets of patterns or other complex results of data mining . in this paper , we consider a simple randomization technique that deals with this shortcoming . the approach consists of producing random datasets that have the same row and column margins with the given dataset , computing the results of interest on the randomized instances , and comparing them against the results on the actual data . this randomization technique can be used to assess the results of many different types of data mining algorithms , such as frequent sets , clustering , and rankings . to generate random datasets with given margins , we use variations of a markov chain approach , which is based on a simple swap operation . we give theoretical results on the efficiency of different randomization methods , and apply the swap randomization method to several well-known datasets . our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact , while for other datasets the discovered structure conveys meaningful information .

['0-1 data', 'randomization tests', 'significance testing', 'swaps']


learning sparse metrics via linear programming calculation of object similarity , for example through a distance function , is a common part of data mining and machine learning algorithms . this calculation is crucial for efficiency since distances are usually evaluated a large number of times , the classical example being query-by-example ( find objects that are similar to a given query object ) . moreover , the performance of these algorithms depends critically on choosing a good distance function . however , it is often the case that ( 1 ) the correct distance is unknown or chosen by hand , and ( 2 ) its calculation is computationally expensive ( e.g. , such as for large dimensional objects ) . in this paper , we propose a method for constructing relative-distance preserving low-dimensional mapping ( sparse mappings ) . this method allows learning unknown distance functions ( or approximating known functions ) with the additional property of reducing distance computation time . we present an algorithm that given examples of proximity comparisons among triples of objects ( object i is more like object j than object k ) , learns a distance function , in as few dimensions as possible , that preserves these distance relationships . the formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships . unlike other popular embedding algorithms , this method can easily generalize to new points , does not have local minima , and explicitly models computational efficiency by finding a mapping that is sparse , i.e. one that depends on a small subset of features or dimensions . experimental evaluation shows that the proposed formulation compares favorably with a state-of-the art method in several publicly available datasets .

['convex optimization', 'dimensionality reduction', 'information search and retrieval', 'linear programming', 'linear projections', 'metric learning', 'miscellaneous', 'relative distance constraints']


using relational knowledge discovery to prevent securities fraud we describe an application of relational knowledge discovery to a key regulatory mission of the national association of securities dealers ( nasd ) . nasd is the world 's largest private-sector securities regulator , with responsibility for preventing and discovering misconduct among securities brokers . our goal was to help focus nasd 's limited regulatory resources on the brokers who are most likely to engage in securities violations . using statistical relational learning algorithms , we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future . our models incorporate organizational relationships among brokers ( e.g. , past coworker ) , which domain experts consider important but have not been easily used before now . the learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of effort by nasd staff . model predictions were found to correlate highly with the subjective evaluations of experienced nasd examiners . furthermore , in all performance measures , our models performed as well as or better than the handcrafted rules that are currently in use at nasd .

['fraud detection', 'learning', 'relational probability trees', 'statistical relational learning']

learning NOUN compound algorithms

using structure indices for efficient approximation of network properties statistics on networks have become vital to the study of relational data drawn from areas such as bibliometrics , fraud detection , bioinformatics , and the internet . calculating many of the most important measures - such as betweenness centrality , closeness centrality , and graph diameter-requires identifying short paths in these networks . however , finding these short paths can be intractable for even moderate-size networks . we introduce the concept of a network structure index ( nsi ) , a composition of ( 1 ) a set of annotations on every node in the network and ( 2 ) a function that uses the annotations to estimate graph distance between pairs of nodes . we present several varieties of nsis , examine their time and space complexity , and analyze their performance on synthetic and real data sets . we show that creating an nsi for a given network enables extremely efficient and accurate estimation of a wide variety of network statistics on that network .

['centrality', 'knowledge discovery in graphs', 'network structure index', 'social network analysis']

centrality NOUN pobj as
centrality NOUN conj centrality

ordering patterns by combining opinions from multiple sources pattern ordering is an important task in data mining because the number of patterns extracted by standard data mining algorithms often exceeds our capacity to manually analyze them . in this paper , we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate sources . although rank aggregation techniques have been developed for applications such as meta-search engines , they are not directly applicable to pattern ordering for two reasons . first , the techniques are mostly supervised , i.e. , they require a sufficient amount of labeled data . second , the objects to be ranked are assumed to be independent and identically distributed ( i.i. d ) , an assumption that seldom holds in pattern ordering . the method proposed in this paper is an adaptation of the original hedge algorithm , modified to work in an unsupervised learning setting . techniques for addressing the i.i.d. violation in pattern ordering are also presented . experimental results demonstrate that our unsupervised hedge algorithm outperforms many alternative techniques such as those based on weighted average ranking and singular value decomposition .

['ensemble learning', 'pattern ordering', 'rank aggregation']


efficient handling of high-dimensional feature spaces by randomized classifier ensembles handling massive datasets is a difficult problem not only due to prohibitively large numbers of entries but in some cases also due to the very high dimensionality of the data . often , severe feature selection is performed to limit the number of attributes to a manageable size , which unfortunately can lead to a loss of useful information . feature space reduction may well be necessary for many stand-alone classifiers , but recent advances in the area of ensemble classifier techniques indicate that overall accurate classifier aggregates can be learned even if each individual classifier operates on incomplete `` feature view '' training data , i.e. , such where certain input attributes are excluded . in fact , by using only small random subsets of features to build individual component classifiers , surprisingly accurate and robust models can be created . in this work we demonstrate how these types of architectures effectively reduce the feature space for submodels and groups of sub-models , which lends itself to efficient sequential and\/or parallel implementations . experiments with a randomized version of adaboost are used to support our arguments , using the text classification task as an example .

['bounded-action devices']


dense itemsets frequent itemset mining has been the subject of a lot of work in data mining research ever since association rules were introduced . in this paper we address a problem with frequent itemsets : that they only count rows where all their attributes are present , and do not allow for any noise . we show that generalizing the concept of frequency while preserving the performance of mining algorithms is nontrivial , and introduce a generalization of frequent itemsets , dense itemsets . dense itemsets do not require all attributes to be present at the same time ; instead , the itemset needs to define a sufficiently large submatrix that exceeds a given density threshold of attributes present . we consider the problem of computing all dense itemsets in a database . we give a levelwise algorithm for this problem , and also study the top - $ k $ variations , i.e. , finding the k densest sets with a given support , or the k best-supported sets with a given density . these algorithms select the other parameter automatically , which simplifies mining dense itemsets in an explorative way . we show that the concept captures natural facets of data sets , and give extensive empirical results on the performance of the algorithms . combining the concept of dense itemsets with set cover ideas , we also show that dense itemsets can be used to obtain succinct descriptions of large datasets . we also discuss some variations of dense itemsets .

['error tolerance', 'frequent itemsets']


estimating rates of rare events at multiple resolutions we consider the problem of estimating occurrence rates of rare eventsfor extremely sparse data , using pre-existing hierarchies to perform inference at multiple resolutions . in particular , we focus on the problem of estimating click rates for ( webpage , advertisement ) pairs ( called impressions ) where both the pages and the ads are classified into hierarchies that capture broad contextual information at different levels of granularity . typically the click rates are low and the coverage of the hierarchies is sparse . to overcome these difficulties we devise a sampling method whereby we analyze aspecially chosen sample of pages in the training set , and then estimate click rates using a two-stage model . the first stage imputes the number of ( webpage , ad ) pairs at all resolutions of the hierarchy to adjust for the sampling bias . the second stage estimates clickrates at all resolutions after incorporating correlations among sibling nodes through a tree-structured markov model . both models are scalable and suited to large scale data mining applications . on a real-world dataset consisting of 1\/2 billion impressions , we demonstrate that even with 95 % negative ( non-clicked ) events in the training set , our method can effectively discriminate extremely rare events in terms of their click propensity .

['clickthrough rate', 'contextual matching', 'hierarchy', 'hypertext/hypermedia', 'imputation', 'internet advertising', 'maximum entropy', 'miscellaneous', 'on-line information services', 'tree-structured markov model']

hierarchy NOUN pobj of

automatic labeling of multinomial topic models multinomial distributions over words are frequently used to model topics in text collections . a common , major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic . so far , such labels have been generated manually in a subjective way . in this paper , we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way . we cast this labeling problem as an optimization problem involving minimizing kullback-leibler divergence between word distributions and maximizing mutual information between a label and a topic model . experiments with user study have been done on two text data sets with different genres . the results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models . our methods are general and can be applied to labeling topics learned through all kinds of topic models such as plsa , lda , and their variations .

['information search and retrieval', 'multinomial distribution', 'statistical topic models', 'topic model labeling']


statistical entity-topic models the primary purpose of news articles is to convey information about who , what , when and where . but learning and summarizing these relationships for collections of thousands to millions of articles is difficult . while statistical topic models have been highly successful at topically summarizing huge collections of text documents , they do not explicitly address the textual interactions between who\/where , i.e. named entities ( persons , organizations , locations ) and what , i.e. the topics . we present new graphical models that directly learn the relationship between topics discussed in news articles and entities mentioned in each article . we show how these entity-topic models , through a better understanding of the entity-topic relationships , are better at making predictions about entities .

['entity recognition', 'miscellaneous', 'probabilistic algorithms', 'text modeling', 'topic modeling']


relational data pre-processing techniques for improved securities fraud detection commercial datasets are often large , relational , and dynamic . they contain many records of people , places , things , events and their interactions over time . such datasets are rarely structured appropriately for knowledge discovery , and they often contain variables whose meanings change across different subsets of the data . we describe how these challenges were addressed in a collaborative analysis project undertaken by the university of massachusetts amherst and the national association of securities dealers ( nasd ) . we describe several methods for data pre-processing that we applied to transform a large , dynamic , and relational dataset describing nearly the entirety of the u.s. securities industry , and we show how these methods made the dataset suitable for learning statistical relational models . to better utilize social structure , we first applied known consolidation and link formation techniques to associate individuals with branch office locations . in addition , we developed an innovative technique to infer professional associations by exploiting dynamic employment histories . finally , we applied normalization techniques to create a suitable class label that adjusts for spatial , temporal , and other heterogeneity within the data . we show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity .

['data pre-processing', 'fraud detection', 'normalization', 'relational probability trees', 'statistical relational learning']

normalization NOUN compound techniques

evaluating similarity measures : a large-scale study in the orkut social network online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering , which makes recommendations to users based on their collective past behavior . while many similarity measures have been proposed and individually evaluated , they have not been evaluated relative to each other in a large real-world environment . we present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the orkut social network . we determine the usefulness of the different recommendations by actually measuring users ' propensity to visit and join recommended communities . we also examine how the ordering of recommendations influenced user selection , as well as interesting social issues that arise in recommending communities within a real social network .

['collaborative filtering', 'on-line information services', 'online communities', 'recommender system', 'similarity measure', 'social networks']


creating social networks to improve peer-to-peer networking we use knowledge discovery techniques to guide the creation of efficient overlay networks for peer-to-peer file sharing . an overlay network specifies the logical connections among peers in a network and is distinct from the physical connections of the network . it determines the order in which peers will be queried when a user is searching for a specific file . to better understand the role of the network overlay structure in the performance of peer-to-peer file sharing protocols , we compare several methods for creating overlay networks . we analyze the networks using data from a campus network for peer-to-peer file sharing that recorded anonymized data on 6,528 users sharing 291,925 music files over an 81-day period . we propose a novel protocol for overlay creation based on a model of user preference identified by latent-variable clustering with hierarchical dirichlet processes ( hdps ) . our simulations and empirical studies show that the clusters of songs created by hdps effectively model user behavior and can be used to create desirable network overlays that outperform alternative approaches .

['distributed hash tables', 'hierarchical dirichlet processes', 'learning', 'overlay networks', 'peer-to-peer networks', 'social networks']

social networks ADJ dobj creating

new em derived from kullback-leibler divergence we introduce a new em framework in which it is possible not only to optimize the model parameters but also the number of model components . a key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the em framework . while the classical em algorithm estimates model parameters empirically using the data points themselves , we estimate them using nonparametric density estimates . there exist many possible applications that require optimal adjustment of model components . we present experimental results in two domains . one is polygonal approximation of laser range data , which is an active research topic in robot navigation . the other is grouping of edge pixels to contour boundaries , which still belongs to unsolved problems in computer vision .

['em', 'expectation maximization', 'general', 'kullback-leibler divergence']

em PRON ROOT em
em PRON compound framework
em PRON nsubj estimates

finding similar files in large document repositories hewlett-packard has many millions of technical support documents in a variety of collections . as part of content management , such collections are periodically merged and groomed . in the process , it becomes important to identify and weed out support documents that are largely duplicates of newer versions . doing so improves the quality of the collection , eliminates chaff from search results , and improves customer satisfaction . the technical challenge is that through workflow and human processes , the knowledge of which documents are related is often lost . we required a method that could identify similar documents based on their content alone , without relying on metadata , which may be corrupt or missing . we present an approach for finding similar files that scales up to large document repositories . it is based on chunking the byte stream to find unique signatures that may be shared in multiple files . an analysis of the file-chunk graph yields clusters of related files . an optional bipartite graph partitioning algorithm can be applied to greatly increase scalability .

['content analysis and indexing', 'content management', 'document management', 'near duplicate detection', 'scalability', 'similarity']

scalability NOUN dobj increase

a web page prediction model based on click-stream tree representation of user behavior predicting the next request of a user as she visits web pages has gained importance as web-based activity increases . markov models and their variations , or models based on sequence mining have been found well suited for this problem . however , higher order markov models are extremely complicated due to their large number of states whereas lower order markov models do not capture the entire behavior of a user in a session . the models that are based on sequential pattern mining only consider the frequent sequences in the data set , making it difficult to predict the next request following a page that is not in the sequential pattern . furthermore , it is hard to find models for mining two different kinds of information of a user session . we propose a new model that considers both the order information of pages in a session and the time spent on them . we cluster user sessions based on their pair-wise similarity and represent the resulting clusters by a click-stream tree . the new user session is then assigned to a cluster based on a similarity measure . the click-stream tree of that cluster is used to generate the recommendation set . the model can be used as part of a cache prefetching system as well as a recommendation model .

['design methodology', 'graph based clustering', 'two dimensional sequential model', 'web usage mining']


local decomposition for rare class analysis given its importance , the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great attentions in the literature . however , the rare-class problem remains a critical challenge , because there is no natural way developed for handling imbalanced class distributions . this paper thus fills this crucial void by developing a method for classification using local clustering ( cog ) . specifically , for a data set with an imbalanced class distribution , we perform clustering within each large class and produce sub-classes with relatively balanced sizes . then , we apply traditional supervised learning algorithms , such as support vector machines ( svms ) , for classification . indeed , our experimental results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods . furthermore , we show that cog can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions .

['k-means clustering support vector machines', 'local clustering', 'rare class analysis']


using randomized response techniques for privacy-preserving data mining privacy is an important issue in data mining and knowledge discovery . in this paper , we propose to use the randomized response techniques to conduct the data mining computation . specially , we present a method to build decision tree classifiers from the disguised data . we conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data . our results show that although the data are disguised , our method can still achieve fairly high accuracy . we also show how the parameter used in the randomized response techniques affects the accuracy of the results .

['data mining', 'database applications', 'decision tree', 'privacy']

privacy NOUN npadvmod preserving
privacy NOUN pobj for

characterising the difference characterising the differences between two databases is an often occurring problem in data mining . detection of change over time is a prime example , comparing databases from two branches is another one . the key problem is to discover the patterns that describe the difference . emerging patterns provide only a partial answer to this question . in previous work , we showed that the data distribution can be captured in a pattern-based model using compression ( 12 ) . here , we extend this approach to define a generic dissimilarity measure on databases . moreover , we show that this approach can identify those patterns that characterise the differences between two distributions . experimental results show that our method provides a well-founded way to independently measure database dissimilarity that allows for thorough inspection of the actual differences . this illustrates the use of our approach in real world data mining .

['applications', 'compression', 'database dissimilarity', 'temporal data mining']

compression NOUN dobj using

efficient kernel feature extraction for massive data sets maximum margin discriminant analysis ( mmda ) was proposed that uses the margin idea for feature extraction . it often outperforms traditional methods like kernel principal component analysis ( kpca ) and kernel fisher discriminant analysis ( kfd ) . however , as in other kernel methods , its time complexity is cubic in the number of training points m , and is thus computationally inefficient on massive data sets . in this paper , we propose an ( 1 + îµ ) 2-approximation algorithm for obtaining the mmda features by extending the core vector machines . the resultant time complexity is only linear in m , while its space complexity is independent of m. extensive comparisons with the original mmda , kpca , and kfd on a number of large data sets show that the proposed feature extractor can improve classification accuracy , and is also faster than these kernel-based methods by more than an order of magnitude .

['design methodology', 'extraction', 'kernel feature', 'learning', 'scalability', 'svm']

extraction NOUN nsubj sets
extraction NOUN pobj for

finding partial orders from unordered 0-1 data in applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order ( the ages of the fossil sites , the locations of markers in the genome ) . the order might be total or partial : for example , two sites in different parts of the globe might be ecologically incomparable , or the ordering of certain markers might be different in different subgroups of the data . we consider the following problem . given a table over a set of 0-1 variables , find a partial order for the rows minimizing a score function and being as specific as possible . the score function can be , e.g. , the number of changes from 1 to 0 in a column ( for paleontology ) or the likelihood of the marker sequence ( for genomic data ) . our solution for this task first constructs small totally ordered fragments of the partial order , then finds good orientations for the fragments , and finally uses a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragments . we describe the method , discuss its properties , and give empirical results on paleontological data demonstrating the usefulness of the method . in the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data .

['consecutive ones property', 'hidden ordering', 'partial order']


incremental maintenance of quotient cube for median data cube pre-computation is an important concept for supporting olap ( online analytical processing ) and has been studied extensively . it is often not feasible to compute a complete data cube due to the huge storage requirement . recently proposed quotient cube addressed this issue through a partitioning method that groups cube cells into equivalence partitions . such an approach is not only useful for distributive aggregate functions such as sum but can also be applied to the holistic aggregate functions like median . maintaining a data cube for holistic aggregation is a hard problem since its difficulty lies in the fact that history tuple values must be kept in order to compute the new aggregate when tuples are inserted or deleted . the quotient cube makes the problem harder since we also need to maintain the equivalence classes . in this paper , we introduce two techniques called addset data structure and sliding window to deal with this problem . we develop efficient algorithms for maintaining a quotient cube with holistic aggregation functions that takes up reasonably small storage space . performance study shows that our algorithms are effective , efficient and scalable over large databases .

['data cube', 'holistic aggregation']


finding low-entropy sets and trees from binary data the discovery of subsets with special properties from binary data hasbeen one of the key themes in pattern discovery . pattern classes suchas frequent itemsets stress the co-occurrence of the value 1 in the data . while this choice makes sense in the context of sparse binary data , it disregards potentially interesting subsets of attributes that have some other type of dependency structure . we consider the problem of finding all subsets of attributes that have low complexity . the complexity is measured by either the entropy of the projection of the data on the subset , or the entropy of the data for the subset when modeled using a bayesian tree , with downward or upward pointing edges . we show that the entropy measure on sets has a monotonicity property , and thus a levelwise approach can find all low-entropy itemsets . we also show that the tree-based measures are bounded above by the entropy of the corresponding itemset , allowing similar algorithms to be used for finding low-entropy trees . we describe algorithms for finding all subsets satisfying an entropy condition . we give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data . we also discuss the search for high-entropy subsets and the computation of the vapnik-chervonenkis dimension of the data .

['local models', 'pattern discovery']


handling very large numbers of association rules in the analysis of microarray data the problem of analyzing microarray data became one of important topics in bioinformatics over the past several years , and different data mining techniques have been proposed for the analysis of such data . in this paper , we propose to use association rule discovery methods for determining associations among expression levels of different genes . one of the main problems related to the discovery of these associations is the scalability issue . microarrays usually contain very large numbers of genes that are sometimes measured in 10,000 s. therefore , analysis of such data can generate a very large number of associations that can often be measured in millions . the paper addresses this problem by presenting a method that enables biologists to evaluate these very large numbers of discovered association rules during the post-analysis stage of the data mining process . this is achieved by providing several rule evaluation operators , including rule grouping , filtering , browsing , and data inspection operators , that allow biologists to validate multiple individual gane regulation patterns at a time . by iteratively applying these operators , biologists can explore a significant part of all the initially generated rules in an acceptable period of time and thus answer biological questions that are of a particular interest to him or her . to validate our method , we tested our system on the microarray data pertaining to the studies of environmental hazards and their influence of gane expression processes . as a result , we managed to answer several questions that were of interest to the biologists that had collected this data .

['analysis of microarray data', 'association rules', 'bioinformatics', 'deduction', 'expert-driven rule validation', 'post-processing of discovered rules', 'rule filtering', 'rule grouping']

bioinformatics NOUN pobj in

xproj : a framework for projected structural clustering of xml documents xml has become a popular method of data representation both on the web and in databases in recent years . one of the reasons for the popularity of xml has been its ability to encode structural information about data records . however , this structural characteristic of data sets also makes it a challenging problem for a variety of data mining problems . one such problem is that of clustering , in which the structural aspects of the data result in a high implicit dimensionality of the data representation . as a result , it becomes more difficult to cluster the data in a meaningful way . in this paper , we propose an effective clustering algorithm for xml data which uses substructures of the documents in order to gain insights about the important underlying structures . we propose new ways of using multiple sub-structuralinformation in xml documents to evaluate the quality of intermediate cluster solutions , and guide the algorithms to a final solution which reflects the true structural behavior in individual partitions . we test the algorithm on a variety of real and synthetic data sets .

['clustering', 'xml']

clustering NOUN pobj for
xml NOUN compound documents
xml NOUN pobj of
xml NOUN pobj of
clustering NOUN pobj of
clustering NOUN compound algorithm
xml NOUN compound data
xml NOUN compound documents

alpha seeding for support vector machines

['classification', 'implementation', 'support vector machines', 'training speed-ups']


a mixture model for contextual text mining contextual text mining is concerned with extracting topical themes from a text collection with context information ( e.g. , time and location ) and comparing\/analyzing the variations of themes over different contexts . since the topics covered in a document are usually related to the context of the document , analyzing topical themes within context can potentially reveal many interesting theme patterns . in this paper , we generalize some of these models proposed in the previous work and we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases . specifically , we extend the probabilistic latent semantic analysis ( plsa ) model by introducing context variables to model the context of a document . the proposed mixture model , called contextual probabilistic latent semantic analysis ( cplsa ) model , can be applied to many interesting mining tasks , such as temporal text mining , spatiotemporal text mining , author-topic analysis , and cross-collection comparative analysis . empirical experiments show that the proposed mixture model can discover themes and their contextual variations effectively .

['clustering', 'context', 'contextual text mining', 'em algorithm', 'information search and retrieval', 'mixture model', 'theme pattern']

context NOUN compound information
context NOUN pobj to
context NOUN pobj within
context NOUN compound variables
context NOUN dobj model

playing hide-and-seek with correlations we present a method for very high-dimensional correlation analysis . the method relies equally on rigorous search strategies and on human interaction . at each step , the method conservatively `` shaves off '' a fraction of the database tuples and attributes , so that most of the correlations present in the data are not affected by the decomposition . instead , the correlations become more obvious to the user , because they are hidden in a much smaller portion of the database . this process can be repeated iteratively and interactively , until only the most important correlations remain . the main technical difficulty of the approach is figuring out how to `` shave off '' part of the database so as to preserve most correlations . we develop an algorithm for this problem that has a polynomial running time and guarantees result quality .

['association rules', 'correlations', 'data mining', 'database applications', 'minimum cut']

correlations NOUN pobj with
correlations NOUN pobj of
correlations NOUN nsubj become
correlations NOUN nsubj remain
correlations NOUN dobj preserve

pattern-based similarity search for microarray data one fundamental task in near-neighbor search as well as other similarity matching efforts is to find a distance function that can efficiently quantify the similarity between two objects in a meaningful way . in dna microarray analysis , the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli . although the magnitude of their expression levels may not be close , the patterns they exhibit can be very similar . unfortunately , none of the conventional distance metrics such as the lp norm can model this similarity effectively . in this paper , we study the near-neighbor search problem based on this new type of similarity . we propose to measure the distance between two genes by subspace pattern similarity , i.e. , whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions . we then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance , and we perform tests on various data sets to show its effectiveness .

['distance function', 'near neighbor', 'pattern recognition']


building connected neighborhood graphs for isometric data embedding neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space . this paper begins by explaining the algorithmic fundamentals of techniques for isometric data embedding and derives a general classification of these techniques . we will see that the nearest neighbor approaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neighborhood graphs and , consequently , may cause an algorithm fail to project data to a single low dimensional coordinate system . in this paper , we review three existing methods to construct k-edge-connected neighborhood graphs and propose a new method to construct k-connected neighborhood graphs . these methods are applicable to a wide range of data including data distributed among clusters . their features are discussed and compared through experiments .

['data embedding', 'dimensionality reduction', 'graph connectivity', 'manifold learning']


cleaning disguised missing data : a heuristic approach in some applications such as filling in a customer information form on the web , some missing values may not be explicitly represented as such , but instead appear as potentially valid data values . such missing values are known as disguised missing data , which may impair the quality of data analysis severely , such as causing significant biases and misleading results in hypothesis tests , correlation analysis and regressions . the very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection . they highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers . to tackle the problem of cleaning disguised missing data , in this paper , we first model the distribution of disguised missing data , and propose the embedded unbiased sample heuristic . then , we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data . our method does not require any domain background knowledge to find the suspicious disguise values . we report an empirical evaluation using real data sets , which shows that our method is effective - the frequently used disguise values found by our method match the values identified by the domain experts nicely . our method is also efficient and scalable for processing large data sets .

['data cleaning', 'data quality', 'disguised missing data']


applying collaborative filtering techniques to movie search for better ranking and browsing we propose a new ranking method , which combines recommender systems with information search tools for better search and browsing . our method uses a collaborative filtering algorithm to generate personal item authorities for each user and combines them with item proximities for better ranking . to demonstrate our approach , we build a prototype movie search and browsing engine called mad6 ( movies , actors and directors ; 6 degrees of separation ) . we conduct offline and online tests of our ranking algorithm . for offline testing , we use yahoo ! search queries that resulted in a click on a yahoo ! movies or internet movie database ( imdb ) movie url . our online test involved 44 yahoo ! employees providing subjective assessments of results quality . in both tests , our ranking methods show significantly better recall and quality than imdb search and yahoo ! movies current search .

['collaborative filtering', 'information retrieval', 'movie search', 'performance evaluation', 'recommender systems', 'search ranking']


statistical change detection for multi-dimensional data this paper deals with detecting change of distribution in multi-dimensional data sets . for a given baseline data set and a set of newly observed data points , we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set . we define a test statistic that is strictly distribution-free under the null hypothesis . our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection .

['change detection', 'database applications', 'density test', 'kernel density estimation']


spatial scan statistics : approximations and performance study spatial scan statistics are used to determine hotspots in spatial data , and are widely used in epidemiology and biosurveillance . in recent years , there has been much effort invested in designing efficient algorithms for finding such `` high discrepancy '' regions , with methods ranging from fast heuristics for special cases , to general grid-based methods , and to efficient approximation algorithms with provable guarantees on performance and quality . in this paper , we make a number of contributions to the computational study of spatial scan statistics . first , we describe a simple exact algorithm for finding the largest discrepancy region in a domain . second , we propose a new approximation algorithm for a large class of discrepancy functions ( including the kulldorff scan statistic ) that improves the approximation versus run time trade-off of prior methods . third , we extend our simple exact and our approximation algorithms to data sets which lie naturally on a grid or are accumulated onto a grid . fourth , we conduct a detailed experimental comparison of these methods with a number of known methods , demonstrating that our approximation algorithm has far superior performance in practice to prior methods , and exhibits a good performance-accuracy trade-off . all extant methods ( including those in this paper ) are suitable for data sets that are modestly sized ; if data sets are of the order of millions of data points , none of these methods scale well . for such massive data settings , it is natural to examine whether small-space streaming algorithms might yield accurate answers . here , we provide some negative results , showing that any streaming algorithms that even provide approximately optimal answers to the discrepancy maximization problem must use space linear in the input .

['discrepancy', 'kulldorff scan statistic', 'probability and statistics', 'spatial scan statistics']

discrepancy NOUN nmod regions
discrepancy NOUN compound region
discrepancy NOUN compound functions
discrepancy NOUN compound maximization

outlier detection by sampling with accuracy guarantees an effective approach to detecting anomalous points in a data set is distance-based outlier detection . this paper describes a simple sampling algorithm to effciently detect distance-based outliers in domains where each and every distance computation is very expensive . unlike any existing algorithms , the sampling algorithm requires a xed number of distance computations and can return good results with accuracy guarantees . the most computationally expensive aspect of estimating the accuracy of the result is sorting all of the distances computed by the sampling algorithm . the experimental study on two expensive domains as well as ten additional real-life datasets demonstrates both the effciency and effectiveness of the sampling algorithm in comparison with the state-of-the-art algorithm and there liability of the accuracy guarantees .

['active learning', 'ensemble method', 'outlier detection']


identifying bridging rules between conceptual clusters a bridging rule in this paper has its antecedent and action from different conceptual clusters . we first design two algorithms for mining bridging rules between clusters in a database , and then propose two non-linear metrics for measuring the interestingness of bridging rules . bridging rules can be distinct from association rules ( or frequent itemsets ) . this is because ( 1 ) bridging rules can be generated by infrequent itemsets that are pruned in association rule mining ; and ( 2 ) bridging rules are measured by the importance that includes the distance between two conceptual clusters , whereas frequent itemsets are measured by only the support .

['association rule', 'bridging rule', 'clustering', 'entropy', 'learning', 'outlier']


generation of synthetic data sets for evaluating the accuracy of knowledge discovery systems information discovery and analysis systems ( idas ) are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events . application domains for idas are numerous and include the emerging area of homeland security . developing test cases for an idas requires background data sets into which hypothetical future scenarios can be overlaid . the idas can then be measured in terms of false positive and false negative error rates . obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources . in this paper , we give an overview of the design and architecture of an idas data set generator ( idsg ) that enables a fast and comprehensive test of an idas . the idsg generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes . a credit card transaction application is used to illustrate the approach .

['data generation', 'data mining', 'information discovery', 'testing tools']


detecting outliers using transduction and statistical testing outlier detection can uncover malicious behavior in fields like intrusion detection and fraud analysis . although there has been a significant amount of work in outlier detection , most of the algorithms proposed in the literature are based on a particular definition of outliers ( e.g. , density-based ) , and use ad-hoc thresholds to detect them . in this paper we present a novel technique to detect outliers with respect to an existing clustering model . however , the test can also be successfully utilized to recognize outliers when the clustering information is not available . our method is based on transductive confidence machines , which have been previously proposed as a mechanism to provide individual confidence measures on classification decisions . the test uses hypothesis testing to prove or disprove whether a point is fit to be in each of the clusters of the model . we experimentally demonstrate that the test is highly robust , and produces very few misdiagnosed points , even when no clustering information is available . furthermore , our experiments demonstrate the robustness of our method under the circumstances of data contaminated by outliers . we finally show that our technique can be successfully applied to identify outliers in a noisy data set for which no information is available ( e.g. , ground truth , clustering structure , etc. ) . as such our proposed methodology is capable of bootstrapping from a noisy data set a clean one that can be used to identify future outliers .

['clustering']

clustering NOUN compound model
clustering NOUN amod information
clustering NOUN amod information
clustering VERB advcl applied

density-based clustering for real-time stream data existing data-stream clustering algorithms such as clustream arebased on k-means . these clustering algorithms are incompetent tofind clusters of arbitrary shapes and can not handle outliers . further , they require the knowledge of k and user-specified time window . to address these issues , this paper proposes d-stream , a framework for clustering stream data using adensity-based approach . the algorithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the density . the algorithm adopts a density decaying technique to capture the dynamic changes of a data stream . exploiting the intricate relationships between the decay factor , data density and cluster structure , our algorithm can efficiently and effectively generate and adjust the clusters in real time . further , a theoretically sound technique is developed to detect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system . the technique makes high-speed data stream clustering feasible without degrading the clustering quality . the experimental results show that our algorithm has superior quality and efficiency , can find clusters of arbitrary shapes , and can accurately recognize the evolving behaviors of real-time data streams .

['d-stream', 'density-based clustering', 'sporadic grids', 'stream data mining']


semi-supervised time series classification the problem of time series classification has attracted great interest in the last decade . however current research assumes the existence of large amounts of labeled training data . in reality , such data may be very difficult or expensive to obtain . for example , it may require the time and expertise of cardiologists , space launch technicians , or other domain specialists . as in many other domains , there are often copious amounts of unlabeled data available . for example , the physiobank archive contains gigabytes of ecg data . in this work we propose a semi-supervised technique for building time series classifiers . while such algorithms are well known in text domains , we will show that special considerations must be made to make them both efficient and effective for the time series domain . we evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms , handwritten documents , and video datasets . the experimental results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers .

['classification', 'semi-supervised learning', 'time series']

classification NOUN ROOT classification
classification NOUN pobj of

pragmatic text mining : minimizing human effort to quantify many issues in call logs we discuss our experiences in analyzing customer-support issues from the unstructured free-text fields of technical-support call logs . the identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type , to appropriately target engineering resources , and to provide the best diagnosis , support and documentation for most common issues . we present a new set of techniques for doing this efficiently on an industrial scale , without requiring manual coding of calls in the call center . our approach involves ( 1 ) a new text clustering method to identify common and emerging issues ; ( 2 ) a method to rapidly train large numbers of categorizers in a practical , interactive manner ; and ( 3 ) a method to accurately quantify categories , even in the face of inaccurate classifications and training sets that necessarily can not match the class distribution of each new month 's data . we present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at hp .

['applications', 'decision support', 'log processing', 'quantification', 'supervised machine learning', 'text classification', 'text mining']

quantification NOUN nsubj is

quantifying trends accurately despite classifier error and class imbalance this paper promotes a new task for supervised machine learning research : quantification - the pursuit of learning methods for accurately estimating the class distribution of a test set , with no concern for predictions on individual cases . a variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers . these tasks cover a large and important family of applications that measure trends over time . the paper establishes a research methodology , and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications . in empirical tests , median sweep methods show outstanding ability to estimate the class distribution , despite wide disparity in testing and training conditions . the paper addresses shifting class priors and costs , but not concept drift in general .

['classification', 'cost quantification', 'decision support', 'quantification', 'text mining']

quantification NOUN appos task
quantification NOUN pobj for
classification NOUN compound threshold

predicting prostate cancer recurrence via maximizing the concordance index in order to effectively use machine learning algorithms , e.g. , neural networks , for the analysis of survival data , the correct treatment of censored data is crucial . the concordance index ( ci ) is a typical metric for quantifying the predictive ability of a survival model . we propose a new algorithm that directly uses the ci as the objective function to train a model , which predicts whether an event will eventually occur or not . directly optimizing the ci allows the model to make complete use of the information from both censored and non-censored observations . in particular , we approximate the ci via a differentiable function so that gradient-based methods can be used to train the model . we applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy . compared with the traditional cox proportional hazards model and several other algorithms based on neural networks and support vector machines , our algorithm achieves a significant improvement in being able to identify high-risk and low-risk groups of patients .

['concordance index', 'neural networks', 'nomogram', 'prostate cancer recurrence', 'survival analysis']


a data mining approach to modeling relationships among categories in image collection this paper proposes a data mining approach to modeling relationships among categories in image collection . in our approach , with image feature grouping , a visual dictionary is created for color , texture , and shape feature attributes respectively . labeling each training image with the keywords in the visual dictionary , a classification tree is built . based on the statistical properties of the feature space we define a structure , called î±-semantics graph , to discover the hidden semantic relationships among the semantic categories embodied in the image collection . with the î±-semantics graph , each semantic category is modeled as a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the categories in the feature space . the model is utilized in the semantics-intensive image retrieval application . an algorithm using the classification accuracy measures is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image . the experimental evaluations have demonstrated that the proposed approach models the semantic relationships effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency .

['fuzzy model', 'image collection', 'relationships', 'semantic category']

relationships NOUN pobj to
relationships NOUN pobj to
relationships NOUN dobj discover
relationships NOUN dobj models

learning to detect malicious executables in the wild in this paper , we describe the development of a fielded application for detecting malicious executables in the wild . we gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features . such processing resulted in more than 255 million distinct n-grams . after selecting the most relevant n-grams for prediction , we evaluated a variety of inductive methods , including naive bayes , decision trees , support vector machines , and boosting . ultimately , boosted decision trees outperformed other methods with an area under the roc curve of 0.996 . results also suggest that our methodology will scale to larger collections of executables . to the best of our knowledge , ours is the only fielded application for this task developed using techniques from machine learning and data mining .

['invasive software', 'malicious software']


short term performance forecasting in enterprise systems we use data mining and machine learning techniques to predict upcoming periods of high utilization or poor performance in enterprise systems . the abundant data available and complexity of these systems defies human characterization or static models and makes the task suitable for data mining techniques . we formulate the problem as one of classification : given current and past information about the system 's behavior , can we forecast whether the system will meet its performance targets over the next hour ? using real data gathered from several enterprise systems in hewlett-packard , we compare several approaches ranging from time series to bayesian networks . besides establishing the predictive power of these approaches our study analyzes three dimensions that are important for their application as a stand alone tool . first , it quantifies the gain in accuracy of multivariate prediction methods over simple statistical univariate methods . second , it quantifies the variations in accuracy when using different classes of system and workload features . third , it establishes that models induced using combined data from various systems generalize well and are applicable to new systems , enabling accurate predictions on systems with insufficient historical data . together this analysis offers a promising outlook on the development of tools to automate assignment of resources to stabilize performance , ( e.g. , adding servers to a cluster ) and allow opportunistic job scheduling ( e.g. , backups or virus scans ) .

['enterprise systems', 'performance forecasting', 'system management']


scalable look-ahead linear regression trees most decision tree algorithms base their splitting decisions on a piecewise constant model . often these splitting algorithms are extrapolated to trees with non-constant models at the leaf nodes . the motivation behind look-ahead linear regression trees ( llrt ) is that out of all the methods proposed to date , there has been no scalable approach to exhaustively evaluate all possible models in the leaf nodes in order to obtain an optimal split . using several optimizations , llrt is able to generate and evaluate thousands of linear regression models per second . this allows for a near-exhaustive evaluation of all possible splits in a node , based on the quality of fit of linear regression models in the resulting branches . we decompose the calculation of the residual sum of squares in such a way that a large part of it is pre-computed . the resulting method is highly scalable . we observe it to obtain high predictive accuracy for problems with strong mutual dependencies between attributes . we report on experiments with two simulated and seven real data sets .

['linear regression tree', 'model tree', 'models', 'predictive model', 'regression', 'scalable algorithms']

regression NOUN compound trees
models NOUN pobj with
regression NOUN compound trees
models NOUN dobj evaluate
regression NOUN compound models
models NOUN pobj of
regression NOUN compound models
models NOUN pobj of

mining relational data through correlation-based multiple view validation commercial relational databases currently store vast amounts of real-world data . the data within these relational repositories are represented by multiple relations , which are inter-connected by means of foreign key joins . the mining of such interrelated data poses a major challenge to the data mining community . unfortunately , traditional data mining algorithms usually only explore one relation , the so-called target relation , thus excluding crucial knowledge embedded in the related so-called background relations . in this paper , we propose a novel approach for classifying relational such domains . this strategy employs multiple views to capture crucial information not only from the target relation , but also from related relations . this information is integrated into the relational mining process . the framework presented here , firstly , explore the relational domain to partition its features space into multiple subsets . subsequently , these subsets are used to construct multiple uncorrelated views , based on a novel correlation-based view validation method , against the target concept . finally , the knowledge possessed by multiple views are incorporated into a meta-learning mechanism to augment one another . based on this framework , a wide range of conventional data mining methods can be applied to mine relational databases . our experiments on benchmark real-world data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time , when compared with two other relational data mining approaches .

['classification', 'multi-relational data mining', 'multi-view learning', 'relational database']


the minimum consistent subset cover problem and its applications in data mining in this paper , we introduce and study the minimum consistent subset cover ( mcsc ) problem . given a finite ground set x and a constraint t , find the minimum number of consistent subsets that cover x , where a subset of x is consistent if it satisfies t. the mcsc problem generalizes the traditional set covering problem and has minimum clique partition , a dual problem of graph coloring , as an instance . many practical data mining problems in the areas of rule learning , clustering , and frequent pattern mining can be formulated as mcsc instances . in particular , we discuss the minimum rule set problem that minimizes model complexity of decision rules as well as some converse k-clustering problems that minimize the number of clusters satisfying certain distance constraints . we also show how the mcsc problem can find applications in frequent pattern summarization . for any of these mcsc formulations , our proposed novel graph-based generic algorithm cag can be directly applicable . cag starts by constructing a maximal optimal partial solution , then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set . our experiments on benchmark datasets show that cag achieves good results compared to existing popular heuristics .

['converse k-clustering', 'minimum consistent subset cover', 'minimum rule set', 'pattern summarization']


parallel mining of closed sequential patterns discovery of sequential patterns is an essential data mining task with broad applications . among several variations of sequential patterns , closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it . unfortunately , there is no parallel closed sequential pattern mining method proposed yet . in this paper we develop an algorithm , called par-csp ( parallel closed sequential pattern mining ) , to conduct parallel mining of closed sequential patterns on a distributed memory system . par-csp partitions the work among the processors by exploiting the divide-and-conquer property so that the overhead of interprocessor communication is minimized . par-csp applies dynamic scheduling to avoid processor idling . moreover , it employs a technique , called selective sampling to address the load imbalance problem . we implement par-csp using mpi on a 64-node linux cluster . our experimental results show that par-csp attains good parallelization efficiencies on various input datasets .

['load balancing', 'parallel algorithms', 'sampling']

sampling NOUN oprd called

visualization support for a user-centered kdd process viewing knowledge discovery as a user-centered process that requires an effective collaboration between the user and the discovery system , our work aims to support an active role of the user in that process by developing synergistic visualization tools integrated in our discovery system d2ms . these tools provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing , selecting mining algorithms and parameters , evaluating and comparing discovered models , and taking control of the whole discover process . our case-studies with two medical datasets on meningitis and stomach cancer show that , with visualization tools in d2ms , the user gains better insight in each step of the knowledge discovery process as well the relationship between data and discovered knowledge .

['data and knowledge visualization', 'knowledge discovery process', 'model selection', 'representations', "the user's active role"]


optimizing time series discretization for knowledge discovery knowledge discovery in time series usually requires symbolic time series . many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values . this often leads to symbols that do not correspond to states of the process generating the time series and can not be interpreted meaningfully . we propose a new method for meaningful unsupervised discretization of numeric time series called persist . the algorithm is based on the kullback-leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols . its performance is evaluated on both artificial and real life data in comparison to the most common discretization methods . persist achieves significantly higher accuracy than existing static methods and is robust against noise . it also outperforms hidden markov models for all but very simple cases .

['discretization', 'pattern recognition', 'persistence', 'time series']

time series NOUN compound discretization
discretization NOUN dobj optimizing
discretization NOUN compound methods
discretization NOUN pobj for
discretization NOUN compound symbols
discretization NOUN compound methods

distributed cooperative mining for information consortia we consider the situation where a number of agents are distributed and each of them collects a data sequence generated according to an unknown probability distribution . here each of the distributions is specified by common parameters and individual parameters e.g. , a normal distribution with an identical mean and a different variance . here we introduce a notion of an information consortium , which is a framework where the agents can not show raw data to one another , but they like to enjoy significant information gain for estimating the respective distributions . such an information consortium has recently received much interest in a broad range of areas including financial risk management , ubiquitous network mining , etc. . in this paper we are concerned with the following three issues : 1 ) how to design a collaborative strategy for agents to estimate the respective distributions in the information consortium , 2 ) characterizing when each agent has a benefit in terms of information gain for estimating its distribution or information loss for predicting future data , and 3 ) charracterizing how much benefit each agent obtains . in this paper we yield a statistical formulation of information consortia and solve all of the above three problems for a general form of probability distributions . specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit .

['collective mining', 'database applications', 'distributed mining', 'estimation', 'information consortium', 'statistical model']

estimation NOUN pobj for

structure and evolution of online social networks in this paper , we consider the evolution of structure within large online social networks . we present a series of measurements of two such networks , together comprising in excess of five million people and ten million friendship links , annotated with metadata capturing the time of every event in the life of the network . our measurements expose a surprising segmentation of these networks into three regions : singletons who do not participate in the network ; isolated communities which overwhelmingly display star structure ; and a giant component anchored by a well-connected core region which persists even in the absence of stars . we present a simple model of network growth which captures these aspects of component structure . the model follows our experimental results , characterizing users as either passive members of the network ; inviters who encourage offline friends and acquaintances to migrate online ; and linkers who fully participate in the social evolution of the network .

['graph evolution', 'graph mining', 'small-world phenomenon', 'social networks', 'stars']

stars NOUN pobj of

exploiting unlabeled data in ensemble methods an adaptive semi-supervised ensemble method , assemble , is proposed that constructs classification ensembles based on both labeled and unlabeled data . assemble alternates between assigning `` pseudo-classes '' to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data . mathematically , this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data . unlike alternative approaches , assemble does not require a semi-supervised learning method for the base classifier . assemble can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems . assemble using decision trees won the nips 2001 unlabeled data competition . in addition , strong results on several benchmark datasets using both decision trees and neural networks support the proposed method .

['boosting', 'classification', 'ensemble learning', 'self-modifying machines', 'semi-supervised learning']

classification NOUN compound ensembles
classification NOUN compound margin
classification NOUN compound algorithm

a fast algorithm for finding frequent episodes in event streams frequent episode discovery is a popular framework for mining data available as a long sequence of events . an episode is essentially a short ordered sequence of event types and the frequency of an episode is some suitable measure of how often the episode occurs in the data sequence . recently , we proposed a new frequency measure for episodes based on the notion of non-overlapped occurrences of episodes in the event sequence , and showed that , such a definition , in addition to yielding computationally efficient algorithms , has some important theoretical properties in connecting frequent episode discovery with hmm learning . this paper presents some new algorithms for frequent episode discovery under this non-overlapped occurrences-based frequency definition . the algorithms presented here are better ( by a factor of n , where n denotes the size of episodes being discovered ) in terms of both time and space complexities when compared to existing methods for frequent episode discovery . we show through some simulation experiments , that our algorithms are very efficient . the new algorithms presented here have arguably the least possible orders of spaceand time complexities for the task of frequent episode discovery .

['event streams', 'frequent episodes', 'non-overlapped occurrences', 'temporal data mining']


cross channel optimized marketing by reinforcement learning the issues of cross channel integration and customer life time value modeling are two of the most important topics surrounding customer relationship management ( crm ) today . in the present paper , we describe and evaluate a novel solution that treats these two important issues in a unified framework of markov decision processes ( mdp ) . in particular , we report on the results of a joint project between ibm research and saks fifth avenue to investigate the applicability of this technology to real world problems . the business problem we use as a testbed for our evaluation is that of optimizing direct mail campaign mailings for maximization of profits in the store channel . we identify a problem common to cross-channel crm , which we call the cross-channel challenge , due to the lack of explicit linking between the marketing actions taken in one channel and the customer responses obtained in another . we provide a solution for this problem based on old and new techniques in reinforcement learning . our in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected , by employing a mailing policy automatically generated by our methodology . these results confirm that our approach is valid in dealing with the cross channel crm scenarios in the real world .

['cost sensitive learning', 'crm', 'customer life time value', 'learning', 'reinforcement learning', 'targeted marketing']

learning VERB acl reinforcement
crm NOUN appos management
crm NOUN pobj to
learning VERB pobj in
crm NOUN compound scenarios

unsupervised learning on k-partite graphs various data mining applications involve data objects of multiple types that are related to each other , which can be naturally formulated as a k-partite graph . however , the research on mining the hidden structures from a k-partite graph is still limited and preliminary . in this paper , we propose a general model , the relation summary network , to find the hidden structures ( the local cluster structures and the global community structures ) from a k-partite graph . the model provides a principal framework for unsupervised learning on k-partite graphs of various structures . under this model , we derive a novel algorithm to identify the hidden structures of a k-partite graph by constructing a relation summary network to approximate the original k-partite graph under a broad range of distortion measures . experiments on both synthetic and real datasets demonstrate the promise and effectiveness of the proposed model and algorithm . we also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the clustering approaches .

['bregman divergence', 'k-partite graph', 'relation summary network', 'unsupervised learning']


mining for misconfigured machines in grid systems grid systems are proving increasingly useful for managing the batch computing jobs of organizations . one well-known example is intel , whose internally developed netbatch system manages tens of thousands of machines . the size , heterogeneity , and complexity of grid systems make them very difficult , however , to configure . this often results in misconfigured machines , which may adversely affect the entire system . we investigate a distributed data mining approach for detection of misconfigured machines . our grid monitoring system ( gms ) non-intrusively collects data from all sources ( log files , system services , etc. ) available throughout the grid system . it converts raw data to semantically meaningful data and stores this data on the machine it was obtained from , limiting incurred overhead and allowing scalability . afterwards , when analysis is requested , a distributed outliers detection algorithm is employed to identify misconfigured machines . the algorithm itself is implemented as a recursive workflow of grid jobs . it is especially suited to grid systems , in which the machines might be unavailable most of the time and often fail altogether .

['distributed data mining', 'distributed systems', 'grid information system', 'grid systems', 'outliers detection', 'system monitoring']


a framework for simultaneous co-clustering and learning from complex data for difficult classification or regression problems , practitioners often segment the data into relatively homogenous groups and then build a model for each group . this two-step procedure usually results in simpler , more interpretable and actionable models without any lossin accuracy . we consider problems such as predicting customer behavior across products , where the independent variables can be naturally partitioned into two groups . a pivoting operation can now result in the dependent variable showing up as entries in a `` customer by product '' data matrix . we present a model-based co-clustering ( meta ) - algorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models . this algorithm provably converges to a local minimum of a suitable cost function . the framework not only generalizes co-clustering and collaborative filtering to model-basedco-clustering , but can also be viewed as simultaneous co-segmentation and classification or regression , which is better than independently clustering the data first and then building models . moreover , it applies to a wide range of bi-modal or multimodal data , and can be easily specialized to address classification and regression problems . we demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data .

['classification', 'co-clustering', 'multimodal data', 'prediction models', 'regression']

classification NOUN conj segmentation
regression NOUN conj classification
classification NOUN dobj address
regression NOUN compound problems

expertise modeling for matching papers with reviewers an essential part of an expert-finding task , such as matching reviewers to submitted papers , is the ability to model the expertise of a person based on documents . we evaluate several measures of the association between an author in an existing collection of research papers and a previously unseen document . we compare two language model based approaches with a novel topic model , author-persona-topic ( apt ) . in this model , each author can write under one or more `` personas , '' which are represented as independent distributions over hidden topics . examples of previous papers written by prospective reviewers are gathered from the rexa database , which extracts and disambiguates author mentions from documents gathered from the web . we evaluate the models using a reviewer matching task based on human relevance judgments determining how well the expertise of proposed reviewers matches a submission . we find that the apt topic model outperforms the other models .

['reviewer finding', 'topic models']


beyond classification and ranking : constrained optimization of the roi classification has been commonly used in many data mining projects in the financial service industry . for instance , to predict collectability of accounts receivable , a binary class label is created based on whether a payment is received within a certain period . however , optimization of the classifier does not necessarily lead to maximization of return on investment ( roi ) , since maximization of the true positive rate is often different from maximization of the collectable amount which determines the roi under a fixed budget constraint . the typical cost sensitive learning does not solve this problem either since it involves an unknown opportunity cost due to the budget constraint . learning the ranks of collectable amount would ultimately solve the problem , but it tries to tackle an unnecessarily difficult problem and often results in poorer results for our specific target . we propose a new algorithm that uses gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the roi . by comparison with several classification , regression , and ranking algorithms , we demonstrate the new algorithm 's substantial improvement of the financial impact on our clients in the financial service industry .

['constrained optimization', 'neural networks', 'return on investment']


boostcluster : boosting clustering by pairwise constraints data clustering is an important task in many disciplines . a large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints . however , these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints . we present a boosting framework for data clustering , termed as boostcluster , that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints . the key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised . the proposed framework addresses this problem by dynamically generating new data representations at each iteration that are , on the one hand , adapted to the clustering results at previous iterations by the given algorithm , and on the other hand consistent with the given side information . our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithms ( k-means , partitional singlelink , spectral clustering ) , and its performance is comparable to the state-of-the-art algorithms for data clustering with side information .

['boosting', 'data clustering', 'pairwise constraints', 'semi-supervised learning']

boosting VERB compound framework
boosting VERB compound framework
boosting VERB compound framework

measuring and extracting proximity in networks measuring distance or some other form of proximity between objects is a standard data mining tool . connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks . we propose a new way of measuring and extracting proximity in networks called `` cycle free effective conductance '' ( cfec ) . our proximity measure can handle more than two endpoints , directed edges , is statistically well-behaved , and produces an effectiveness score for the computed subgraphs . we provide an efficien talgorithm . also , we report experimental results and show examples for three large network data sets : a telecommunications calling graph , the imdb actors graph , and an academic co-authorship network .

['connection subgraph', 'cycle-free escape probability', 'escape probability', 'proximity', 'proximity subgraph', 'random walks']

proximity NOUN dobj demonstrate
proximity NOUN dobj extracting
proximity NOUN compound measure

algorithms for time series knowledge mining temporal patterns composed of symbolic intervals are commonly formulated with allen 's interval relations originating in temporal reasoning . this representation has severe disadvantages for knowledge discovery . the time series knowledge representation ( tskr ) is a new hierarchical language for interval patterns expressing the temporal concepts of coincidence and partial order . we present effective and efficient mining algorithms for such patterns based on itemset techniques . a novel form of search space pruning effectively reduces the size of the mining result to ease interpretation and speed up the algorithms . on a real data set a concise set of tskr patterns can explain the underlying temporal phenomena , whereas the patterns found with allen 's relations are far more numerous yet only explain fragments of the data .

['interval patterns', 'knowledge discovery', 'pattern recognition', 'time series']


lungcad : a clinically approved , machine learning system for lung cancer detection we present lungcad , a computer aided diagnosis ( cad ) system that employs a classification algorithm for detecting solid pulmonary nodules from ct thorax studies . we briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain . the most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system , is the requirement that the cad system be tested in a clinical trial . we describe the clinical trial in which lungcad was tested : a large scale multi-reader , multi-case ( mrmc ) retrospective observational study to evaluate the effect of cad in clinical practice for detecting solid pulmonary nodules from ct thorax studies . the clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with lungcad , both for detecting nodules and identifying potentially actionable nodules ; this , along with other findings from the trial , has resulted in fda approval for lungcad in late 2006 .

['classification', 'clinical trial', 'computer aided detection', 'lung cancer prognosis', 'miscellaneous']

classification NOUN compound algorithm

dynamic syslog mining for network failure monitoring syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring . they are used to address a wide range of important issues including network failure symptom detection and event correlation discovery . syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time . this paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices . the key ideas of dynamic syslog mining are 1 ) to represent syslog behavior using a mixture of hidden markov models , 2 ) to adaptively learn the model using an on-line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components , and 3 ) to give anomaly scores using universal test statistics with a dynamically optimized threshold . using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection , emerging pattern identification , and correlation discovery .

['artificial intelligence', 'correlation analysis', 'failure detection', 'model selection', 'probabilistic modeling', 'syslog mining']


compressed data cubes for olap aggregate query approximation on continuous dimensions

['approximate query answering', 'data cubes', 'decision support', 'density estimation', 'olap']


empirical bayesian data mining for discovering patterns in post-marketing drug safety because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing , manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports ( `` pharmacovigilance '' ) . the resulting databases , while rich in real-world information , are notoriously difficult to analyze using traditional techniques . each report may involve multiple medicines , symptoms , and demographic factors , and there is no easily linked information on drug exposure in the reporting population . kdd techniques , such as association finding , are well-matched to the problem , but are difficult for medical staff to apply and interpret . to deploy kdd effectively for pharmacovigilance , lincoln technologies and glaxosmithkline collaborated to create a webbased safety data mining web environment . the analytical core is a high-performance implementation of the mgps ( multi-item gamma poisson shrinker ) algorithm described previously by dumouchel and pregibon , with several significant extensions and enhancements . the environment offers an interface for specifying data mining runs , a batch execution facility , tabular and graphical methods for exploring associations , and drilldown to case details . substantial work was involved in preparing the raw adverse event data for mining , including harmonization of drug names and removal of duplicate reports . the environment can be used to explore both drug-event and multi-way associations ( interactions , syndromes ) . it has been used to study age\/gender effects , to predict the safety profiles of proposed combination drugs , and to separate contributions of individual drugs to safety problems in polytherapy situations .

['association rules', 'empirical bayes methods', 'pharmacovigilance', 'post-marketing surveillance']

pharmacovigilance NOUN dobj perform
pharmacovigilance NOUN pobj for

learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming the kernel function plays a central role in kernel methods . in this paper , we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in regularized kernel discriminant analysis ( rkda ) , which performs lineardiscriminant analysis in the feature space via the kernel trick . previous studies have shown that this kernel learning problem can be formulated as a semidefinite program ( sdp ) , which is however computationally expensive , even with the recent advances in interior point methods . based on the equivalence relationship between rkda and least square problems in the binary-class case , we propose a quadratically constrained quadratic programming ( qcqp ) formulation for the kernel learning problem , which can be solved more efficiently than sdp . while most existing work on kernel learning deal with binary-class problems only , we show that our qcqp formulation can be extended naturally to the multi-class case . experimental results on both binary-class and multi-class benchmarkdata sets show the efficacy of the proposed qcqp formulations .

['convex optimization', 'kernel discriminant analysis', 'kernel learning', 'model selection', 'quadratically constrained quadratic programming']


truth discovery with multiple conflicting information providers on the web the world-wide web has become the most important information source for most of us . unfortunately , there is no guarantee for the correctness of information on the web . moreover , different web sites often provide conflicting information on a subject , such as different specifications for the same product . in this paper we propose a new problem called veracity , i.e. , conformity to truth , which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites . we design a general framework for the veracity problem , and invent an algorithm called truthfinder , which utilizes the relationships between web sites and their information , i.e. , a web site is trustworthy if it provides many pieces of true information , and a piece of information is likely to be true if it is provided by many trustworthy web sites . our experiments show that truthfinder successfully finds true facts among conflicting information , and identifies trustworthy web sites better than the popular search engines .

['data quality', 'link analysis', 'web mining']


exploiting duality in summarization with deterministic guarantees summarization is an important task in data mining . a major challenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guarantee , often expressed in terms of a maximum-error metric . histograms and several hierarchical techniques have been proposed for this problem . however , their time and\/or space complexities remain impractically high and depend not only on the data set size n , but also on the space budget b. these handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data . in this paper we develop an alternative methodology that dispels these deficiencies , thanks to a fruitful application of the solution to the dual problem : given a maximum allowed error , determine the minimum-space synopsis that achieves it . compared to the state-of-the-art , our histogram construction algorithm reduces time complexity by ( at least ) a blog2n over logîµ \* factor and our hierarchical synopsis algorithm reduces the complexity by ( at least ) a factor of log2b over logîµ \* + logn in time and b ( 1-log b over log n ) in space , where îµ \* is the optimal error . these complexity advantages offer both a space-efficiency and a scalability that previous approaches lacked . we verify the benefits of our approach in practice by experimentation .

['efficiency', 'histograms', 'miscellaneous', 'synopses', 'wavelets']

synopses NOUN pobj of
histograms NOUN nsubjpass proposed
efficiency NOUN dobj offer

webpage understanding : an integrated approach recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling html elements . however , how to effectively segment and label the text contents inside html elements is still an open problem . since many text contents on a webpage are often text fragments and not strictly grammatical , traditional natural language processing techniques , that typically expect grammatical sentences , are no longer directly applicable . in this paper , we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages . we propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model . in this model , semantic labels of page structure can be leveraged to help text content understanding , and semantic labels ofthe text phrases can be used in page structure understanding tasks such as data record detection . thus , integration of both page structure and text content understanding leads to an integrated solution of webpage understanding . experimental results on research homepage extraction show the feasibility and promise of our approach .

['conditional random fields', 'text processing', 'webpage understanding']


canonicalization of database records using adaptive similarity measures it is becoming increasingly common to construct databases from information automatically culled from many heterogeneous sources . for example , a research publication database can be constructed by automatically extracting titles , authors , and conference information from online papers . a common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways ( e.g. abbreviations , aliases , and misspellings ) . therefore , it can be difficult to construct a single , standard representation to present to the user . we refer to the task of constructing this representation as canonicalization . despite its importance , there is little existing work on canonicalization . in this paper , we explore the use of edit distance measures to construct a canonical representation that is `` central '' in the sense that it is most similar to each of the disparate records . this approach reduces the impact of noisy records on the canonical representation . furthermore , because the user may prefer different styles of canonicalization , we show how different edit distance costs can result in different forms of canonicalization . for example , reducing the cost of character deletions can result in representations that favor abbreviated forms over expanded forms ( e.g. kdd versus conference on knowledge discovery and data mining ) . we describe how to learn these costs from a small amount of manually annotated data using stochastic hill-climbing . additionally , we investigate feature-based methods to learn ranking preferences over canonicalizations . these approaches can incorporate arbitrary textual evidence to select a canonical record . we evaluate our approach on a real-world publications database and show that our learning method results in a canonicalization solution that is robust to errors and easily customizable to user preferences .

['data cleaning', 'information extraction']


mining high dimensional data for classifier knowledge we present in this paper the problem of discovering sets of attribute-value pairs in high dimensional data sets that are of interest not because of co-occurrence alone , but due to their value in serving as cores for potential classifiers of clusters . we present our algorithm in the context of a gene-expression dataset . gene expression data , in most situations , is insufficient for clustering algorithms and any statistical inference because for 6000 + genes , typically only 10s and at most 100s of data points become available . it is difficult to use statistical techniques to design a classifier for such immensely under-specified data . the observed data , though statistically , insufficient contains some information about the domain . our goal is to discover as much information about all potential classifiers as possible from the data and then summarize this knowledge . this summarization provides insights into the composition of potential classifiers . we present here algorithms and methods for mining a high dimensional data set , exemplified by a gene expression data set , for mining such information .

['design methodology', 'high dimensional data', 'pattern recognition']


targeting the right students using data mining

['data mining application in education', 'scoring', 'target selection']


automating exploratory data analysis for efficient data mining

['attribute selection', 'automation', 'encoding', 'transformation']


trajectory clustering with mixtures of regression models

['video']


suppressing model overfitting in mining concept-drifting data streams mining data streams of changing class distributions is important for real-time business decision support . the stream classifier must evolve to reflect the current class distribution . this poses a serious challenge . on the one hand , relying on historical data may increase the chances of learning obsolete models . on the other hand , learning only from the latest data may lead to biased classifiers , as the latest data is often an unrepresentative sample of the current class distribution . the problem is particularly acute in classifying rare events , when , for example , instances of the rare class do not even show up in the most recent training data . in this paper , we use a stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one : from the historical and the current training data that we have observed , find the most-likely current distribution , and learn a classifier based on the most-likely distribution . we derive an analytic solution and approximate this solution with an efficient algorithm , which calibrates the influence of historical data carefully to create an accurate classifier . we evaluate our algorithm with both synthetic and real-world datasets . our results show that our algorithm produces accurate and efficient classification .

['classifier', 'classifier ensemble', 'concept drift', 'data streams']

classifier NOUN nsubj evolve
classifier NOUN dobj learn
classifier NOUN dobj create

the predictive power of online chatter an increasing fraction of the global discourse is migrating online in the form of blogs , bulletin boards , web pages , wikis , editorials , and a dizzying array of new collaborative technologies . the migration has now proceeded to the point that topics reflecting certain individual products are sufficiently popular to allow targeted online tracking of the ebb and flow of chatter around these topics . based on an analysis of around half a million sales rank values for 2,340 books over a period of four months , and correlating postings in blogs , media , and web pages , we are able to draw several interesting conclusions . first , carefully hand-crafted queries produce matching postings whose volume predicts sales ranks . second , these queries can be automatically generated in many cases . and third , even though sales rank motion might be difficult to predict in general , algorithmic predictors can use online postings to successfully predict spikes in sales rank .

['blogs', 'information search and retrieval', 'prediction', 'sales rank', 'time-series analysis']

blogs NOUN pobj of
blogs NOUN pobj in

fast nonlinear regression via eigenimages applied to galactic morphology astronomy increasingly faces the issue of massive , unwieldly data sets . the sloan digital sky survey ( sdss ) ( 11 ) has so far generated tens of millions of images of distant galaxies , of which only a tiny fraction have been morphologically classified . morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image . this is a nonlinear regression problem , whose challenges are threefold , 1 ) blurring of the image caused by atmosphere and mirror imperfections , 2 ) large numbers of local minima , and 3 ) massive data sets . our strategy is to use the eigenimages of the parametric model to form a new feature space , and then to map both target image and the model parameters into this feature space . in this low-dimensional space we search for the best image-to-parameter match . to search the space , we sample it by creating a database of many random parameter vectors ( prototypes ) and mapping them into the feature space . the search problem then becomes one of finding the best prototype match , so the fitting process a nearest-neighbor search . in addition to the savings realized by decomposing the original space into an eigenspace , we can use the fact that the model is a linear sum of functions to reduce the prototypes further : the only prototypes stored are the components of the model function . a modified form of nearest neighbor is used to search among them . additional complications arise in the form of missing data and heteroscedasticity , both of which are addressed with weighted linear regression . compared to existing techniques , speed-ups ach-ieved are between 2 and 3 orders of magnitude . this should enable the analysis of the entire sdss dataset .

['astronomy', 'implementation', 'morphology', 'nearest neighbor', 'pincipal component analysis', 'regression']

regression NOUN compound problem
regression NOUN pobj with

efficient incremental constrained clustering clustering with constraints is an emerging area of data mining research . however , most work assumes that the constraints are given as one large batch . in this paper we explore the situation where the constraints are incrementally given . in this way the user after seeing a clustering can provide positive and negative feedback via constraints to critique a clustering solution . we consider the problem of efficiently updating a clustering to satisfy the new and old constraints rather than reclustering the entire data set . we show that the problem of incremental clustering under constraints is np-hard in general , but identify several sufficient conditions which lead to efficiently solvable versions . these translate into a set of rules on the types of constraints thatcan be added and constraint set properties that must be maintained . we demonstrate that this approach is more efficient than re-clustering the entire data set and has several other advantages .

['clustering', 'complexity', 'constraints']

clustering NOUN nsubj is
clustering NOUN acl clustering
constraints NOUN pobj with
constraints NOUN nsubjpass given
constraints NOUN nsubjpass given
clustering NOUN nsubj provide
constraints NOUN pobj via
clustering NOUN compound solution
clustering NOUN dobj updating
constraints NOUN dobj satisfy
clustering NOUN pobj of
constraints NOUN pobj under
constraints NOUN compound thatcan
clustering VERB amod set

support feature machine for classification of abnormal brain activity in this study , a novel multidimensional time series classification technique , namely support feature machine ( sfm ) , is proposed . sfm is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data . this paper also describes an application of sfm for detecting abnormal brain activity . epilepsy is a case in point in this study . in epilepsy studies , electroencephalograms ( eegs ) , acquired in multidimensional time series format , have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain . from multi-dimensional eeg time series data , sfm was used to identify seizure pre-cursors and detect seizure susceptibility ( pre-seizure ) periods . the empirical results showed that sfm achieved over 80 % correct classification of per-seizure eeg on average in 10 patients using 5-fold cross validation . the proposed optimization model of sfm is very compact and scalable , and can be implemented as an online algorithm . the outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through eeg classification .

['classification', 'epilepsy', 'multi-dimensional time series', 'nearest neighbor', 'optimization']

optimization NOUN compound model
epilepsy NOUN nsubj is
epilepsy NOUN compound studies
classification NOUN dobj achieved
optimization NOUN compound model
classification NOUN pobj through

bias and controversy : beyond the statistical deviation in this paper , we investigate how deviation in evaluation activities may reveal bias on the part of reviewers and controversy on the part of evaluated objects . we focus on a ` data-centric approach ' where the evaluation data is assumed to represent the ` ground truth ' . the standard statistical approaches take evaluation and deviation at face value . we argue that attention should be paid to the subjectivity of evaluation , judging the evaluation score not just on ` what is being said ' ( deviation ) , but also on ` who says it ' ( reviewer ) as well as on ` whom it is said about ' ( object ) . furthermore , we observe that bias and controversy are mutually dependent , as there is more bias if there is higher deviation on a less controversial object . to address this mutual dependency , we propose a reinforcement model to identify bias and controversy . we test our model on real-life data to verify its applicability .

['bias', 'controversy', 'evaluation', 'information systems applications', 'social and behavioral sciences', 'social network']

bias NOUN nsubj investigate
controversy NOUN conj bias
evaluation NOUN compound activities
bias NOUN dobj reveal
controversy NOUN conj reviewers
evaluation NOUN compound data
evaluation NOUN dobj take
evaluation NOUN pobj of
evaluation NOUN compound score
bias NOUN nsubj are
controversy NOUN conj bias
bias NOUN attr is
bias NOUN dobj identify
controversy NOUN conj bias

web object indexing using domain knowledge a web object is defined to represent any meaningful object embedded in web pages ( e.g. images , music ) or pointed to by hyperlinks ( e.g. downloadable files ) . in many cases , users would like to search for information of a certain ` object ' , rather than a web page containing the query terms . to facilitate web object searching and organizing , in this paper , we propose a novel approach to web object indexing , by discovering its inherent structure information with existed domain knowledge . in our approach , first , layered lsi spaces are built for a better representation of the hierarchically structured domain knowledge , in order to emphasize the specific semantics and term space in each layer of the domain knowledge . meanwhile , the web object representation is constructed by hyperlink analysis , and further pruned to remove the noises . then an optimal matching between the web object and the domain knowledge is performed , in order to pick out the structure attributes of the web object from the knowledge . finally , the obtained structure attributes are used to re-organize and index the web objects . our approach also indicates a new promising way to use trust-worthy deep web knowledge to help organize dispersive information of surface web .

['confidence propagation', 'domain knowledge', 'indexing', 'information retrieval', 'latent semantic indexing', 'link analysis', 'music indexing', 'web object']

indexing NOUN pobj to

nonlinear adaptive distance metric learning for clustering a good distance metric is crucial for many data mining tasks . to learn a metric in the unsupervised setting , most metric learning algorithms project observed data to a low-dimensional manifold , where geometric relationships such as pairwise distances are preserved . it can be extended to the nonlinear case by applying the kernel trick , which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space . in this paper , we propose a novel unsupervised nonlinear adaptive metric learning algorithm , called naml , which performs clustering and distance metric learning simultaneously . naml firstmaps the data to a high-dimensional space through a kernel function ; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized ; and finally performs clustering in the low-dimensional space . the performance of naml depends on the selection of the kernel function and the projection . we show that the joint kernel learning , dimensionality reduction , and clustering can be formulated as a trace maximization problem , which can be solved via an iterative procedure in the em framework . experimental results demonstrated the efficacy of the proposed algorithm .

['clustering', 'convex programming', 'distance metric', 'kernel']

clustering VERB pcomp for
kernel NOUN compound trick
kernel NOUN compound function
clustering NOUN amod learning
kernel NOUN compound function
clustering NOUN xcomp performs
kernel NOUN compound function
kernel NOUN compound learning
clustering NOUN conj reduction

when do data mining results violate privacy ? privacy-preserving data mining has concentrated on obtaining valid results when the input data is private . an extreme example is secure multiparty computation-based methods , where only the results are revealed . however , this still leaves a potential privacy breach : do the results themselves violate privacy ? this paper explores this issue , developing a framework under which this question can be addressed . metrics are proposed , along with analysis that those metrics are consistent in the face of apparent problems .

['inference', 'privacy']

privacy NOUN dobj violate
privacy NOUN npadvmod preserving
privacy NOUN compound breach
privacy NOUN dobj violate

detecting time series motifs under uniform scaling time series motifs are approximately repeated patterns foundwithin the data . such motifs have utility for many data mining algorithms , including rule-discovery , novelty-detection , summarization and clustering . since the formalization of the problem and the introduction of efficient linear time algorithms , motif discovery has been successfully applied tomany domains , including medicine , motion capture , robotics and meteorology . in this work we show that most previous applications of time series motifs have been severely limited by the definition 's brittleness to even slight changes of uniform scaling , the speed at which the patterns develop . we introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling , and show that it produces objectively superior results in several important domains . apart from being more general than all other motifdiscovery algorithms , a further contribution of our work isthat it is simpler than previous approaches , in particular we have drastically reduced the number of parameters that need to be specified .

['motifs', 'random projection', 'time series', 'uniform scaling']

time series NOUN nsubj motifs
motifs NOUN ccomp detecting
motifs NOUN pobj under
motifs NOUN nsubj have
motifs NOUN pobj of
motifs NOUN ccomp allows

a probabilistic framework for relational clustering relational clustering has attracted more and more attention due to its phenomenal impact in various important applications which involve multi-type interrelated data objects , such as web mining , search marketing , bioinformatics , citation analysis , and epidemiology . in this paper , we propose a probabilistic model for relational clustering , which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering , semi-supervised clustering , co-clustering and graph clustering . the proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects . under this model , we propose parametric hard and soft relational clustering algorithms under a large number of exponential family distributions . the algorithms are applicable to relational data of various structures and at the same time unifies a number of stat-of-the-art clustering algorithms : co-clustering algorithms , the k-partite graph clustering , bregman k-means , and semi-supervised clustering based on hidden markov random fields .

['relational clustering', 'relational data']


disease progression modeling from historical clinical databases this paper considers the problem of modeling disease progression from historical clinical databases , with the ultimate objective of stratifying patients into groups with clearly distinguishable prognoses or suitability for different treatment strategies . to meet this objective , we describe a procedure that first fits clinical variables measured over time to a disease progression model . the resulting parameter estimates are then used as the basis for a stepwise clustering procedure to stratify patients into groups with distinct survival characteristics . as a practical illustration , we apply this procedure to survival prediction , using a liver transplant database from the national institute of diabetes and digestive and kidney diseases ( niddk ) .

['censoring', 'cluster analysis', 'disease progression modeling', 'logistic model', 'model development', 'niddk liver transplant database']


unweaving a web of documents we develop an algorithmic framework to decompose a collection of time-stamped text documents into semantically coherent threads . our formulation leads to a graph decomposition problem on directed acyclic graphs , for which we obtain three algorithms -- an exact algorithm that is based on minimum cost flow and two more efficient algorithms based on maximum matching and dynamic programming that solve specific versions of the graph decomposition problem . applications of our algorithms include superior summarization of news search results , improved browsing paradigms for large collections of text-intensive corpora , and integration of time-stamped documents from a variety of sources . experimental results based on over 250,000 news articles from a major newspaper over a period of four years demonstrate that our algorithms efficiently identify robust threads of varying lengths and time-spans .

['graph algorithms', 'graph decomposition', 'information search and retrieval', 'news threads']


detection of emerging space-time clusters we propose a new class of spatio-temporal cluster detection methods designed for the rapid detection of emerging space-time clusters . we focus on the motivating application of prospective disease surveillance : detecting space-time clusters of disease cases resulting from an emerging disease outbreak . automatic , real-time detection of outbreaks can enable rapid epidemiological response , potentially reducing rates of morbidity and mortality . building on the prior work on spatial and space-time scan statistics , our methods combine time series analysis ( to determine how many cases we expect to observe for a given spatial region in a given time interval ) with new `` emerging cluster '' space-time scan statistics ( to decide whether an observed increase in cases in a region is significant ) , enabling fast and accurate detection of emerging outbreaks . we evaluate these methods on two types of simulated outbreaks : aerosol release of inhalational anthrax ( e.g. from a bioterrorist attack ) and floo ( `` fictional linear onset outbreak '' ) , injected into actual baseline data ( emergency department records and over-the-counter drug sales data from allegheny county ) . we demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positives low , and show that our new `` emerging cluster '' scan statistics consistently outperform the standard `` persistent cluster '' scan statistics approach .

['biosurveillance', 'cluster detection', 'space-time scan statistics']


a new multi-view regression approach with an application to customer wallet estimation motivated by the problem of customer wallet estimation , we propose a new setting for multi-view regression , where we learn a completely unobserved target ( in our case , customer wallet ) by modeling it as a `` central link '' in a directed graphical model , connecting multiple sets of observed variables . the resulting conditional independence allows us to reduce the maximum discriminative likelihood estimation problem to a convex optimization problem for exponential linear models . we show that under certain modeling assumptions , in particular , when there exist two conditionally independent views and the noise is gaussian , this problem can be reduced to a single least squares regression . thus , for this specific , but widely applicable setting , the `` unsupervised '' multi-view problem can be solved via a simple supervised learning approach . this reduction also allows us to test the statistical independence assumptions underlying the graphical model and perform variable selection . we demonstrate the effectiveness of our approach on our motivating problem of customer wallet estimation and on simulation data .

['bayesian networks', 'learning', 'multi-view learning', 'regression']

regression NOUN compound approach
regression NOUN pobj for
regression NOUN pobj to
learning NOUN compound approach

wavelet synopsis for data streams : minimizing non-euclidean error we consider the wavelet synopsis construction problem for data streams where given n numbers we wish to estimate the data by constructing a synopsis , whose size , say b is much smaller than n. the b numbers are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis . several good one-pass wavelet construction streaming algorithms minimizing the l2 error exist . for other error measures , the problem is less understood . we provide the first one-pass small space streaming algorithms with provable error guarantees ( additive approximation ) for minimizing a variety of non-euclidean error measures including all weighted lp ( including l âˆž ) and relative error lp metrics . in several previous works solutions ( for weighted l2 , l âˆž and maximum relative error ) where the b synopsis coefficients are restricted to be wavelet coefficients of the data were proposed . this restriction yields suboptimal solutions on even fairly simple examples . other lines of research , such as probabilistic synopsis , imposed restrictions on how the synopsis was arrived at . to the best of our knowledge this paper is the first paper to address the general problem , without any restriction on how the synopsis is arrived at , as well as provide the first streaming algorithms with guaranteed performance for these classes of error measures .

['miscellaneous', 'miscellaneous', 'streaming algorithm', 'wavelet synopses']


extracting relevant named entities for automated expense reimbursement expense reimbursement is a time-consuming and labor-intensive process across organizations . in this paper , we present a prototype expense reimbursement system that dramatically reduces the elapsed time and costs involved , by eliminating paper from the process life cycle . our complete solution involves ( 1 ) an electronic submission infrastructure that provides multi - channel image capture , secure transport and centralized storage of paper documents ; ( 2 ) an unconstrained data mining approach to extracting relevant named entities from un-structured document images ; ( 3 ) automation of auditing procedures that enables automatic expense validation with minimum human interaction . extracting relevant named entities robustly from document images with unconstrained layouts and diverse formatting is a fundamental technical challenge to image-based data mining , question answering , and other information retrieval tasks . in many applications that require such capability , applying traditional language modeling techniques to the stream of ocr text does not give satisfactory result due to the absence of linguistic context . we present an approach for extracting relevant named entities from document images by combining rich page layout features in the image space with language content in the ocr text using a discriminative conditional random field ( crf ) framework . we integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collections of real-world receipt images provided by ibm world wide reimbursement center .

['conditional random fields', 'document layout analysis', 'general', 'learning', 'named entity extraction', 'optical character recognition']


practical learning from one-sided feedback in many data mining applications , online labeling feedback is only available for examples which were predicted to belong to the positive class . such applications includespam filtering in the case where users never checkemails marked `` spam '' , document retrieval where users cannotgive relevance feedback on unretrieved documents , and online advertising where user behavior can not beobserved for unshown advertisements . one-sided feedback can cripple the performance of classical mistake-driven online learners such as perceptron . previous work under the apple tasting framework showed how to transform standard online learners into successful learners from one sided feedback . however , we find in practice that this transformation may request more labels than necessary to achieve strong performance . in this paper , we employ two active learning methods which reduce the number of labels requested in practice . one method is the use of label efficient active learning . the other method , somewhat surprisingly , is the use of margin-based learners without modification , which we show combines implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff . experimental results show that these methods can be significantly more effective in practice than those using the apple tasting transformation , even on minority class problems .

['active learning', 'apple tasting', 'general', 'online learning', 'streaming data']


mining distance-based outliers from large databases in any metric space let r be a set of objects . an object o âˆˆ r is an outlier , if there exist less than k objects in r whose distances to o are at most r. the values of k , r , and the distance metric are provided by a user at the run time . the objective is to return all outliers with the smallest i\/o cost . this paper considers a generic version of the problem , where no information is available for outlier computation , except for objects ' mutual distances . we prove an upper bound for the memory consumption which permits the discovery of all outliers by scanning the dataset 3 times . the upper bound turns out to be extremely low in practice , e.g. , less than 1 % of r. since the actual memory capacity of a realistic dbms is typically larger , we develop a novel algorithm , which integrates our theoretical findings with carefully-designed heuristics that leverage the additional memory to improve i\/o efficiency . our technique reports all outliers by scanning the dataset at most twice ( in some cases , even once ) , and significantly outperforms the existing solutions by a factor up to an order of magnitude .

['information search and retrieval', 'metric data', 'mining', 'outlier']

mining NOUN amod outliers
outlier NOUN attr is
outlier ADJ amod computation

feature bagging for outlier detection outlier detection has recently become an important problem in many industrial and financial applications . in this paper , a novel feature bagging approach for detecting outliers in very large , high dimensional and noisy databases is proposed . it combines results from multiple outlier detection algorithms that are applied using different set of features . every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set . as a result , each outlier detector identifies different outliers , and thus assigns to all data records outlier scores that correspond to their probability of being outliers . the outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers . experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm .

['bagging', 'detection rate', 'false alarm', 'feature subsets', 'integration', 'outlier detection']

bagging NOUN nsubj become
bagging NOUN compound approach

cfi-stream : mining closed frequent itemsets in data streams mining frequent closed itemsets provides complete and condensed information for non-redundant association rules generation . extensive studies have been done on mining frequent closed itemsets , but they are mainly intended for traditional transaction databases and thus do not take data stream characteristics into consideration . in this paper , we propose a novel approach for mining closed frequent itemsets over data streams . it computes and maintains closed itemsets online and incrementally , and can output the current closed frequent itemsets in real time based on users ' specified thresholds . experimental results show that our proposed method is both time and space efficient , has good scalability as the number of transactions processed increases and adapts very rapidly to the change in data streams .

['association rules', 'data stream', 'frequent closed itemsets']


failure detection and localization in component based systems by online tracking the increasing complexity of today 's systems makes fast and accurate failure detection essential for their use in mission-critical applications . various monitoring methods provide a large amount of data about system 's behavior . analyzing this data with advanced statistical methods holds the promise of not only detecting the errors faster , but also detecting errors which are difficult to catch with current monitoring tools . two challenges to building such detection tools are : the high dimensionality of observation data , which makes the models expensive to apply , and frequent system changes , which make the models expensive to update . in this paper , we present algorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes . we decompose the observation data into signal and noise subspaces . two statistics , the hotelling t2 score and squared prediction error ( spe ) are calculated to represent the data characteristics in signal and noise subspaces respectively . instead of tracking the original data , we use a sequentially discounting expectation maximization ( sdem ) algorithm to learn the distribution of the two extracted statistics . a failure event can then be detected based on the abnormal change of the distribution . applying our technique to component interaction data in a simple e-commerce application shows better accuracy than building independent profiles for each component . additionally , experiments on synthetic data show that the detection accuracy is high even for changing systems .

['distributed computing', 'failure detection', 'internet services', 'learning', 'online tracking', 'statistics', 'subspace decomposition', 'system management']

statistics NOUN nsubjpass calculated
statistics NOUN pobj of

knowledge base maintenance using knowledge gap analysis as the web and e-business have proliferated , the practice of using customer facing knowledge bases to augment customer service and support operations has increased . this can be a very efficient , scalable and cost effective way to share knowledge . the effectiveness and cost savings are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge . to address this issue , we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required . in this paper , we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem tickets to determine which categories and subcategories are not well addressed within the knowledge base . we utilize text clustering on problem ticket text to determine a set of problem categories . we then compare each knowledge base solution document to each problem category centroid using a cosine distance metric . the distance between the `` closest '' solution document and the corresponding centroid becomes the basis of that problem category 's `` knowledge gap '' . our claim is that this gap metric serves as a useful method for quickly and automatically determining which problem categories have no relevant solutions in a knowledge base . we have implemented our approach , and we present the results of performing a knowledge gap analysis on a set of support center problem tickets .

['clustering', 'clustering', 'database applications', 'gap analysis', 'information search and retrieval', 'knowledge management', 'text mining']

clustering VERB acl text

consistent bipartite graph co-partitioning for star-structured high-order heterogeneous data co-clustering heterogeneous data co-clustering has attracted more and more attention in recent years due to its high impact on various applications . while the co-clustering algorithms for two types of heterogeneous data ( denoted by pair-wise co-clustering ) , such as documents and terms , have been well studied in the literature , the work on more types of heterogeneous data ( denoted by high-order co-clustering ) is still very limited . as an attempt in this direction , in this paper , we worked on a specific case of high-order co-clustering in which there is a central type of objects that connects the other types so as to form a star structure of the inter-relationships . actually , this case could be a very good abstract for many real-world applications , such as the co-clustering of categories , documents and terms in text mining . in our philosophy , we treated such kind of problems as the fusion of multiple pair-wise co-clustering sub-problems with the constraint of the star structure . accordingly , we proposed the concept of consistent bipartite graph co-partitioning , and developed an algorithm based on semi-definite programming ( sdp ) for efficient computation of the clustering results . experiments on toy problems and real data both verified the effectiveness of our proposed method .

['co-clustering', 'consistency', 'high-order heterogeneous data', 'spectral graph']


ilink : search and routing in social networks the growth of web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks . this paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration . we propose a general interaction model for the underlying social networks and then a specific model ( ilink for social search and message routing . a key contribution here is the development of a general learning framework for making such online peer production systems work at scale . the ilink model has been used to develop a system for faq generation in a social network ( faqtory ) , and experience with its application in the context of a full-scale learning-driven workflow application ( calo ) is reported . we also discuss methods of adapting ilink technology for use in military knowledge sharing portals and other message routing systems . finally , the paper shows the connection of ilink to sqm , a theoretical model for social search that is a generalization of markov decision processes and the popular pagerank model .

['expert identification', 'learning', 'learning', 'message routing', 'peer production', 'smart rss', 'social faq generation', 'social search']

learning NOUN compound framework
learning NOUN npadvmod driven

nestedness and segmented nestedness consider each row of a 0-1 dataset as the subset of the columns for which the row has an 1 . then a dataset is nested , if for all pairs of rows one row is either a superset or subset of the other . the concept of nestedness has its origins in ecology , where approximate versions of it has been used to model the species distribution in different locations . we argue that nestedness and its extensions are interesting properties of datasets , and that they can be applied also to domains other than ecology . we first define natural measures of nestedness and study their properties . we then define the concept of k-nestedness : a dataset is ( almost ) k-nested if the set of columns can be partitioned to k parts so that each part is ( almost ) nested . we consider the algorithmic problems of computing how far a dataset is from being k-nested , and for finding a good partition of the columns into k parts . the algorithms are based on spectral partitioning , and scale to moderately large datasets . we apply the methods to real data from ecology and from other applications , and demonstrate the usefulness of the concept .

['0-1 matrices', 'nestedness', 'nonnumerical algorithms and problems', 'presence/absence data']

nestedness ADJ amod nestedness
nestedness NOUN nsubj consider
nestedness NOUN pobj of
nestedness NOUN nsubj are
nestedness NOUN pobj of
nestedness NOUN pobj of

privacy preserving mining of association rules we present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions . while it is feasible to recover association rules and preserve privacy using a straightforward `` uniform '' randomization , the discovered rules can unfortunately be exploited to find privacy breaches . we analyze the nature of privacy breaches and propose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches . we derive formulae for an unbiased support estimator and its variance , which allow us to recover itemset supports from randomized datasets , and show how to incorporate these formulae into mining algorithms . finally , we present experimental results that validate the algorithm by applying it on real datasets .

['deduction', 'unauthorized access']


extracting decision trees from trained neural networks neural networks are successful in acquiring hidden knowledge in datasets . their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans . researchers tried to address this problem by extracting rules from trained neural networks . most of the proposed rule extraction methods required specialized type of neural networks ; some required binary inputs and some were computationally expensive . craven proposed extracting mofn type decision trees from neural networks . we believe mofn type decision trees are only good for mofn type problems and trees created for regular high dimensional real world problems may be very complex . in this paper , we introduced a new method for extracting regular c4 .5 like decision trees from trained neural networks . we showed that the new method ( dectext ) is effective in extracting high fidelity trees from trained networks . we also introduced a new discretization technique to make dectext be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity .

['self-modifying machines']


real-time ranking with concept drift using expert advice in many practical applications , one is interested in generating a ranked list of items using information mined from continuous streams of data . for example , in the context of computer networks , one might want to generate lists of nodes ranked according to their susceptibility to attack . in addition , real-world data streams often exhibit concept drift , making the learning task even more challenging . we present an online learning approach to ranking with concept drift , using weighted majority techniques . by continuously modeling different snapshots of the data and tuning our measure of belief in these models over time , we capture changes in the underlying concept and adapt our predictions accordingly . we measure the performance of our algorithm on real electricity data as well as asynthetic data stream , and demonstrate that our approach to ranking from stream data outperforms previously known batch-learning methods and other online methods that do not account for concept drift .

['concept drift', 'data streams', 'learning', 'miscellaneous', 'online learning', 'ranking']

ranking VERB advcl is
learning VERB compound task
learning NOUN compound approach
ranking VERB pcomp to
ranking VERB pcomp to
learning VERB amod methods

probabilistic query models for transaction data we investigate the application of bayesian networks , markov random fields , and mixture models to the problem of query answering for transaction data sets . we formulate two versions of the querying problem : the query selectivity estimation ( i.e. , finding exact counts for tuples in a data set ) and the query generalization problem ( i.e. , computing the probability that a tuple will occur in new data ) . we show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an adtree data structure . in an extension of our earlier work on this topic we propose several new schemes for query answering based on the compressed representation that avoid direct scans of the data at query time . experimental results on real-world transaction data sets provide insights into various tradeoffs involving the offline time for model-building , the online time for query-answering , the memory footprint of the compressed data , and the accuracy of the estimate provided to the query .

['probabilistic algorithms']


data mining in metric space : an empirical analysis of supervised learning performance criteria many criteria can be used to evaluate the performance of supervised learning . different criteria are appropriate in different settings , and it is not always clear which criteria to use . a further complication is that learning methods that perform well on one criterion may not perform well on other criteria . for example , svms and boosting are designed to optimize accuracy , whereas neural nets typically optimize squared error or cross entropy . we conducted an empirical study using a variety of learning methods ( svms , neural nets , k-nearest neighbor , bagged and boosted trees , and boosted stumps ) to compare nine boolean classification performance metrics : accuracy , lift , f-score , area under the roc curve , average precision , precision\/recall break-even point , squared error , cross entropy , and probability calibration . multidimensional scaling ( mds ) shows that these metrics span a low dimensional manifold . the three metrics that are appropriate when predictions are interpreted as probabilities : squared error , cross entropy , and calibration , lay in one part of metric space far away from metrics that depend on the relative order of the predicted values : roc area , average precision , break-even point , and lift . in between them fall two metrics that depend on comparing predictions to a threshold : accuracy and f-score . as expected , maximum margin methods such as svms and boosted trees have excellent performance on metrics like accuracy , but perform poorly on probability metrics such as squared error . what was not expected was that the margin methods have excellent performance on ordering metrics such as roc area and average precision . we introduce a new metric , sar , that combines squared error , accuracy , and roc area into one metric . mds and correlation analysis shows that sar is centrally located and correlates well with other metrics , suggesting that it is a good general purpose metric to use when more specific criteria are not known .

['cross entropy', 'lift', 'metrics', 'performance evaluation', 'precision', 'recall', 'roc', 'supervised learning']

metrics NOUN dobj compare
lift NOUN conj accuracy
roc NOUN compound curve
precision NOUN conj area
metrics NOUN nsubj span
metrics NOUN nsubj lay
metrics NOUN pobj from
roc NOUN compound area
precision NOUN conj area
lift VERB conj point
metrics NOUN dobj fall
metrics NOUN pobj on
metrics NOUN pobj on
metrics NOUN dobj ordering
roc NOUN compound area
precision NOUN conj area
roc NOUN compound area
metrics NOUN pobj with

active learning using adaptive resampling

['active learning', 'adaptive resampling', 'classification', 'learning', 'machine learning']

learning VERB ROOT learning

a distributed learning framework for heterogeneous data sources we present a probabilistic model-based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse , possibly overlapping subsets of features . our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy-safe probabilistic models of the local data , which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites . we provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution . for certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence , we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem . to address interpretability concerns , we also present a modified formulation where the global model is assumed to belong to a specified parametric family . finally , to highlight the generality of our framework , we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector , categorical and directional attributes . the results show that high quality global models can be obtained without much loss of privacy .

['distributed learning', 'heterogeneous data sources', 'learning', 'privacy', 'probabilistic models']

learning NOUN compound framework
learning NOUN pobj for
privacy NOUN compound restrictions
privacy NOUN compound issues
privacy NOUN npadvmod safe
learning NOUN compound tasks
privacy NOUN pobj of

clope : a fast and effective clustering algorithm for transactional data this paper studies the problem of categorical data clustering , especially for transactional data characterized by high dimensionality and large volume . starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram , we develop a novel algorithm -- clope , which is very fast and scalable , while being quite effective . we demonstrate the performance of our algorithm on two real world datasets , and compare clope with the state-of-art algorithms .

['categorical data', 'scalability']


attack detection in time series for recommender systems recent research has identified significant vulnerabilities in recommender systems . shilling attacks , in which attackers introduce biased ratings in order to influence future recommendations , have been shown to be effective against collaborative filtering algorithms . we postulate that the distribution of item ratings in time can reveal the presence of a wide range of shilling attacks given reasonable assumptions about their duration . to construct a time series of ratings for an item , we use a window size of k to group consecutive ratings for the item into disjoint windows and compute the sample average and sample entropy in each window . we derive a theoretically optimal window size to best detect an attack event if the number of attack profiles is known . for practical applications where this number is unknown , we propose a heuristic algorithm that adaptively changes the window size . our experimental results demonstrate that monitoring rating distributions in time series is an effective approach for detecting shilling attacks .

['anomaly detection', 'recommender systems', 'shilling attacks', 'time series']


simple and effective visual models for gene expression cancer diagnostics in the paper we show that diagnostic classes in cancer gene expression data sets , which most often include thousands of features ( genes ) , may be effectively separated with simple two-dimensional plots such as scatterplot and radviz graph . the principal innovation proposed in the paper is a method called vizrank , which is able to score and identify the best among possibly millions of candidate projections for visualizations . compared to recently much applied techniques in the field of cancer genomics that include neural networks , support vector machines and various ensemble-based approaches , vizrank is fast and finds visualization models that can be easily examined and interpreted by domain experts . our experiments on a number of gene expression data sets show that vizrank was always able to find data visualizations with a small number of ( two to seven ) genes and excellent class separation . in addition to providing grounds for gene expression cancer diagnosis , vizrank and its visualizations also identify small sets of relevant genes , uncover interesting gene interactions and point to outliers and potential misclassifications in cancer data sets .

['cancer diagnosis', 'data mining', 'data visualization', 'gene expression analysis', 'machine learning']


visual classification : an interactive approach to decision tree construction

['decision support']


xrules : an effective structural classifier for xml data xml documents have recently become ubiquitous because of their varied applicability in a number of applications . classification is an important problem in the data mining domain , but current classification methods for xml documents use ir-based methods in which each document is treated as a bag of words . such techniques ignore a significant amount of information hidden inside the documents . in this paper we discuss the problem of rule based classification of xml data by using frequent discriminatory substructures within xml documents . such a technique is more capable of finding the classification characteristics of documents . in addition , the technique can also be extended to cost sensitive classification . we show the effectiveness of the method with respect to other classifiers . we note that the methodology discussed in this paper is applicable to any kind of semi-structured data .

['classification', 'database applications', 'tree mining', 'xml/semi-structured data']

classification NOUN nsubj is
classification NOUN compound methods
classification NOUN pobj of
classification NOUN compound characteristics
classification NOUN dobj cost

a microeconomic data mining problem : customer-oriented catalog segmentation the microeconomic framework for data mining ( 7 ) assumes that an enterprise chooses a decision maximizing the overall utility over all customers where the contribution of a customer is a function of the data available on that customer . in catalog segmentation , the enterprise wants to design k product catalogs of size r that maximize the overall number of catalog products purchased . however , there are many applications where a customer , once attracted to an enterprise , would purchase more products beyond the ones contained in the catalog . therefore , in this paper , we investigate an alternative problem formulation , that we call customer-oriented catalog segmentation , where the overall utility is measured by the number of customers that have at least a specified minimum interest t in the catalogs . we formally introduce the customer-oriented catalog segmentation problem and discuss its complexity . then we investigate two different paradigms to design efficient , approximate algorithms for the customer-oriented catalog segmentation problem , greedy ( deterministic ) and randomized algorithms . since greedy algorithms may be trapped in a local optimum and randomized algorithms crucially depend on a reasonable initial solution , we explore a combination of these two paradigms . our experimental evaluation on synthetic and real data demonstrates that the new algorithms yield catalogs of significantly higher utility compared to classical catalog segmentation algorithms .

['catalog segmentation', 'clustering', 'microeconomic data mining']


nantonac collaborative filtering : recommendation based on order responses a recommender system suggests the items expected to be preferred by the users . recommender systems use collaborative filtering to recommend items by summarizing the preferences of people who have tendencies similar to the user preference . traditionally , the degree of preference is represented by a scale , for example , one that ranges from one to five . this type of measuring technique is called the semantic differential ( sd ) method . web adopted the ranking method , however , rather than the sd method , since the sd method is intrinsically not suited for representing individual preferences . in the ranking method , the preferences are represented by orders , which are sorted item sequences according to the users ' preferences . we here propose some methods to recommed items based on these order responses , and carry out the comparison experiments of these methods .

['collaborative filtering', 'order', 'recommender system']

order NOUN pobj on
order NOUN compound responses

evolutionary algorithms in data mining : multi-objective performance modeling for direct marketing

['database marketing', 'evolutionary computation', 'multiple objectives', 'pareto-optimal models']


evaluation of prediction models for marketing campaigns we consider prediction-model evaluation in the context of marketing-campaign planning . in order to evaluate and compare models with specific campaign objectives in mind , we need to concentrate our attention on the appropriate evaluation-criteria . these should portray the model 's ability to score accurately and to identify the relevant target population . in this paper we discuss some applicable model-evaluation and selection criteria , their relevance for campaign planning , their robustness under changing population distributions , and their employment when constructing confidence intervals . we illustrate our results with a case study based on our experience from several projects .

['confidence intervals', 'marketing campaigns', 'model evaluation', 'performance measures']


identifying prospective customers

['customer prospecting']


idr\/qr : an incremental dimension reduction algorithm via qr decomposition dimension reduction is critical for many database and data mining applications , such as efficient storage and retrieval of high-dimensional data . in the literature , a well-known dimension reduction scheme is linear discriminant analysis ( lda ) . the common aspect of previously proposed lda based algorithms is the use of singular value decomposition ( svd ) . due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in lda , there is little work on designing incremental lda algorithms . in this paper , we propose an lda based incremental dimension reduction algorithm , called idr\/qr , which applies qr decomposition rather than svd . unlike other lda based algorithms , this algorithm does not require the whole data matrix in main memory . this is desirable for large data sets . more importantly , with the insertion of new data items , the idr\/qr algorithm can constrain the computational cost by applying efficient qr-updating techniques . finally , we evaluate the effectiveness of the idr\/qr algorithm in terms of classification accuracy on the reduced dimensional space . our experiments on several real-world data sets reveal that the accuracy achieved by the idr\/qr algorithm is very close to the best possible accuracy achieved by other lda based algorithms . however , the idr\/qr algorithm has much less computational cost , especially when new data items are dynamically inserted .

['dimension reduction', 'incremental learning', 'linear discriminant analysis', 'qr decomposition']


immc : incremental maximum margin criterion subspace learning approaches have attracted much attention in academia recently . however , the classical batch algorithms no longer satisfy the applications on streaming data or large-scale data . to meet this desirability , incremental principal component analysis ( ipca ) algorithm has been well established , but it is an unsupervised subspace learning approach and is not optimal for general classification tasks , such as face recognition and web document categorization . in this paper , we propose an incremental supervised subspace learning algorithm , called incremental maximum margin criterion ( immc ) , to infer an adaptive subspace by optimizing the maximum margin criterion . we also present the proof for convergence of the proposed algorithm . experimental results on both synthetic dataset and real world datasets show that immc converges to the similar subspace as that of batch approach .

['learning', 'linear discriminant analysis', 'maximum margin criterion', 'optimization', 'principal component analysis']

learning NOUN compound approaches
learning NOUN compound approach
learning NOUN compound algorithm

svm selective sampling for ranking with application to data retrieval learning ranking ( or preference ) functions has been a major issue in the machine learning community and has produced many applications in information retrieval . svms ( support vector machines ) - a classification and regression methodology - have also shown excellent performance in learning ranking functions . they effectively learn ranking functions of high generalization based on the `` large-margin '' principle and also systematically support nonlinear ranking by the `` kernel trick '' . in this paper , we propose an svm selective sampling technique for learning ranking functions . svm selective sampling ( or active learning with svm ) has been studied in the context of classification . such techniques reduce the labeling effort in learning classification functions by selecting only the most informative samples to be labeled . however , they are not extendable to learning ranking functions , as the labeled data in ranking is relative ordering , or partial orders of data . our proposed sampling technique effectively learns an accurate svm ranking function with fewer partial orders . we apply our sampling technique to the data retrieval application , which enables fuzzy search on relational databases by interacting with users for learning their preferences . experimental results show a significant reduction of the labeling effort in inducing accurate ranking functions .

['active learning', 'miscellaneous', 'ranking', 'selective sampling', 'support vector machine']

selective sampling ADJ dobj svm
ranking VERB pcomp for
ranking NOUN pobj to
ranking VERB amod functions
ranking ADJ amod functions
ranking VERB ccomp support
ranking ADJ amod functions
ranking ADJ amod functions
ranking NOUN pobj in
ranking VERB amod function
ranking ADJ amod functions

privacy-preservation for gradient descent methods gradient descent is a widely used paradigm for solving many optimization problems . stochastic gradient descent performs a series of iterations to minimize a target function in order to reach a local minimum . in machine learning or data mining , this function corresponds to a decision model that is to be discovered . the gradient descent paradigm underlies many commonly used techniques in data mining and machine learning , such as neural networks , bayesian networks , genetic algorithms , and simulated annealing . to the best of our knowledge , there has not been any work that extends the notion of privacy preservation or secure multi-party computation to gradient-descent-based techniques . in this paper , we propose a preliminary approach to enable privacy preservation in gradient descent methods in general and demonstrate its feasibility in specific gradient descent methods .

['gradient descent method', 'privacy preservation', 'regression', 'secure multi-party computation']


hierarchical model-based clustering of large datasets through fractionation and refractionation the goal of clustering is to identify distinct groups in a dataset . compared to non-parametric clustering methods like complete linkage , hierarchical model-based clustering has the advantage of offering a way to estimate the number of groups present in the data . however , its computational cost is quadratic in the number of items to be clustered , and it is therefore not applicable to large problems . we review an idea called fractionation , originally conceived by cutting , karger , pedersen and tukey for non-parametric hierarchical clustering of large datasets , and describe an adaptation of fractionation to model-based clustering . a further extension , called refractionation , leads to a procedure that can be successful even in the difficult situation where there are large numbers of small groups .

['clustering', 'fractionation', 'model-based clustering', 'refractionation']

clustering NOUN nsubj is
fractionation NOUN pobj through
refractionation NOUN conj clustering
clustering NOUN pobj of
clustering NOUN compound methods
clustering NOUN pobj like
fractionation NOUN oprd called
clustering NOUN pobj for
fractionation NOUN pobj of
clustering NOUN pobj to
refractionation NOUN oprd called

k-ttp : a new privacy model for large-scale distributed environments secure multiparty computation allows parties to jointly compute a function of their private inputs without revealing anything but the output . theoretical results ( 2 ) provide a general construction of such protocols for any function . protocols obtained in this way are , however , inefficient , and thus , practically speaking , useless when a large number of participants are involved . the contribution of this paper is to define a new privacy model -- k-privacy -- by means of an innovative , yet natural generalization of the accepted trusted third party model . this allows implementing cryptographically secure efficient primitives for real-world large-scale distributed systems . as an example for the usefulness of the proposed model , we employ k-privacy to introduce a technique for obtaining knowledge -- by way of an association-rule mining algorithm -- from large-scale data grids , while ensuring that the privacy is cryptographically secure .

['association rule mining', 'distributed data mining', 'privacy', 'privacy-preserving data mining']

privacy NOUN compound model
privacy NOUN compound model
privacy NOUN appos model
privacy NOUN dobj employ
privacy NOUN nsubj is

joint latent topic models for text and citations in this work , we address the problem of joint modeling of text and citations in the topic modeling framework . we present two different models called the pairwise-link-lda and the link-plsa-lda models . the pairwise-link-lda model combines the ideas of lda ( 4 ) and mixed membership block stochastic models ( 1 ) and allows modeling arbitrary link structure . however , the model is computationally expensive , since it involves modeling the presence or absence of a citation ( link ) between every pair of documents . the second model solves this problem by assuming that the link structure is a bipartite graph . as the name indicates , link-plsa-lda model combines the lda and plsa models into a single graphical model . our experiments on a subset of citeseer data show that both these models are able to predict unseen data better than the baseline model of erosheva and lafferty ( 8 ) , by capturing the notion of topical similarity between the contents of the cited and citing documents . our experiments on two different data sets on the link prediction task show that the link-plsa-lda model performs the best on the citation prediction task , while also remaining highly scalable . in addition , we also present some interesting visualizations generated by each of the models .

['citations', 'hyperlinks', 'influence', 'lda', 'learning', 'plsa', 'topic models', 'variational inference']

citations NOUN conj text
citations NOUN conj text
lda NOUN oprd called
plsa NOUN compound lda
lda NOUN compound models
lda NOUN compound model
lda NOUN pobj of
plsa NOUN compound lda
lda NOUN compound model
lda NOUN nmod models
plsa NOUN conj lda
plsa NOUN compound lda
lda NOUN compound model

a probabilistic framework for semi-supervised clustering unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints , i.e. , pairs of instances labeled as belonging to same or different clusters . in recent years , a number of algorithms have been proposed for enhancing clustering quality by employing such supervision . such methods use the constraints to either modify the objective function , or to learn the distance measure . we propose a probabilistic model for semi-supervised clustering based on hidden markov random fields ( hmrfs ) that provides a principled framework for incorporating supervision into prototype-based clustering . the model generalizes a previous approach that combines constraints and euclidean distance learning , and allows the use of a broad range of clustering distortion measures , including bregman divergences ( e.g. , euclidean distance and i-divergence ) and directional similarity measures ( e.g. , cosine similarity ) . we present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the hmrf model . experimental results on several text data sets demonstrate the advantages of the proposed framework .

['distance metric learning', 'hidden markov random fields', 'learning', 'semi-supervised clustering']

learning NOUN conj constraints

a general probabilistic framework for clustering individuals and objects

['em algorithm', 'mixture models']


mining complex models from arbitrarily large databases in constant time in this paper we propose a scaling-up method that is applicable to essentially any induction algorithm based on discrete search . the result of applying the method to an algorithm is that its running time becomes independent of the size of the database , while the decisions made are essentially identical to those that would be made given infinite data . the method works within pre-specified memory limits and , as long as the data is iid , only requires accessing it sequentially . it gives anytime results , and can be used to produce batch , stream , time-changing and active-learning versions of an algorithm . we apply the method to learning bayesian networks , developing an algorithm that is faster than previous ones by orders of magnitude , while achieving essentially the same predictive performance . we observe these gains on a series of large databases `` generated from benchmark networks , on the kdd cup 2000 e-commerce data , and on a web log containing 100 million requests .

['bayesian networks', 'discrete search', 'hoeffding bounds', 'scalable learning algorithms', 'subsampling']


bursty and hierarchical structure in streams a fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time . e-mail and news articles are two natural examples of such streams , each characterized by topics that appear , grow in intensity for a period of time , and then fade away . the published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale . underlying much of the text mining work in this area is the following intuitive premise -- that the appearance of a topic in a document stream is signaled by a `` burst of activity , '' with certain features rising sharply in frequency as the topic emerges . the goal of the present work is to develop a formal approach for modeling such `` bursts , '' in such a way that they can be robustly and efficiently identified , and can provide an organizational framework for analyzing the underlying content . the approach is based on modeling the stream using an infinite-state automaton , in which bursts appear naturally as state transitions ; in some ways , it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic . the resulting algorithms are highly efficient , and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream . experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them .

['probabilistic algorithms']


a two-way visualization method for clustered data we describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low-dimensional embedding , thereby gaining the benefits of both approaches . in a single image one can view all the clusters , examine the relations between them and study many of their properties . the method is based on an algorithm for low-dimensional embedding of clustered data , with the property that separation between all clusters is guaranteed , regardless of their nature . in particular , the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data , so that every two disjoint clusters in the hierarchy are drawn separately .

['dendrogram', 'hierarchical clustering', 'information visualization']

dendrogram NOUN dobj superimposes

mining progressive confident rules many real world objects have states that change over time . by tracking the state sequences of these objects , we can study their behavior and take preventive measures before they reach some undesirable states . in this paper , we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state . we give a formal definition of progressive confident rules and their concise set . we devise pruning strategies to reduce the enormous search space . experiment result shows that the proposed algorithm is efficient and scalable . we also demonstrate the application of progressive confident rules in classification .

['classification', 'progressive confident', 'sequence']

classification NOUN pobj in

detecting anomalous records in categorical datasets we consider the problem of detecting anomalies in high aritycategorical datasets . in most applications , anomalies are defined as datapoints that are `` abnormal '' . quite often we have access to data which consists mostly of normal records , a long with a small percentage of unlabelled anomalous records . we are interested in the problem of unsupervised anomaly detection , where we use the unlabelled data for training , and detect records that do not follow the definition of normality . a standard approach is to create a model of normal data , and compare test records against it . a probabilistic approach builds a likelihood model from the training data . records are tested for anomalies based on the complete record likelihood given the probability model . for categorical attributes , bayes nets give a standard representation of the likelihood . while this approach is good at finding outliers in the dataset , it often tends to detect records with attribute values that are rare . sometimes , just detecting rare values of an attribute is not desired and such outliers are not considered as anomalies in that context . we present an alternative definition of anomalies , and propose an approach of comparing against marginal distribution of attribute subsets . we show that this is a more meaningful way of detecting anomalies , and has a better performance over semi-synthetic as well as real world datasets .

['anomaly detection', 'machine learning']


active exploration for learning rankings from clickthrough data we address the task of learning rankings of documents from search enginelogs of user behavior . previous work on this problem has relied onpassively collected clickthrough data . in contrast , we show that anactive exploration strategy can provide data that leads to much fasterlearning . specifically , we develop a bayesian approach for selectingrankings to present users so that interactions result in more informativetraining data . our results using the trec-10 web corpus , as well assynthetic data , demonstrate that a directed exploration strategy quicklyleads to users being presented improved rankings in an online learningsetting . we find that active exploration substantially outperformspassive observation and random exploration .

['active exploration', 'clickthrough data', 'information search and retrieval', 'learning to rank', 'web search']


summarizing itemset patterns using probabilistic models in this paper , we propose a novel probabilistic approach to summarize frequent itemset patterns . such techniques are useful for summarization , post-processing , and end-user interpretation , particularly for problems where the resulting set of patterns are huge . in our approach items in the dataset are modeled as random variables . we then construct a markov random fields ( mrf ) on these variables based on frequent itemsets and their occurrence statistics . the summarization proceeds in a level-wise iterative fashion . occurrence statistics of itemsets at the lowest level are used to construct an initial mrf . statistics of itemsets at the next level can then be inferred from the model . we use those patterns whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner , repeating the procedure until all frequent itemsets can be modeled . the resulting mrf model affords a concise and useful representation of the original collection of itemsets . extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approaches .

['itemset pattern summarization', 'markov random field', 'probabilistic graphical model']


robust boosting and its relation to bagging several authors have suggested viewing boosting as a gradient descent search for a good fit in function space . at each iteration observations are re-weighted using the gradient of the underlying loss function . we present an approach of weight decay for observation weights which is equivalent to `` robustifying '' the underlying loss function . at the extreme end of decay this approach converges to bagging , which can be viewed as boosting with a linear underlying loss function . we illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and `` huberizing '' -- a statistical method for making loss functions more robust .

['bagging', 'boosting', 'design methodology', 'robust fitting']

boosting NOUN nsubj suggested
bagging VERB pcomp to
boosting VERB xcomp viewing
bagging NOUN pobj to
boosting VERB pcomp as

adversarial learning many classification tasks , such as spam filtering , intrusion detection , and terrorism detection , are complicated by an adversary who wishes to avoid detection . previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier ( 2 ) . in this paper , we introduce the adversarial classifier reverse engineering ( acre ) learning problem , the task of learning sufficient information about a classifier to construct adversarial attacks . we present efficient algorithms for reverse engineering linear classifiers with either continuous or boolean features and demonstrate their effectiveness using real data from the domain of spam filtering .

['adversarial classification', 'linear classifiers', 'miscellaneous', 'spam']

spam NOUN nsubj filtering
spam NOUN pobj of

classification features for attack detection in collaborative recommender systems collaborative recommender systems are highly vulnerable to attack . attackers can use automated means to inject a large number of biased profiles into such a system , resulting in recommendations that favor or disfavor given items . since collaborative recommender systems must be open to user input , it is difficult to design a system that can not be so attacked . researchers studying robust recommendation have therefore begun to identify types of attacks and study mechanisms for recognizing and defeating them . in this paper , we propose and study different attributes derived from user profiles for their utility in attack detection . we show that a machine learning classification approach that includes attributes derived from attack models is more successful than more generalized detection algorithms previously studied .

['attack detection', 'collaborative filtering', 'information search and retrieval', 'learning', 'recommender systems', 'robustness', 'systems and software']

learning VERB amod approach

clustering moving objects due to the advances in positioning technologies , the real time information of moving objects becomes increasingly available , which has posed new challenges to the database research . as a long-standing technique to identify overall distribution patterns in data , clustering has achieved brilliant successes in analyzing static datasets . in this paper , we study the problem of clustering moving objects , which could catch interesting pattern changes during the motion process and provide better insight into the essence of the mobile data points . in order to catch the spatial-temporal regularities of moving objects and handle large amounts of data , micro-clustering ( 20 ) is employed . efficient techniques are proposed to keep the moving micro-clusters geographically small . important events such as the collisions among moving micro-clusters are also identified . in this way , high quality moving micro-clusters are dynamically maintained , which leads to fast and competitive clustering result at any given time instance . we validate our approaches with a through experimental evaluation , where orders of magnitude improvement on running time is observed over normal k-means clustering method ( 14 ) .

['clustering', 'micro-cluster', 'moving object']

clustering NOUN advcl becomes
clustering NOUN nsubj achieved
clustering NOUN pcomp of
clustering NOUN conj data
clustering NOUN compound result
clustering NOUN compound method

imds : intelligent malware detection system the proliferation of malware has presented a serious threat to the security of computer systems . traditional signature-based anti-virus systems fail to detect polymorphic and new , previously unseen malicious executables . in this paper , resting on the analysis of windows api execution sequences called by pe files , we develop the intelligent malware detection system ( imds ) using objective-oriented association ( ooa ) mining based classification . imds is an integrated system consisting of three major modules : pe parser , ooa rule generator , and rule based classifier . an ooa algorithm is adapted to efficiently generate ooa rules for classification . a comprehensive experimental study on a large collection of pe files obtained from the anti-virus laboratory of king-soft corporation is performed to compare various malware detection approaches . promising experimental results demonstrate that the accuracy and efficiency of our imds system out perform popular anti-virus software such as norton antivirus and mcafee virusscan , as well as previous data mining based detection systems which employed naive bayes , support vector machine ( svm ) and decision tree techniques .

['learning', 'malware', 'ooa mining', 'pe file', 'windows api sequence']

malware NOUN amod system
malware NOUN pobj of
malware NOUN compound system
malware NOUN compound detection

a generative probabilistic approach to visualizing sets of symbolic sequences there is a notable interest in extending probabilistic generative modeling principles to accommodate for more complex structured data types . in this paper we develop a generative probabilistic model for visualizing sets of discrete symbolic sequences . the model , a constrained mixture of discrete hidden markov models , is a generalization of density-based visualization methods previously developed for static data sets . we illustrate our approach on sequences representing web-log data and chorals by j.s. bach .

['em algorithm', 'hidden markov model', 'information search and retrieval', 'latent space models', 'topographic mapping']


on-board analysis of uncalibrated data for a spacecraft at mars analyzing data on-board a spacecraft as it is collected enables several advanced spacecraft capabilities , such as prioritizing observations to make the best use of limited bandwidth and reacting to dynamic events as they happen . in this paper , we describe how we addressed the unique challenges associated with on-board mining of data as it is collected : uncalibrated data , noisy observations , and severe limitations on computational and memory resources . the goal of this effort , which falls into the emerging application area of spacecraft-based data mining , was to study three specific science phenomena on mars . following previous work that used a linear support vector machine ( svm ) on-board the earth observing 1 ( eo-1 ) spacecraft , we developed three data mining techniques for use on-board the mars odyssey spacecraft . these methods range from simple thresholding to state-of-the-art reduced-set svm technology . we tested these algorithms on archived data in a flight software testbed . we also describe a significant , serendipitous science discovery of this data mining effort : the confirmation of a water ice annulus around the north polar cap of mars . we conclude with a discussion on lessons learned in developing algorithms for use on-board a spacecraft .

['lessons learned', 'on-board data mining', 'real-time data analysis', 'resource-constrained computing']


a general approach to incorporate data quality matrices into data mining algorithms data quality is a central issue for many information-oriented organizations . recent advances in the data quality field reflect the view that a database is the product of a manufacturing process . while routine errors , such as non-existent zip codes , can be detected and corrected using traditional data cleansing tools , many errors systemic to the manufacturing process can not be addressed . therefore , the product of the data manufacturing process is an imprecise recording of information about the entities of interest ( i.e. customers , transactions or assets ) . in this way , the database is only one ( flawed ) version of the entities it is supposed to represent . quality assurance systems such as motorola 's six-sigma and other continuous improvement methods document the data manufacturing process 's shortcomings . a widespread method of documentation is quality matrices . in this paper , we explore the use of the readily available data quality matrices for the data mining classification task . we first illustrate that if we do not factor in these quality matrices , then our results for prediction are sub-optimal . we then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance .

['classification', 'data quality', 'decision trees', 'ensemble approaches', 'six-sigma']

classification NOUN compound task

on mining cross-graph quasi-cliques joint mining of multiple data sets can often discover interesting , novel , and reliable patterns which can not be obtained solely from any single source . for example , in cross-market customer segmentation , a group of customers who behave similarly in multiple markets should be considered as a more coherent and more reliable cluster than clusters found in a single market . as another example , in bioinformatics , by joint mining of gene expression data and protein interaction data , we can find clusters of genes which show coherent expression patterns and also produce interacting proteins . such clusters may be potential pathways . in this paper , we investigate a novel data mining problem , mining cross-graph quasi-cliques , which is generalized from several interesting applications such as cross-market customer segmentation and joint mining of gene expression data and protein interaction data . we build a general model for mining cross-graph quasi-cliques , show why the complete set of cross-graph quasi-cliques can not be found by previous data mining methods , and study the complexity of the problem . while the problem is difficult , we develop an efficient algorithm , crochet , which exploits several interesting and effective techniques and heuristics to efficaciously mine cross-graph quasi-cliques . a systematic performance study is reported on both synthetic and real data sets . we demonstrate some interesting and meaningful cross-graph quasi-cliques in bioinformatics . the experimental results also show that algorithm crochet is efficient and scalable .

['bioinformatics', 'graph mining', 'patterns']

patterns NOUN conj novel
bioinformatics NOUN pobj in
patterns NOUN dobj show
bioinformatics NOUN pobj in

a large-scale analysis of query logs for assessing personalization opportunities query logs , the patterns of activity left by millions of users , contain a wealth of information that can be mined to aid personalization . we perform a large-scale study of yahoo ! search engine logs , tracking 1.35 million browser-cookies over a period of 6 months . we define metrics to address questions such as 1 ) how much history is available ? , 2 ) how do users ' topical interests vary , as reflected by their queries ? , and 3 ) what can we learn from user clicks ? we find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user . we show that users exhibit consistent topical interests that vary between users . we also see that user clicks indicate a variety of special interests . our findings shed light on user activity and can inform future personalization efforts .

['categorization', 'clustering', 'miscellaneous', 'personalization', 'query logs', 'user history', 'user interests']

personalization NOUN compound opportunities
personalization NOUN dobj aid
personalization NOUN compound efforts

pva : a self-adaptive personal view agent system in this paper , we present pva , an adaptive personal view information agent system to track , learn and manage , user 's interests in internet documents . when user 's interests change , pva , in not only the contents , but also in the structure of user profile , is modified to adapt to the changes . experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems .

['machine learning', 'personal view', 'personalization', 'systems and software', 'user interfaces', 'www']

personalization NOUN compound systems

an objective evaluation criterion for clustering we propose and test an objective criterion for evaluation of clustering performance : how well does a clustering algorithm run on unlabeled data aid a classification algorithm ? the accuracy is quantified using the pac-mdl bound ( 3 ) in a semisupervised setting . clustering algorithms which naturally separate the data according to ( hidden ) labels with a small number of clusters perform well . a simple extension of the argument leads to an objective model selection method . experimental results on text analysis datasets demonstrate that this approach empirically results in very competitive bounds on test set performance on natural datasets .

['clustering', 'clustering', 'evaluation', 'mdl', 'pac bounds']

evaluation NOUN compound criterion
clustering NOUN pcomp for
evaluation NOUN pobj for
clustering NOUN amod performance
clustering VERB amod algorithm
mdl NOUN compound bound
clustering VERB csubj perform

gpca : an efficient dimension reduction scheme for image compression and retrieval recent years have witnessed a dramatic increase in the quantity of image data collected , due to advances in fields such as medical imaging , reconnaissance , surveillance , astronomy , multimedia etc. . with this increase has come the need to be able to store , transmit , and query large volumes of image data efficiently . a common operation on image databases is the retrieval of all images that are similar to a query image . for this , the images in the database are often represented as vectors in a high-dimensional space and a query is answered by retrieving all image vectors that are proximal to the query image in this space , under a suitable similarity metric . to overcome problems associated with high dimensionality , such as high storage and retrieval times , a dimension reduction step is usually applied to the vectors to concentrate relevant information in a small number of dimensions . principal component analysis ( pca ) is a well-known dimension reduction scheme . however , since it works with vectorized representations of images , pca does not take into account the spatial locality of pixels in images . in this paper , a new dimension reduction scheme , called generalized principal component analysis ( gpca ) , is presented . this scheme works directly with images in their native state , as two-dimensional matrices , by projecting the images to a vector space that is the tensor product of two lower-dimensional vector spaces . experiments on databases of face images show that , for the same amount of storage , gpca is superior to pca in terms of quality of the compressed images , query precision , and computational cost .

['dimension reduction', 'image compression', 'information search and retrieval', 'principal component analysis', 'singular value decomposition', 'tensor product', 'vector space']


1-dimensional splines as building blocks for improving accuracy of risk outcomes models transformation of both the response variable and the predictors is commonly used in fitting regression models . however , these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors , especially when there are non-linear relationships between predictors and the response such as the medical data set used in this study . a spline based transformation method is proposed that is second order smooth , continuous , and minimizes the mean squared error between the response and each predictor . since the computation time for generating this spline is o ( n ) , the processing time is reasonable with massive data sets . in contrast to cubic smoothing splines , the resulting transformation equations also display a high level of efficiency for scoring . data used for predicting health outcomes contains an abundance of non-linear relationships between predictors and the outcomes requiring an algorithm for modeling them accurately . thus , a transformation that fits an adaptive cubic spline to each of a set of variables is proposed . these curves are used as a set of transformation functions on the predictors . a case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented . the results show significant improvement over the performance of the original variables in both linear and non-linear models .

['adaptive', 'data mining', 'learning', 'linear model', 'outcomes', 'prediction', 'risk', 'spline', 'variable transformation']

spline NOUN npadvmod based
spline NOUN dobj generating
outcomes NOUN dobj predicting
outcomes NOUN conj predictors
adaptive ADJ amod spline
spline NOUN dobj fits
risk NOUN compound outcomes
outcomes NOUN nsubjpass presented

discovery of climate indices using clustering to analyze the effect of the oceans and atmosphere on land climate , earth scientists have developed climate indices , which are time series that summarize the behavior of selected regions of the earth 's oceans and atmosphere . in the past , earth scientists have used observation and , more recently , eigenvalue analysis techniques , such as principal components analysis ( pca ) and singular value decomposition ( svd ) , to discover climate indices . however , eigenvalue techniques are only useful for finding a few of the strongest signals . furthermore , they impose a condition that all discovered signals must be orthogonal to each other , making it difficult to attach a physical interpretation to them . this paper presents an alternative clustering-based methodology for the discovery of climate indices that overcomes these limitiations and is based on clusters that represent regions with relatively homogeneous behavior . the centroids of these clusters are time series that summarize the behavior of the ocean or atmosphere in those regions . some of these centroids correspond to known climate indices and provide a validation of our methodology ; other centroids are variants of known indices that may provide better predictive power for some land areas ; and still other indices may represent potentially new earth science phenomena . finally , we show that cluster based indices generally outperform svd derived indices , both in terms of area weighted correlation and direct correlation with the known indices .

['clustering', 'clustering', 'earth science data', 'mining scientific data', 'singular value decomposition', 'time series']

clustering NOUN dobj using
clustering NOUN npadvmod based

intelliclean : a knowledge-based intelligent data cleaner

['learning']


reducing the human overhead in text categorization many applications in text processing require significant human effort for either labeling large document collections ( when learning statistical models ) or extrapolating rules from them ( when using knowledge engineering ) . in this work , we describe away to reduce this effort , while retaining the methods ' accuracy , by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text patterns to complement machine learning . using a standard sentiment-classification dataset and real customer feedback data , we demonstrate that the resulting technique results in significant reduction of the human effort required to obtain a given classification accuracy . moreover , the hybrid text classifier also results in a significant boost in accuracy over machine-learning based classifiers when a comparable amount of labeled data is used .

['active learning', 'classification', 'machine learning', 'supervised learning', 'support vector machines', 'text classification', 'text mining']

classification NOUN compound dataset
classification NOUN compound accuracy

regularized discriminant analysis for high dimensional , low sample size data linear and quadratic discriminant analysis have been used widely in many areas of data mining , machine learning , and bioinformatics . friedman proposed a compromise between linear and quadratic discriminant analysis , called regularized discriminant analysis ( rda ) , which has been shown to be more flexible in dealing with various class distributions . rda applies the regularization techniques by employing two regularization parameters , which are chosen to jointly maximize the classification performance . the optimal pair of parameters is commonly estimated via cross-validation from a set of candidate pairs . it is computationally prohibitive for high dimensional data , especially when the candidate set is large , which limits the applications of rda to low dimensional data . in this paper , a novel algorithm for rda is presented for high dimensional data . it can estimate the optimal regularization parameters from a large set of parameter candidates efficiently . experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency , and also show that , for a properly chosen pair of regularization parameters , rda performs favorably in classification , in comparison with other existing classification methods .

['cross-validation', 'dimensionality reduction', 'quadratic discriminant analysis', 'regularization']

regularization NOUN compound techniques
regularization NOUN compound parameters
regularization NOUN compound parameters
regularization NOUN compound parameters

experiments with random projections for machine learning dimensionality reduction via random projections has attracted considerable attention in recent years . the approach has interesting theoretical underpinnings and offers computational advantages . in this paper we report a number of experiments to evaluate random projections in the context of inductive supervised learning . in particular , we compare random projections and pca on a number of different datasets and using different machine learning methods . while we find that the random projection approach predictively underperforms pca , its computational advantages may make it attractive for certain applications .

['dimensionality reduction', 'pattern recognition', 'probability and statistics', 'random projection']


data selection for support vector machine classifiers

['concave minimization', 'data classification', 'data selection', 'support vector machines']


efficient search for association rules

['association rule', 'information search and retrieval', 'learning', 'miscellaneous', 'search']

search NOUN ROOT search

cross-language information retrieval using parafac2 a standard approach to cross-language information retrieval ( clir ) uses latent semantic analysis ( lsa ) in conjunction with a multilingual parallel aligned corpus . this approach has been shown to be successful in identifying similar documents across languages - or more precisely , retrieving the most similar document in one language to a query in another language . however , the approach has severe drawbacks when applied to a related task , that of clustering documents `` language-independently '' , so that documents about similar topics end up closest to one another in the semantic space regardless of their language . the problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language , but on the same topic . as a result , when using multilingual lsa , documents will in practice cluster by language , not by topic . we propose a novel application of parafac2 ( which is a variant of parafac , a multi-way generalization of the singular value decomposition ( svd ) ) to overcome this problem . instead of forming a single multilingual term-by-document matrix which , under lsa , is subjected to svd , we form an irregular three-way array , each slice of which is a separate term-by-document matrix for a single language in the parallel corpus . the goal is to compute an svd for each language such that v ( the matrix of right singular vectors ) is the same across all languages . effectively , parafac2 imposes the constraint , not present in standard lsa , that the `` concepts '' in all documents in the parallel corpus are the same regardless of language . intuitively , this constraint makes sense , since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations . we tested this approach by comparing the performance of parafac2 with standard lsa in solving a particular clir problem . from our results , we conclude that parafac2 offers a very promising alternative to lsa not only for multilingual document clustering , but also for solving other problems in cross-language information retrieval .

['information retrieval', 'latent semantic analysis', 'multilingual', 'parafac2', 'performance evaluation']

parafac2 NOUN dative using
multilingual ADJ amod corpus
multilingual ADJ amod lsa
parafac2 NOUN pobj of
multilingual ADJ amod matrix
parafac2 NOUN nsubj imposes
parafac2 NOUN pobj of
parafac2 NOUN nsubj offers
multilingual ADJ amod document

mining the space of graph properties existing data mining algorithms on graphs look for nodes satisfying specific properties , such as specific notions of structural similarity or specific measures of link-based importance . while such analyses for predetermined properties can be effective in well-understood domains , sometimes identifying an appropriate property for analysis can be a challenge , and focusing on a single property may neglect other important aspects of the data . in this paper , we develop a foundation for mining the properties themselves . we present a theoretical framework defining the space of graph properties , a variety of mining queries enabled by the framework , techniques to handle the enormous size of the query space , and an experimental system called f-miner that demonstrates the utility and feasibility of property mining .

['graph mining']


redundancy based feature selection for microarray data in gene expression microarray data analysis , selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes . the problem becomes particularly challenging due to the large number of features ( genes ) and small sample size . traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes . latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy . hence , we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes . the efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets .

['feature redundancy', 'gene selection', 'microarray data']


a general model for clustering binary data clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes . this paper studies the problem of clustering binary data . this is the case for market basket datasets where the transactions contain items and for document datasets where the documents contain `` bag of words '' . the contribution of the paper is three-fold . first a general binary data clustering model is presented . the model treats the data and features equally , based on their symmetric association relations , and explicitly describes the data assignments as well as feature assignments . we characterize several variations with different optimization procedures for the general model . second , we also establish the connections between our clustering model with other existing clustering methods . third , we also discuss the problem for determining the number of clusters for binary clustering . experimental results show the effectiveness of the proposed clustering model .

['binary data', 'clustering', 'clustering', 'general model', 'matrix approximation']

clustering VERB pcomp of
clustering NOUN compound model
clustering NOUN compound model
clustering NOUN compound methods
clustering NOUN pobj for
clustering NOUN compound model

incspan : incremental mining of sequential patterns in large database many real life sequence databases grow incrementally . it is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow , or when some new sequences are added into the database . incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates . however , it is nontrivial to mine sequential patterns incrementally , especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones . in this study , we develop an efficient algorithm , incspan , for incremental mining of sequential patterns , by exploring some interesting properties . our performance study shows that incspan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin .

['buffering pattern', 'incremental mining', 'reverse pattern matching', 'shared projection']


deriving marketing intelligence from online discussion weblogs and message boards provide online forums for discussion that record the voice of the public . woven into this mass of discussion is a wide range of opinion and commentary about consumer products . this presents an opportunity for companies to understand and respond to the consumer by analyzing this unsolicited feedback . given the volume , format and content of the data , the appropriate approach to understand this data is to use large-scale web and text data mining technologies . this paper argues that applications for mining large volumes of textual data for marketing intelligence should provide two key elements : a suite of powerful mining and visualization technologies and an interactive analysis environment which allows for rapid generation and testing of hypotheses . this paper presents such a system that gathers and annotates online discussion relating to consumer products using a wide variety of state-of-the-art techniques , including crawling , wrapping , search , text classification and computational linguistics . marketing intelligence is derived through an interactive analysis framework uniquely configured to leverage the connectivity and content of annotated online discussion .

['computational linguistics', 'content systems', 'information retrieval', 'information search and retrieval', 'machine learning', 'text mining']


event summarization for system management in system management applications , an overwhelming amount of data are generated and collected in the form of temporal events . while mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts , users of the applications are overwhelmed by the mining results . the extracted patterns are generally of large volume and hard to interpret , they may be of no emphasis , intricate and meaningless to non-experts , even to domain experts . while traditional research efforts focus on finding interesting patterns , in this paper , we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data . event summarization aims at providing a concise interpretation of the seemingly chaotic data , so that domain experts may take actions upon the summarized models . event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset .

['ern', 'event summarization', 'log', 'temporal dependency']


a framework for community identification in dynamic social networks we propose frameworks and algorithms for identifying communities in social networks that change over time . communities are intuitively characterized as `` unusually densely knit '' subsets of a social network . this notion becomes more problematic if the social interactions change over time . aggregating social networks over time can radically misrepresent the existing and changing community structure . instead , we propose an optimization-based approach for modeling dynamic community structure . we prove that finding the most explanatory community structure is np-hard and apx-hard , and propose algorithms based on dynamic programming , exhaustive search , maximum matching , and greedy heuristics . we demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and real-world examples .

['community identification', 'dynamic social networks', 'model development']


an iterative method for multi-class cost-sensitive learning cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification . in this paper , we present a method for solving multi-class cost-sensitive learning problems using any binary classification algorithm . this algorithm is derived using hree key ideas : 1 ) iterative weighting ; 2 ) expanding data space ; and 3 ) gradient boosting with stochastic ensembles . we establish some theoretical guarantees concerning the performance of this method . in particular , we show that a certain variant possesses the boosting property , given a form of weak learning assumption on the component binary classifier . we also empirically evaluate the performance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning , in terms of predictive performance ( cost minimization ) and , in many cases , computational efficiency .

['boosting', 'cost-sensitive learning', 'learning', 'multi-class classification']

learning NOUN compound problems
boosting VERB acl gradient
boosting VERB amod property
learning NOUN compound assumption
learning NOUN pobj for

efficient mining of weighted association rules ( war )

['database applications', 'ordered shrinkage', 'weighted association rules']


learning and making decisions when costs and probabilities are both unknown in many data mining domains , misclassification costs are different for different examples , in the same way that class membership probabilities are example-dependent . in these domains , both costs and probabilities are unknown for test examples , so both cost estimators and probability estimators must be learned . after discussing how to make optimal decisions given cost and probability estimates , we present decision tree and naive bayesian learning methods for obtaining well-calibrated probability estimates . we then explain how to obtain unbiased estimators for example-dependent costs , taking into account the difficulty that in general , probabilities and costs are not independent random variables , and the training examples for which costs are known are not representative of all examples . the latter problem is called sample selection bias in econometrics . our solution to it is based on nobel prize-winning work due to the economist james heckman . we show that the methods we propose perform better than metacost and all other known methods , in a comprehensive experimental comparison that uses the well-known , large , and challenging dataset from the kdd '98 data mining contest .

['probabilistic algorithms']


discovering complex matchings across web query interfaces : a correlation mining approach to enable information integration , schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources . while complex matchings are common , because of their far more complex search space , most existing techniques focus on simple 1:1 matchings . to tackle this challenge , this paper takes a conceptually novel approach by viewing schema matching as correlation mining , for our task of matching web query interfaces to integrate the myriad databases on the internet . on this `` deep web , '' query interfaces generally form complex matchings between attribute groups ( e.g. , ( author ) corresponds to ( first name , last name ) in the books domain ) . we observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships : grouping attributes ( e.g. , ( first name , last name ) ) tend to be co-present in query interfaces and thus positively correlated . in contrast , synonym attributes are negatively correlated because they rarely co-occur . this insight enables us to discover complex matchings by a correlation mining approach . in particular , we develop the dcm framework , which consists of data preparation , dual mining of positive and negative correlations , and finally matching selection . unlike previous correlation mining algorithms , which mainly focus on finding strong positive correlations , our algorithm cares both positive and negative correlations , especially the subtlety of negative correlations , due to its special importance in schema matching . this leads to the introduction of a new correlation measure , $ h$ - measure , distinct from those proposed in previous work . we evaluate our approach extensively and the results show good accuracy for discovering complex matchings .

['correlation measure', 'correlation mining', 'data integration', 'deep web', 'heterogeneous databases', 'schema matching']


rule interestingness analysis using olap operations the problem of interestingness of discovered rules has been investigated by many researchers . the issue is that data mining algorithms often generate too many rules , which make it very hard for the user to find the interesting ones . over the years many techniques have been proposed . however , few have made it to real-life applications . since august 2004 , we have been working on a major application for motorola . the objective is to find causes of cellular phone call failures from a large amount of usage log data . class association rules have been shown to be suitable for this type of diagnostic data mining application . we were also able to put several existing interestingness methods to the test , which revealed some major shortcomings . one of the main problems is that most existing methods treat rules individually . however , we discovered that users seldom regard a single rule to be interesting by itself . a rule is only interesting in the context of some other rules . furthermore , in many cases , each individual rule may not be interesting , but a group of them together can represent an important piece of knowledge . this led us to discover a deficiency of the current rule mining paradigm . using non-zero minimum support and non-zero minimum confidence eliminates a large amount of context information , which makes rule analysis difficult . this paper proposes a novel approach to deal with all of these issues , which casts rule analysis as olap operations and general impression mining . this approach enables the user to explore the knowledge space to find useful knowledge easily and systematically . it also provides a natural framework for visualization . as an evidence of its effectiveness , our system , called opportunity map , based on these ideas has been deployed , and it is in daily use in motorola for finding actionable knowledge from its engineering and other types of data sets .

['class association rules', 'diagnostic data mining', 'general impressions', 'interestingness analysis', 'miscellaneous', 'olap']

interestingness analysis ADP dobj rule
olap NOUN amod operations
olap NOUN compound operations

a unifying framework for detecting outliers and change points from non-stationary time series data we are concerned with the issues of outlier detection and change point detection from a data stream . in the area of data mining , there have been increased interest in these issues since the former is related to fraud detection , rare event discovery , etc. , while the latter is related to event\/trend by change detection , activity monitoring , etc. . specifically , it is important to consider the situation where the data source is non-stationary , since the nature of data source may change over time in real applications . although in most previous work outlier detection and change point detection have not been related explicitly , this paper presents a unifying framework for dealing with both of them on the basis of the theory of on-line learning of non-stationary time series . in this framework a probabilistic model of the data source is incrementally learned using an on-line discounting learning algorithm , which can track the changing data source adaptively by forgetting the effect of past data gradually . then the score for any given data is calculated to measure its deviation from the learned model , with a higher score indicating a high possibility of being an outlier . further change points in a data stream are detected by applying this scoring method into a time series of moving averaged losses for prediction using the learned model . specifically we develop an efficient algorithms for on-line discounting learning of auto-regression models from time series data , and demonstrate the validity of our framework through simulation and experimental applications to stock market data analysis .

['probabilistic algorithms']


anonymity-preserving data collection protection of privacy has become an important problem in data mining . in particular , individuals have become increasingly unwilling to share their data , frequently resulting in individuals either refusing to share their data or providing incorrect data . in turn , such problems in data collection can affect the success of data mining , which relies on sufficient amounts of accurate data in order to produce meaningful results . random perturbation and randomized response techniques can provide some level of privacy in data collection , but they have an associated cost in accuracy . cryptographic privacy-preserving data mining methods provide good privacy and accuracy properties . however , in order to be efficient , those solutions must be tailored to specific mining tasks , thereby losing generality . in this paper , we propose efficient cryptographic techniques for online data collection in which data from a large number of respondents is collected anonymously , without the help of a trusted third party . that is , our solution allows the miner to collect the original data from each respondent , but in such a way that the miner can not link a respondent 's data to the respondent . an advantage of such a solution is that , because it does not change the actual data , its success does not depend on the underlying data mining problem . we provide proofs of the correctness and privacy of our solution , as well as experimental data that demonstrates its efficiency . we also extend our solution to tolerate certain kinds of malicious behavior of the participants .

['anonymity', 'data collection', 'data encryption', 'data mining']


blosom : a framework for mining arbitrary boolean expressions we introduce a novel framework , called blosom , for mining ( frequent ) boolean expressions over binary-valued datasets . we organize the space of boolean expressions into four categories : pure conjunctions , pure disjunctions , conjunction of disjunctions , and disjunction of conjunctions . we focus on mining the simplest expressions the minimal generators for each class . we also propose a closure operator for each class that yields closed boolean expressions . blosom efficiently mines frequent boolean expressions by utilizing a number of methodical pruning techniques . experiments showcase the behavior of blosom , and an application study on a real dataset is also given .

['boolean expression', 'closed itemsets', 'minimal generator']


out-of-core frequent pattern mining on a commodity pc in this work we focus on the problem of frequent itemset mining on large , out-of-core data sets . after presenting a characterization of existing out-of-core frequent itemset mining algorithms and their drawbacks , we introduce our efficient , highly scalable solution . presented in the context of the fpgrowth algorithm , our technique involves several novel i\/o-conscious optimizations , such as approximate hash-based sorting and blocking , and leverages recent architectural advancements in commodity computers , such as 64-bit processing . we evaluate the proposed optimizations on truly large data sets , up to 75gb , and show they yield greater than a 400-fold execution time improvement . finally , we discuss the impact of this research in the context of other pattern mining challenges , such as sequence mining and graph mining .

['itemsets', 'out of core', 'pattern mining', 'secondary memory']


modeling relationships at multiple scales to improve accuracy of large recommender systems the collaborative filtering approach to recommender systems predicts user preferences for products or services by learning past user-item relationships . in this work , we propose novel algorithms for predicting user ratings of items by integrating complementary models that focus on patterns at different scales . at a local scale , we use a neighborhood-based technique that infers ratings from observed ratings by similar users or of similar items . unlike previous local approaches , our method is based on a formal model that accounts for interactions within the neighborhood , leading to improved estimation quality . at a higher , regional , scale , we use svd-like matrix factorization for recovering the major structural patterns in the user-item rating matrix . unlike previous approaches that require imputations in order to fill in the unknown matrix entries , our new iterative algorithm avoids imputation . because the models involve estimation of millions , or even billions , of parameters , shrinkage of estimated values to account for sampling variability proves crucial to prevent overfitting . both the local and the regional approaches , and in particular their combination through a unifying model , compare favorably with other approaches and deliver substantially better results than the commercial netflix cinematch recommender system on a large publicly available data set .

['collaborative filtering', 'netflix prize', 'recommender systems']


hierarchical topic segmentation of websites in this paper , we consider the problem of identifying and segmenting topically cohesive regions in the url tree of a large website . each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier . we develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels . we propose a general framework to use these measures for describing the quality of a segmentation ; we also provide an efficient algorithm to find the best segmentation in this framework . extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations .

['classification', 'facility location', 'gain ratio', 'information search and retrieval', 'kl-distance', 'tree partitioning', 'website hierarchy', 'website segmentation']


information extraction from wikipedia : moving down the long tail not only is wikipedia a comprehensive source of quality information , it has several kinds of internal structure ( e.g. , relational summaries known as infoboxes ) , which enable self-supervised information extraction . while previous efforts at extraction from wikipedia achieve high precision and recall on well-populated classes of articles , they fail in a larger number of cases , largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data . this paper presents three novel techniques for increasing recall from wikipedia 's long tail of sparse classes : ( 1 ) shrinkage over an automatically-learned subsumption taxonomy , ( 2 ) a retraining technique for improving the training data , and ( 3 ) supplementing results by extracting from the broader web . our experiments compare design variations and show that , used in concert , these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision .

['information extraction', 'information systems applications', 'semantic web', 'wikipedia']

wikipedia NOUN ccomp has
wikipedia NOUN pobj from
wikipedia NOUN poss tail

why collective inference improves relational classification procedures for collective inference make simultaneous statistical judgments about the same variables for a set of related data instances . for example , collective inference could be used to simultaneously classify a set of hyperlinked documents or infer the legitimacy of a set of related financial transactions . several recent studies indicate that collective inference can significantly reduce classification error when compared with traditional inference techniques . we investigate the underlying mechanisms for this error reduction by reviewing past work on collective inference and characterizing different types of statistical models used for making inference in relational data . we show important differences among these models , and we characterize the necessary and sufficient conditions for reduced classification error based on experiments with real and simulated data .

['collective inference', 'learning', 'models', 'probabilistic relational models', 'relational learning']

collective inference ADJ nsubj improves
models NOUN pobj of
models NOUN pobj among

aggregation-based feature invention and relational concept classes model induction from relational data requires aggregation of the values of attributes of related entities . this paper makes three contributions to the study of relational learning . ( 1 ) it presents a hierarchy of relational concepts of increasing complexity , using relational schema characteristics such as cardinality , and derives classes of aggregation operators that are needed to learn these concepts . ( 2 ) expanding one level of the hierarchy , it introduces new aggregation operators that model the distributions of the values to be aggregated and ( for classification problems ) the differences in these distributions by class . ( 3 ) it demonstrates empirically on a noisy business domain that more-complex aggregation methods can increase generalization performance . constructing features using target-dependent aggregations can transform relational prediction tasks so that well-understood feature-vector-based modeling algorithms can be applied successfully .

['aggregation', 'constructive induction', 'feature construction', 'learning', 'propositionalization', 'relational learning']

aggregation NOUN dobj requires
learning NOUN pobj of
aggregation NOUN compound operators
aggregation NOUN compound operators
aggregation NOUN compound methods

finding recent frequent itemsets adaptively over online data streams a data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate . consequently , the knowledge embedded in a data stream is more likely to be changed as time goes by . identifying the recent change of a data stream , specially for an online data stream , can provide valuable information for the analysis of the data stream . in addition , monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge . however , most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present . this paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream . the effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by . furthermore , several optimization techniques are devised to minimize processing time as well as main memory usage . finally , the proposed method is analyzed by a series of experiments .

['data stream', 'database applications', 'decay mechanism', 'delayed-insertion', 'pruning of itemsets', 'recent frequent itemsets']


predicting customer shopping lists from point-of-sale purchase data this paper describes a prototype that predicts the shopping lists for customers in a retail store . the shopping list prediction is one aspect of a larger system we have developed for retailers to provide individual and personalized interactions with customers as they navigate through the retail store . instead of using traditional personalization approaches , such as clustering or segmentation , we learn separate classifiers for each customer from historical transactional data . this allows us to make very fine-grained and accurate predictions about what items a particular individual customer will buy on a given shopping trip . we formally frame the shopping list prediction as a classification problem , describe the algorithms and methodology behind our system , its impact on the business case in which we frame it , and explore some of the properties of the data source that make it an interesting testbed for kdd algorithms . our results show that we can predict a shopper 's shopping list with high levels of accuracy , precision , and recall . we believe that this work impacts both the data mining and the retail business community . the formulation of shopping list prediction as a machine learning problem results in algorithms that should be useful beyond retail shopping list prediction . for retailers , the result is not only a practical system that increases revenues by up to 11 % , but also enhances customer experience and loyalty by giving them the tools to individually interact with customers and anticipate their needs .

['applications', 'classification', 'machine learning', 'pos data']

classification NOUN compound problem

mining images on semantics via statistical learning in this paper , we have proposed a novel framework to enable hierarchical image classification via statistical learning . by integrating the concept hierarchy for semantic image concept organization , a hierarchical mixture model is proposed to enable multi-level modeling of semantic image concepts and hierarchical classifier combination . thus , learning the classifiers for the semantic image concepts at the high level of the concept hierarchy can be effectively achieved by detecting the presences of the relevant base-level atomic image concepts . to effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy , we have proposed a novel adaptive em algorithm to achieve more effective model selection and parameter estimation . in addition , a novel penalty term is proposed to effectively eliminate the misleading effects of the outlying unlabeled images on semi-supervised classifier training . our experimental results in a specific image domain of outdoor photos are very attractive .

['adaptive em algorithm', 'hierarchical mixture model', 'image classification']


automatic record linkage using seeded nearest neighbor and support vector machine classification the task of linking databases is an important step in an increasing number of data mining projects , because linked data can contain information that is not available otherwise , or that would require time-consuming and expensive collection of specific data . the aim of linking is to match and aggregate all records that refer to the same entity . one of the major challenges when linking large databases is the efficient and accurate classification of record pairs into matches and non-matches . while traditionally classification was based on manually-set thresholds or on statistical procedures , many of the more recently developed classification methods are based on supervised learning techniques . they therefore require training data , which is often not available in real world situations or has to be prepared manually , an expensive , cumbersome and time-consuming process . the author has previously presented a novel two-step approach to automatic record pair classification ( 6 , 7 ) . in the first step of this approach , training examples of high quality are automatically selected from the compared record pairs , and used in the second step to train a support vector machine ( svm ) classifier . initial experiments showed the feasibility of the approach , achieving results that outperformed k-means clustering . in this paper , two variations of this approach are presented . the first is based on a nearest-neighbour classifier , while the second improves a svm classifier by iteratively adding more examples into the training sets . experimental results show that this two-step approach can achieve better classification results than other unsupervised approaches .

['data linkage', 'data matching', 'deduplication', 'entity resolution', 'nearest neighbour', 'support vector machine']


mining coherent gene clusters from gene-sample-time microarray data extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications . in this paper , we explore a novel type of gene-sample-time microarray data sets , which records the expression levels of various genes under a set of samples during a series of time points . in particular , we propose the mining of coherent gene clusters from such data sets . each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series . the coherent gene clusters may identify the samples corresponding to some phenotypes ( e.g. , diseases ) , and suggest the candidate genes correlated to the phenotypes . we present two efficient algorithms , namely the sample-gene search and the gene-sample search , to mine the complete set of coherent gene clusters . we empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets . the test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters .

['bioinformatics', 'clustering', 'microarray data']

bioinformatics NOUN compound research

privacy-preserving k-means clustering over vertically partitioned data privacy and security concerns can prevent sharing of data , derailing data mining projects . distributed knowledge discovery , if done correctly , can alleviate this problem . the key is to obtain valid results , while providing guarantees on the ( non ) disclosure of data . we present a method for k-means clustering when different sites contain different attributes for a common set of entities . each site learns the cluster of each entity , but learns nothing about the attributes at other sites .

['privacy']

privacy NOUN pobj over

customer targeting models using actively-selected web content we consider the problem of predicting the likelihood that a company will purchase a new product from a seller . the statistical models we have developed at ibm for this purpose rely on historical transaction data coupled with structured firmographic information like the company revenue , number of employees and so on . in this paper , we extend this methodology to include additional text-based features based on analysis of the content on each company 's website . empirical results demonstrate that incorporating such web content can significantly improve customer targeting . furthermore , we present methods to actively select only the web content that is likely to improve our models , while reducing the costs of acquisition and processing .

['active feature-value acquisition', 'active learning', 'learning', 'models', 'text categorization', 'web mining']

models NOUN nsubj rely
models NOUN dobj improve

on privacy preservation against adversarial data mining privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data . a rudimentary way to preserve privacy is to simply hide the information in some of the sensitive fields picked by a user . however , such a method is far from satisfactory in its ability to prevent adversarial data mining . real data records are not randomly distributed . as a result , some fields in the records may be correlated with one another . if the correlation is sufficiently high , it may be possible for an adversary to predict some of the sensitive fields using other fields . in this paper , we study the problem of privacy preservation against adversarial data mining , which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved . in other words , even by data mining , an adversary still can not accurately recover the hidden data entries . we model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice . an extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach .

['association rules', 'privacy preservation']


next frontier this talk is about the next frontier in knowledge discovery and data mining .

['information search and retrieval', 'knowledge representation formalisms and methods']


on string classification in data streams string data has recently become important because of its use in a number of applications such as computational and molecular biology , protein analysis , and market basket data . in many cases , these strings contain a wide variety of substructures which may have physical significance for that application . for example , such substructures could represent important fragments of a dna string or an interesting portion of a fraudulent transaction . in such a case , it is desirable to determine the identity , location , and extent of that substructure in the data . this is a much more difficult generalization of the classification problem , since the latter problem labels entire strings rather than deal with the more complex task of determining string fragments with a particular kind of behavior . the problem becomes even more complicated when different kinds of substrings show complicated nesting patterns . therefore , we define a somewhat different problem which we refer to as the generalized classification problem . we propose a scalable approach based on hidden markov models for this problem . we show how to implement the generalized string classification procedure for very large data bases and data streams . we present experimental results over a number of large data sets and data streams .

['classification', 'hidden markov models', 'string']

string NOUN pobj of
classification NOUN compound problem
string NOUN compound fragments
classification NOUN compound problem
string NOUN compound classification
classification NOUN compound procedure

fragments of order high-dimensional collections of 0 -- 1 data occur in many applications . the attributes in such data sets are typically considered to be unordered . however , in many cases there is a natural total or partial order â‰º underlying the variables of the data set . examples of variables for which such orders exist include terms in documents , courses in enrollment data , and paleontological sites in fossil data collections . the observations in such applications are flat , unordered sets ; however , the data sets respect the underlying ordering of the variables . by this we mean that if a â‰º b â‰º c are three variables respecting the underlying ordering â‰º , and both of variables a and c appear in an observation , then , up to noise levels , variable b also appears in this observation . similarly , if a1 â‰º a2 â‰º ... â‰º al-1 â‰º ai is a longer sequence of variables , we do not expect to see many observations for which there are indices i ( j ( k such that ai and ak occur in the observation but aj does not . in this paper we study the problem of discovering fragments of orders of variables implicit in collections of unordered observations . we define measures that capture how well a given order agrees with the observed data . we describe a simple and efficient algorithm for finding all the fragments that satisfy certain conditions . we also discuss the sometimes necessary postprocessing for selecting only the best fragments of order . also , we relate our method with a sequencing approach that uses a spectral algorithm , and with the consecutive ones problem . we present experimental results on some real data sets ( author lists of database papers , exam results data , and paleontological data ) .

['consecutive ones property', 'discovering hidden orderings', 'nonnumerical algorithms and problems', 'novel data mining algorithms', 'spectral analysis of data']


selecting the right interestingness measure for association patterns many techniques for association rule mining and feature selection require a suitable metric to capture the dependencies among variables in a data set . for example , metrics such as support , confidence , lift , correlation , and collective strength are often used to determine the interestingness of association patterns . however , many such measures provide conflicting information about the interestingness of a pattern , and the best metric to use for a given application domain is rarely known . in this paper , we present an overview of various measures proposed in the statistics , machine learning and data mining literature . we describe several key properties one should examine in order to select the right measure for a given application domain . a comparative study of these properties is made using twenty one of the existing measures . we show that each measure has different properties which make them useful for some application domains , but not for others . we also present two scenarios in which most of the existing measures agree with each other , namely , support-based pruning and table standardization . finally , we present an algorithm to select a small set of tables such that an expert can select a desirable measure by looking at just this small set of tables .

['associations', 'contingency tables', 'interestingness measure']


query-time entity resolution the goal of entity resolution is to reconcile database references corresponding to the same real-world entities . given the abundance of publicly available databases where entities are not resolved , we motivate the problem of quickly processing queries that require resolved entities from such ` unclean ' databases . we propose a two-stage collective resolution strategy for processing queries . we then show how it can be performed on-the-fly by adaptively extracting and resolving those database references that are the most helpful for resolving the query . we validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing . we then show how the same queries can be answered in real time using our adaptive approach while preserving the gains of collective resolution .

['adaptive', 'entity resolution', 'query', 'relations']

query NOUN dobj resolving
adaptive ADJ amod strategies
query NOUN compound processing
adaptive ADJ amod approach

scalable discovery of hidden emails from large folders the popularity of email has triggered researchers to look for ways to help users better organize the enormous amount of information stored in their email folders . one challenge that has not been studied extensively in text mining is the identification and reconstruction of hidden emails . a hidden email is an original email that has been quoted in at least one email in a folder , but does not present itself in the same folder . it may have been ( un ) intentionally deleted or may never have been received . the discovery and reconstruction of hidden emails is critical for many applications including email classification , summarization and forensics . this paper proposes a framework for reconstructing hidden emails using the embedded quotations found in messages further down the thread hierarchy . we evaluate the robustness and scalability of our framework by using the enron public email corpus . our experiments show that hidden emails exist widely in that corpus and also that our optimization techniques are effective in processing large email folders .

['forensics', 'hidden email', 'text mining']

forensics NOUN conj summarization

mark : a boosting algorithm for heterogeneous kernel models support vector machines and other kernel methods have proven to be very effective for nonlinear inference . practical issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadratically with the data . inspired by ensemble and boosting methods like mart , we propose the multiple additive regression kernels ( mark ) algorithm to address these issues . mark considers a large ( potentially infinite ) library of kernel matrices formed by different kernel functions and parameters . using gradient boosting\/column generation , mark constructs columns of the heterogeneous kernel matrix ( the base hypotheses ) on the fly and then adds them into the kernel ensemble . regularization methods such as used in svm , kernel ridge regression , and mart , are used to prevent overfitting . we investigate how mark is applied to heterogeneous kernel ridge regression . the resulting algorithm is simple to implement and efficient . kernel parameter selection is handled within mark . sampling and `` weak '' kernels are used to further enhance the computational efficiency of the resulting additive algorithm . the user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernels . mark compares very favorably with svm and kernel ridge regression on several benchmark datasets .

['probabilistic algorithms']


combining proactive and reactive predictions for data streams mining data streams is important in both science and commerce . two major challenges are ( 1 ) the data may grow without limit so that it is difficult to retain a long history ; and ( 2 ) the underlying concept of the data may change over time . different from common practice that keeps recent raw data , this paper uses a measure of conceptual equivalence to organize the data history into a history of concepts . along the journey of concept change , it identifies new concepts as well as re-appearing ones , and learns transition patterns among concepts to help prediction . different from conventional methodology that passively waits until the concept changes , this paper incorporates proactive and reactive predictions . in a proactive mode , it anticipates what the new concept will be if a future concept change takes place , and prepares prediction strategies in advance . if the anticipation turns out to be correct , a proper prediction model can be launched instantly upon the concept change . if not , it promptly resorts to a reactive mode : adapting a prediction model to the new data . a system repro is proposed to implement these new ideas . experiments compare the system with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change . empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data streams .

['conceptual equivalence', 'data stream', 'proactive learning']


discovering significant opsm subspace clusters in massive gene expression data order-preserving submatrixes ( opsms ) have been accepted as a biologically meaningful subspace cluster model , capturing the general tendency of gene expressions across a subset of conditions . in an opsm , the expression levels of all genes induce the same linear ordering of the conditions . opsm mining is reducible to a special case of the sequential pattern mining problem , in which a pattern and its supporting sequences uniquely specify an opsm cluster . those small twig clusters , specified by long patterns with naturally low support , incur explosive computational costs and would be completely pruned off by most existing methods for massive datasets containing thousands of conditions and hundreds of thousands of genes , which are common in today 's gene expression analysis . however , it is in particular interest of biologists to reveal such small groups of genes that are tightly coregulated under many conditions , and some pathways or processes might require only two genes to act in concert . in this paper , we introduce the kiwi mining framework for massive datasets , that exploits two parameters k and w to provide a biased testing on a bounded number of candidates , substantially reducing the search space and problem scale , targeting on highly promising seeds that lead to significant clusters and twig clusters . extensive biological and computational evaluations on real datasets demonstrate that kiwi can effectively mine biologically meaningful opsm subspace clusters with good efficiency and scalability .

['gene expression data', 'order-preserving submatrix', 'scalability', 'subspace clustering', 'twig cluster']

scalability NOUN conj efficiency

an integrated framework on mining logs files for computing system management traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies . this has been well known and experienced as a cumbersome , labor intensive , and error prone process . in addition , this process is difficult to keep up with the rapidly changing environments . in this paper , we will describe our research efforts on establishing an integrated framework for mining system log files for automatic management . in particular , we apply text mining techniques to categorize messages in log files into common situations , improve categorization accuracy by considering the temporal characteristics of log messages , develop temporal mining techniques to discover the relationships between different events , and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management .

['applications', 'event relationship', 'learning', 'log categorization', 'system management', 'temporal pattern']


regression error characteristic surfaces this paper presents a generalization of regression error characteristic ( rec ) curves . rec curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of roc curves to regression problems . rec curves provide useful information for analyzing the performance of models , particularly when compared to error statistics like for instance the mean squared error . in this paper we present regression error characteristic ( rec ) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable , i.e. the joint cumulative distribution function of the errors and the target variable . this provides a more detailed analysis of the performance of models when compared to rec curves . this extra detail is particularly relevant in applications with non-uniform error costs , where it is important to study the performance of models for specific ranges of the target variable . in this paper we present the notion of rec surfaces , describe how to use them to compare the performance of models , and illustrate their use with an important practical class of applications : the prediction of rare extreme values .

['evaluation metrics', 'model comparisons', 'regression problems']


the mathematics of causal inference i will review concepts , principles , and mathematical tools that were found useful in applications involving causal and counterfactual relationships . this semantical framework , enriched with a few ideas from logic and graph theory , gives rise to a complete , coherent , and friendly calculus of causation that unifies the graphical and counterfactual approaches to causation and resolves many long-standing problems in several of the sciences . these include questions of causal effect estimation , policy analysis , and the integration of data from diverse studies . of special interest to kdd researchers would be the following topics : the mediation formula , and what it tells us about direct and indirect effects . what mathematics can tell us about `` external validity '' or `` generalizing from experiments '' what can graph theory tell us about recovering from sample-selection bias .

['causal inference', 'miscellaneous']


translation-invariant mixture models for curve clustering in this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid . our approach uses the expectation-maximization ( em ) algorithm to recover both the mean curve shapes for each cluster , and the most likely shifts , offsets , and cluster memberships for each curve . we demonstrate how bayesian estimation methods can improve the results for small sample sizes by enforcing smoothness in the cluster mean curves . we evaluate the methodology on two real-world data sets , time-course gene expression data and storm trajectory data . experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets . the proposed approach provides a non-parametric , computationally efficient , and robust methodology for clustering broad classes of curve data .

['alignment', 'curve clustering', 'em', 'learning', 'mixture model', 'transformation invariance']

em PRON intj algorithm
alignment NOUN dobj incorporate

detecting privacy leaks using corpus-based association rules detecting inferences in documents is critical for ensuring privacy when sharing information . in this paper , we propose a refined and practical model of inference detection using a reference corpus . our model is inspired by association rule mining : inferences are based on word co-occurrences . using the model and taking the web as the reference corpus , we can find inferences and measure their strength through web-mining algorithms that leverage search engines such as google or yahoo ! . our model also includes the important case of private corpora , to model inference detection in enterprise settings in which there is a large private document repository . we find inferences in private corpora by using analogs of our web-mining algorithms , relying on an index for the corpus rather than a web search engine . we present results from two experiments . the first experiment demonstrates the performance of our techniques in identifying all the keywords that allow for inference of a particular topic ( e.g. `` hiv '' ) with confidence above a certain threshold . the second experiment uses the public enron e-mail dataset . we postulate a sensitive topic and use the enron corpus and the web together to find inferences for the topic . these experiments demonstrate that our techniques are practical , and that our model of inference based on word co-occurrence is well-suited to efficient inference detection .

['association rule mining', 'inference control', 'inference detection', 'search engine', 'web mining']


evolutionary spectral clustering by incorporating temporal smoothness evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic web and blog contents and clustering data streams . in evolutionary clustering , a good clustering result should fit the current data well , while simultaneously not deviate too dramatically from the recent history . to fulfill this dual purpose , a measure of temporal smoothness is integrated in the overall measure of clustering quality . in this paper , we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering . for both frameworks , we start with intuitions gained from the well-known k-means clustering problem , and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems . our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts . furthermore , we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems . performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts .

['evolutionary spectral clustering', 'mining data streams', 'preserving cluster membership', 'temporal smoothness', 'treserving cluster quality']


fast direction-aware proximity for graph mining in this paper we study asymmetric proximity measures on directed graphs , which quantify the relationships between two nodes or two groups of nodes . the measures are useful in several graph mining tasks , including clustering , link prediction and connection subgraph discovery . our proximity measure is based on the conceptof escape probability . this way , we strive to summarize the multiple facets of nodes-proximity , while avoiding some of the pitfalls to which alternative proximity measures are susceptible . a unique feature of the measures is accounting for the underlying directional information . we put a special emphasis on computational efficiency , and develop fast solutions that are applicable in several settings . our experimental study shows the usefulness of our proposed direction-aware proximity method for several applications , and that our algorithms achieve a significant speedup ( up to 50,000 x ) over straight forward implementations .

['graph mining', 'miscellaneous', 'proximity', 'random walk']

proximity NOUN ROOT proximity
proximity NOUN compound measures
proximity NOUN compound measure
proximity NOUN pobj of
proximity NOUN compound measures
proximity NOUN compound method

support envelopes : a technique for exploring the structure of association patterns this paper introduces support envelopes -- a new tool for analyzing association patterns -- and illustrates some of their properties , applications , and possible extensions . specifically , the support envelope for a transaction data set and a specified pair of positive integers ( m , n ) consists of the items and transactions that need to be searched to find any association pattern involving m or more transactions and n or more items . for any transaction data set with m transactions and n items , there is a unique lattice of at most m \* n support envelopes that captures the structure of the association patterns in that data set . because support envelopes are not encumbered by a support threshold , this support lattice provides a complete view of the association structure of the data set , including association patterns that have low support . furthermore , the boundary of the support lattice -- the support boundary -- has at most min ( m , n ) envelopes and is especially interesting since it bounds the maximum sizes of potential association patterns -- not only for frequent , closed , and maximal itemsets , but also for patterns , such as error-tolerant itemsets , that are more general . the association structure can be represented graphically as a two-dimensional scatter plot of the ( m , n ) values associated with the support envelopes of the data set , a feature that is useful in the exploratory analysis of association patterns . finally , the algorithm to compute support envelopes is simple and computationally efficient , and it is straightforward to parallelize the process of finding all the support envelopes .

['association analysis', 'error-tolerant itemsets', 'formal concept analysis', 'support envelope']


a fast kernel-based multilevel algorithm for graph clustering graph clustering ( also called graph partitioning ) -- clustering the nodes of a graph -- is an important problem in diverse data mining applications . traditional approaches involve optimization of graph clustering objectives such as normalized cut or ratio association ; spectral methods are widely used for these objectives , but they require eigenvector computation which can be slow . recently , graph clustering with a general cut objective has been shown to be mathematically equivalent to an appropriate weighted kernel k-means objective function . in this paper , we exploit this equivalence to develop a very fast multilevel algorithm for graph clustering . multilevel approaches involve coarsening , initial partitioning and refinement phases , all of which may be specialized to different graph clustering objectives . unlike existing multilevel clustering approaches , such as metis , our algorithm does not constrain the cluster sizes to be nearly equal . our approach gives a theoretical guarantee that the refinement step decreases the graph cut objective under consideration . experiments show that we achieve better final objective function values as compared to a state-of-the-art spectral clustering algorithm : on a series of benchmark test graphs with up to thirty thousand nodes and one million edges , our algorithm achieves lower normalized cut values in 67 % of our experiments and higher ratio association values in 100 % of our experiments . furthermore , on large graphs , our algorithm is significantly faster than spectral methods . finally , our algorithm requires far less memory than spectral methods ; we cluster a 1.2 million node movie network into 5000 clusters , which due to memory requirements can not be done directly with spectral methods .

['graph clustering', 'kernel methods', 'multilevel methods', 'spectral clustering']


algorithms for storytelling we formulate a new data mining problem called it storytelling as a generalization of redescription mining . in traditional redescription mining , we are given a set of objects and a collection of subsets defined over these objects . the goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects . storytelling , on the other hand , aims to explicitly relate object sets that are disjoint ( and hence , maximally dissimilar ) by finding a chain of ( approximate ) redescriptions between the sets . this problem finds applications in bioinformatics , for instance , where the biologist is trying to relate a set of genes expressed in one experiment to another set , implicated in a different pathway . we outline an efficient storytelling implementation that embeds the cart wheels redescription mining algorithm in an a \* search procedure , using the former to supply next move operators on search branches to the latter . this approach is practical and effective for mining large datasets and , at the same time , exploits the structure of partitions imposed by the given vocabulary . three application case studies are presented : a study of word overlaps in large english dictionaries , exploring connections between genesets in a bioinformatics dataset , and relating publications in the pubmed index of abstracts .

['learning', 'redescription', 'storytelling']

storytelling VERB pcomp for
storytelling VERB ccomp called
redescription NOUN compound mining
redescription NOUN compound mining
storytelling VERB advcl aims
storytelling VERB compound implementation
redescription NOUN compound algorithm

discovering similar patterns in time series

['knowledge discovery', 'time series']


incremental quantile estimation for massive tracking

['applications', 'customer profiles', 'customer relationship management', 'dynamic database', 'equi-depth histograms', 'ewma', 'massive data', 'model validation and analysis', 'percentiles', 'sequential estimation', 'simulation output analysis', 'stochastic approximation', 'transaction data']


feature selection in unsupervised learning via evolutionary search

['evolutionary search', 'feature selection']


detecting research topics via the correlation between graphs and texts in this paper we address the problem of detecting topics in large-scale linked document collections . recently , topic detection has become a very active area of research due to its utility for information navigation , trend analysis , and high-level description of data . we present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the nodes are limited to the documents containing the term . this tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models . we develop a topic score measure for each term , using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity . our approach is based on the intuition that if a term is relevant to a topic , the documents containing the term have denser connectivity than a random selection of documents . we extend our algorithm to detect a topic represented by a set of terms , using the intuition that if the co-occurrence of terms represents a new topic , the citation pattern should exhibit the synergistic effect . we test our algorithm on two electronic research literature collections , arxiv and citeseer . our evaluation shows that the approach is effective and reveals some novel aspects of topic detection .

['citation graphs', 'content analysis and indexing', 'correlation of text and links', 'graph mining', 'probabilistic measure', 'topic detection']


efficient algorithms for constructing decision trees with constraints

['classification', 'decision support', 'decision tree']


on the need for time series data mining benchmarks : a survey and empirical demonstration in the last decade there has been an explosion of interest in mining time series data . literally hundreds of papers have introduced new algorithms to index , classify , cluster and segment time series . in this work we make the following claim . much of this work has very little utility because the contribution made ( speed in the case of indexing , accuracy in the case of classification and clustering , model accuracy in the case of segmentation ) offer an amount of `` improvement '' that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets , or the variance that would have been observed by changing minor ( unstated ) implementation details . to illustrate our point , we have undertaken the most exhaustive set of time series experiments ever attempted , re-implementing the contribution of more than two dozen papers , and testing them on 50 real world , highly diverse datasets . our empirical results strongly support our assertion , and suggest the need for a set of time series benchmarks and more careful empirical evaluation in the data mining community .

['experimental evaluation', 'time series']


using a knowledge cache for interactive discovery of association rules

['decision support']


data mining solves tough semiconductor manufacturing problems

['machine learning', 'manufacturing optimization', 'neural networks', 'pattern recognition', 'rule induction', 'self organizing maps', 'semiconductor yield enhancement']


generalized clustering , supervised learning , and data assignment clustering algorithms have become increasingly important in handling and analyzing data . considerable work has been done in devising effective but increasingly specific clustering algorithms . in contrast , we have developed a generalized framework that accommodates diverse clustering algorithms in a systematic way . this framework views clustering as a general process of iterative optimization that includes modules for supervised learning and instance assignment . the framework has also suggested several novel clustering methods . in this paper , we investigate experimentally the efficacy of these algorithms and test some hypotheses about the relation between such unsupervised techniques and the supervised methods embedded in them .

['clustering', 'iterative optimization', 'learning', 'supervised learning']

clustering NOUN nsubj become
learning NOUN conj clustering
clustering NOUN compound algorithms
clustering NOUN compound algorithms
clustering NOUN compound algorithms
clustering NOUN acl views
learning NOUN nmod assignment
clustering NOUN compound methods

distributed data mining in a chain store database of short transactions in this paper , we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database . specifically , the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events , and is designed with the capability of mining non-sequential , inter-transaction information . hence , the causality rule mining provides a very general framework for rule derivation . note , however , that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate sets and a distributed database , and in our opinion , can not be dealt with by direct extensions from existing rule mining methods . consequently , we devise in this paper a series of level matching algorithms , including level matching ( abbreviatedly as lm ) , level matching with selective scan ( abbreviatedly as lms ) , and distributed level matching ( abbreviatedly as distibuted lm ) , to minimize the computing cost needed for the distributed data mining of causality rules . in addition , the phenomena of time window constraints are also taken into consideration for the development of our algorithms . as a result of properly employing the technologies of level matching and selective scan , the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules . scale-up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions . index terms : knowledge discovery , distributed data mining causality rules , triggering events , consequential events

['deduction', 'number-theoretic computations', 'representations']


applying data mining in investigating money laundering crimes in this paper , we study the problem of applying data mining to facilitate the investigation of money laundering crimes ( mlcs ) . we have identified a new paradigm of problems -- that of automatic community generation based on uni-party data , the data in which there is no direct or explicit link information available . consequently , we have proposed a new methodology for link discovery based on correlation analysis ( ldca ) . we have used mlc group model generation as an exemplary application of this problem paradigm , and have focused on this application to develop a specific method of automatic mlc group model generation based on timeline analysis using the ldca methodology , called coral . a prototype of coral method has been implemented , and preliminary testing and evaluations based on a real mlc case data are reported . the contributions of this work are : ( 1 ) identification of the uni-party data community generation problem paradigm , ( 2 ) proposal of a new methodology ldca to solve for problems in this paradigm , ( 3 ) formulation of the mlc group model generation problem as an example of this paradigm , ( 4 ) application of the ldca methodology in developing a specific solution ( coral ) to the mlc group model generation problem , and ( 5 ) development , evaluation , and testing of the coral prototype in a real mlc case data .

['bi-party data', 'clustering', 'community generation', 'coral', 'histogram', 'link discovery based on correlation analysis', 'mlc group models', 'money laundering crimes', 'timeline analysis', 'uni-party data']

coral NOUN oprd called
coral ADJ amod method
coral NOUN appos solution
coral ADJ amod prototype

real world performance of association rule algorithms this study compares five well-known association rule algorithms using three real-world datasets and an artificial dataset . the experimental results confirm the performance improvements previously claimed by the authors on the artificial data , but some of these gains do not carry over to the real datasets , indicating overfitting of the algorithms to the ibm artificial dataset . more importantly , we found that the choice of algorithm only matters at support levels that generate more rules than would be useful in practice . for support levels that generate less than 1,000,000 rules , which is much more than humans can handle and is sufficient for prediction purposes where data is loaded into ram , apriori finishes processing in less than 10 minutes . on our datasets , we observed super-exponential growth in the number of rules . on one of our datasets , a 0.02 % change in the support increased the number of rules from less than a million to over a billion , implying that outside a very narrow range of support values , the choice of algorithm is irrelevant .

['affinity analysis', 'association rules', 'benchmark', 'comparisons', 'frequent itemsets', 'market basket analysis']


the `` dgx '' distribution for mining massive , skewed data skewed distributions appear very often in practice . unfortunately , the traditional zipf distribution often fails to model them well . in this paper , we propose a new probability distribution , the discrete gaussian exponential ( dgx ) , to achieve excellent fits in a wide variety of settings ; our new distribution includes the zipf distribution as a special case . we present a statistically sound method for estimating the dgx parameters based on maximum likelihood estimation ( mle ) . we applied dgx to a wide variety of real world data sets , such as sales data from a large retailer chain , us-age data from at&t , and internet clickstream data ; in all cases , dgx fits these distributions very well , with almost a 99 % correlation coefficient in quantile-quantile plots . our algorithm also scales very well because it requires only a single pass over the data . finally , we illustrate the power of dgx as a new tool for data mining tasks , such as outlier detection .

['dgx', 'frequency-count plot', 'lognormal distribution', 'maximum likelihood estimation', 'outlier detection', 'rank-frequency plot', "zipf's law"]

dgx NOUN nmod distribution
dgx PROPN appos exponential
dgx NOUN compound parameters
dgx NOUN dobj applied
dgx NOUN nsubj fits
dgx NOUN pobj of

clustering seasonality patterns in the presence of errors clustering is a very well studied problem that attempts to group similar data points . most traditional clustering algorithms assume that the data is provided without measurement error . often , however , real world data sets have such errors and one can obtain estimates of these errors . we present a clustering method that incorporates information contained in these error estimates . we present a new distance function that is based on the distribution of errors in data . using a gaussian model for errors , the distance function follows a chi-square distribution and is easy to compute . this distance function is used in hierarchical clustering to discover meaningful clusters . the distance function is scale-invariant so that clustering results are independent of units of measuring data . in the special case when the error distribution is the same for each attribute of data points , the rank order of pair-wise distances is the same for our distance function and the euclidean distance function . the clustering method is applied to the seasonality estimation problem and experimental results are presented for the retail industry data as well as for simulated data , where it outperforms classical clustering methods .

['distance function', 'forecasting', 'gaussian distribution', 'product life cycle', 'seasonality', 'time-series']

seasonality NOUN compound estimation

a system for real-time competitive market intelligence a method is described for real-time market intelligence and competitive analysis . news stories are collected online for a designated group of companies . the goal is to detect critical differences in the text written about a company versus the text for its competitors . a solution is found by mapping the task into a non-stationary text categorization model . the overall design consists of the following components : ( a ) a real-time crawler that monitors newswires for stories about the competitors ( b ) a conditional document retriever that selects only those documents that meet the indicated conditions ( c ) text analysis techniques that convert the documents to a numerical format ( d ) rule induction methods for finding patterns in data ( e ) presentation techniques for displaying results . the method is extended to combine text with numerical measures , such as those based on stock prices and market capitalizations , that allow for more objective evaluations and projections .

['deduction']


tumor cell identification using features rules advances in imaging techniques have led to large repositories of images . there is an increasing demand for automated systems that can analyze complex medical images and extract meaningful information for mining patterns . here , we describe a real-life image mining application to the problem of tumor cell counting . the quantitative analysis of tumor cells is fundamental to characterizing the activity of tumor cells . existing approaches are mostly manual , time-consuming and subjective . efforts to automate the process of cell counting have largely focused on using image processing techniques only . our studies indicate that image processing alone is unable to give accurate results . in this paper , we examine the use of extracted features rules to aid in the process of tumor cell counting . we propose a robust local adaptive thresholding and dynamic water immersion algorithms to segment regions of interesting from background . meaningful features are then extracted from the segmented regions . a number of base classifiers are built to generate features rules to help identify the tumor cell . two voting strategies are implemented to combine the base classifiers into a meta-classifier . experiment results indicate that this process of using extracted features rules to help identify tumor cell leads to better accuracy than pure image processing techniques alone .

['dynamic water immersion', 'features rules', 'identification', 'local adaptive thresholding', 'majority vote', 'meta classifier', 'weighted vote']


topic-conditioned novelty detection automated detection of the first document reporting each new event in temporally-sequenced streams of documents is an open challenge . in this paper we propose a new approach which addresses this problem in two stages : 1 ) using a supervised learning algorithm to classify the on-line document stream into pre-defined broad topic categories , and 2 ) performing topic-conditioned novelty detection for documents in each topic . we also focus on exploiting named-entities for event-level novelty detection and using feature-based heuristics derived from the topic histories . evaluating these methods using a set of broadcast news stories , our results show substantial performance gains over the traditional one-level approach to the novelty detection problem .

['design methodology', 'feature selection', 'information search and retrieval', 'named entity', 'novelty detection', 'text classification']


predicting rare classes : can boosting make any weak learner strong ? boosting is a strong ensemble-based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner , as long as it satisfies the condition of yielding weighted accuracy ) 0.5 . in this paper , we analyze boosting with respect to this basic condition on the base learner , to see if boosting ensures prediction of rarely occurring events with high recall and precision . first we show that a base learner can satisfy the required condition even for poor recall or precision levels , especially for very rare classes . furthermore , we show that the intelligent weight updating mechanism in boosting , even in its strong cost-sensitive form , does not prevent cases where the base learner always achieves high precision but poor recall or high recall but poor precision , when mapped to the original distribution . in either of these cases , we show that the voting mechanism of boosting falls to achieve good overall recall and precision for the ensemble . in effect , our analysis indicates that one can not be blind to the base learner performance , and just rely on the boosting mechanism to take care of its weakness . we validate our arguments empirically on variety of real and synthetic rare class problems . in particular , using adacost as the boosting algorithm , and variations of pnrule and ripper as the base learners , we show that if algorithm a achieves better recall-precision balance than algorithm b , then using a as the base learner in adacost yields significantly better performance than using b as the base learner .

['analyze']

analyze VERB ROOT analyze

symp : an efficient clustering approach to identify clusters of arbitrary shapes in large data sets we propose a new clustering algorithm , called symp , which is based on synchronization of pulse-coupled oscillators . symp represents each data point by an integrate-and-fire oscillator and uses the relative similarity between the points to model the interaction between the oscillators . symp is robust to noise and outliers , determines the number of clusters in an unsupervised manner , identifies clusters of arbitrary shapes , and can handle very large data sets . the robustness of symp is an intrinsic property of the synchronization mechanism . to determine the optimum number of clusters , symp uses a dynamic resolution parameter . to identify clusters of various shapes , symp models each cluster by multiple gaussian components . the number of components is automatically determined using a dynamic intra-cluster resolution parameter . clusters with simple shapes would be modeled by few components while clusters with more complex shapes would require a larger number of components . the scalable version of symp uses an efficient incremental approach that requires a simple pass through the data set . the proposed clustering approach is empirically evaluated with several synthetic and real data sets , and its performance is compared with cure .

['gaussian mixture models', 'large datasets']


cvs : a correlation-verification based smoothing technique on information retrieval and term clustering as information volume in enterprise systems and in the web grows rapidly , how to accurately retrieve information is an important research area . several corpus based smoothing techniques have been proposed to address the data sparsity and synonym problems faced by information retrieval systems . such smoothing techniques are often unable to discover and utilize the correlations among terms . we propose cvs , a correlation-verification based smoothing method , that considers co-occurrence information in smoothing . strongly correlated terms in a document are identified by their co-occurrence frequencies in the document . to avoid missing correlated terms with low co-occurrence frequencies but specific to the theme of the document , the joint distributions of terms in the document are compared with those in the corpus for statistical significance . a common approach to apply corpus based smoothing techniques to information retrieval is by refining the vector representations of documents . this paper investigates the effects of corpus based smoothing on information retrieval by query expansion using term clusters generated from a term clustering process . the results can also be viewed in light of the effects of smoothing on clustering . empirical studies show that our approach outperforms previous corpus based smoothing techniques . it improves retrieval effectiveness by 14.6 % . the results demonstrate that corpus based smoothing can be used for query expansion by term clustering .

['content analysis and indexing', 'information retrieval', 'query expansion', 'term clustering', 'text mining']


proximus : a framework for analyzing very high dimensional discrete-attributed datasets this paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets . such datasets , which frequently arise in a wide variety of applications , pose some of the most significant challenges in data analysis . subsampling and compression are two key technologies for analyzing these datasets . proximus provides a technique for reducing large datasets into a much smaller set of representative patterns , on which traditional ( expensive ) analysis algorithms can be applied with minimal loss of accuracy . we show desirable properties of proximus in terms of runtime , scalability to large datasets , and performance in terms of capability to represent data in a compact form . we also demonstrate applications of proximus in association rule mining . in doing so , we establish proximus as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns . our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values ( near 100 % ) across a range of support thresholds while reducing the time required for association rule mining drastically .

['clustering', 'compressing discrete-valued vectors', 'non-orthogonal matrix decompositions', 'semi-discrete decomposition']


distributed multivariate regression based on influential observations large-scale data sets are sometimes logically and physically distributed in separate databases . the issues of mining these data sets are not just their sizes , but also the distributed nature . the complication is that communicating all the data to a central database would be too slow . to reduce communication costs , one could compress the data during transmission . another method is random sampling . we propose an approach for distributed multivariate regression based on sampling and discuss its relationship with the compression method . the central idea is motivated by the observation that , although communication is limited , each individual site can still scan and process all the data it holds . thus it is possible for the site to communicate only influential samples without seeing data in other sites . we exploit this observation and derive a method that provides tradeoff between communication cost and accuracy . experimental results show that it is better than the compression method and random sampling .

['distributed data mining', 'learning curve', 'multivariate linear regression', 'sampling']

sampling NOUN acomp is
sampling VERB pcomp on
sampling NOUN conj method

screening and interpreting multi-item associations based on log-linear modeling association rules have received a lot of attention in the data mining community since their introduction . the classical approach to find rules whose items enjoy high support ( appear in a lot of the transactions in the data set ) is , however , filled with shortcomings . it has been shown that support can be misleading as an indicator of how interesting the rule is . alternative measures , such as lift , have been proposed . more recently , a paper by dumouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that can not be explained by pairwise associations between the items involved . this approach , however , has its limitations , since it stops short of considering higher order interactions ( other than pairwise ) among the items . in this paper , we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items . since fitting loglinear models for large data sets can be computationally prohibitive , we apply graph-theoretical results to divide the original set of items into components ( sets of items ) that are statistically independent from each other . we then apply loglinear modeling to each of the components and find the interesting associations among items in them . the technique is experimentally evaluated with a real data set ( insurance data ) and a series of synthetic data sets . the results show that the technique is effective in finding interesting associations among the items involved .

['association rule', 'database applications', 'graphical model', 'log-linear model']


estimating the global pagerank of web communities localized search engines are small-scale systems that index a particular community on the web . they offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build , and can provide more precise and complete search capability over their relevant domains . one disadvantage such systems have over large-scale search engines is the lack of global pagerank values . such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole . in this paper , we present well-motivated algorithms to estimate the global pagerank values of a local domain . the algorithms are all highly scalable in that , given a local domain of size n , they use o ( n ) resources that include computation time , bandwidth , and storage . we test our methods across a variety of localized domains , including site-specific domains and topic-specific domains . we demonstrate that by crawling as few as n or 2n additional pages , our methods can give excellent global pagerank estimates .

['information search and retrieval', 'markov chain', 'numerical linear algebra', 'page rank', 'stochastic complementation']


passenger-based predictive modeling of airline no-show rates airlines routinely overbook flights based on the expectation that some fraction of booked passengers will not show for each flight . accurate forecasts of the expected number of no-shows for each flight can increase airline revenue by reducing the number of spoiled seats ( empty seats that might otherwise have been sold ) and the number of involuntary denied boardings at the departure gate . conventional no-show forecasting methods typically average the no-show rates of historically similar flights , without the use of passenger-specific information . we develop two classes of models to predict cabin-level no-show rates using specific information on the individual passengers booked on each flight . the first of these models computes the no-show probability for each passenger , using both the cabin-level historical forecast and the extracted passenger features as explanatory variables . this passenger-level model is implemented using three different predictive methods : a c4 .5 decision-tree , a segmented naive bayes algorithm , and a new aggregation method for an ensemble of probabilistic models . the second cabin-level model is formulated using the desired cabin-level no-show rate as the response variable . inputs to this model include the predicted cabin-level no-show rates derived from the various passenger-level models , as well as simple statistics of the features of the cabin passenger population . the cabin-level model is implemented using either linear regression , or as a direct probability model with explicit incorporation of the cabin-level no-show rates derived from the passenger-level model outputs . the new passenger-based models are compared to a conventional historical model , using train and evaluation data sets taken from over 1 million passenger name records . standard metrics such as lift curves and mean-square cabin-level errors establish the improved accuracy of the passenger-based models over the historical model . all models are also evaluated using a simple revenue model , and it is shown that the cabin-level passenger-based model can produce between 0.4 % and 3.2 % revenue gain over the conventional model , depending on the revenue-model parameters .

['airline overbooking', 'classification', 'model aggregation', 'no-show forecasting', 'predictive modeling', 'probabilistic estimation']


pattern discovery in sequences under a markov assumption in this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences . an important real-world problem of this nature is motif discovery in dna sequences . we investigate the fundamental aspects of this data mining problem that can make discovery `` easy '' or `` hard . '' we present a general framework for characterizing learning in this context by deriving the bayes error rate for this problem under a markov assumption . the bayes error framework demonstrates why certain patterns are much harder to discover than others . it also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery . we demonstrate how the bayes error can be used to calibrate existing discovery algorithms , providing a lower bound on achievable performance . we discuss a number of fundamental issues that characterize sequential pattern discovery in this context , present a variety of empirical results to complement and verify the theoretical analysis , and apply our methodology to real-world motif-discovery problems in computational biology .

['probabilistic algorithms']


efficient elastic burst detection in data streams burst detection is the activity of finding abnormal aggregates in data streams . such aggregates are based on sliding windows over data streams . in some applications , we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods . we will present a general data structure for detecting interesting aggregates over such elastic windows in near linear time . we present applications of the algorithm for detecting gamma ray bursts in large-scale astrophysical data . detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time trade and quote ( taq ) data from the new york stock exchange ( nyse ) . our algorithm beats the direct computation approach by several orders of magnitude .

['data stream', 'elastic burst']


activity monitoring : noticing interesting changes in behavior

['database administration', 'database applications', 'security and protection']


on-line unsupervised outlier detection using finite mixtures with discounting learning algorithms

['learning']

learning NOUN compound algorithms

mining unexpected rules by pushing user dynamics unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest . in this paper , we study three important issues that have been previously ignored in mining unexpected rules . first , the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario , in addition to the knowledge itself . second , the prior knowledge should be considered right from the start to focus the search on unexpected rules . third , the unexpectedness of a rule depends on what other rules the user has seen so far . thus , only rules that remain unexpected given what the user has seen should be considered interesting . we develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules .

['association rule', 'subjective interestingness', 'unexpected rule']


mining gps data to augment road models

['background knowledge', 'case studies', 'evaluating knowledge and potential discoveries', 'implementation and use of kdd systems', 'incremental algorithms', 'noisy data']


on demand classification of data streams current models of the classification problem do not effectively handle bursts of particular classes coming in at different times . in fact , the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets . our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing streams are used for dynamic classification of data sets . this model reflects real life situations effectively , since it is desirable to classify test streams in real time over an evolving training and test stream . the aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream . in order to achieve this goal , we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier . the empirical results indicate that the system maintains a high classification accuracy in an evolving data stream , while providing an efficient solution to the classification task .

['classification', 'data streams']

classification NOUN compound problem
classification NOUN compound modeling
classification NOUN pobj for
classification NOUN compound problem
classification NOUN pobj for
classification NOUN compound system
classification NOUN compound process
classification NOUN compound accuracy
classification NOUN compound task

b-em : a classifier incorporating bootstrap with em approach for data mining this paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy . this is significant for many applications such as image classification where obtaining classification labels is expensive , while large unlabeled examples are easily available . we investigate an expectation maximization ( em ) algorithm for learning from labeled and unlabeled data . the reason why unlabeled data boosts learning accuracy is because it provides the information about the joint probability distribution . a theoretical argument shows that the more unlabeled examples are combined in learning , the more accurate the result . we then introduce b-em algorithm , based on the combination of em with bootstrap method , to exploit the large unlabeled data while avoiding prohibitive i\/o cost . experimental results over both synthetic and real data sets that the proposed approach has a satisfactory performance .

['bootstrap method', 'classification', 'expectation maximization', 'miscellaneous', 'supervised and unsupervised learning']

classification NOUN compound accuracy
classification NOUN pobj as
classification NOUN compound labels

probabilistic discovery of time series motifs several important time series data mining problems reduce to the core task of finding approximately repeated subsequences in a longer time series . in an earlier work , we formalized the idea of approximately repeated subsequences by introducing the notion of time series motifs . two limitations of this work were the poor scalability of the motif discovery algorithm , and the inability to discover motifs in the presence of noise . here we address these limitations by introducing a novel algorithm inspired by recent advances in the problem of pattern discovery in biosequences . our algorithm is probabilistic in nature , but as we show empirically and theoretically , it can find time series motifs with very high probability even in the presence of noise or `` do n't care '' symbols . not only is the algorithm fast , but it is an anytime algorithm , producing likely candidate motifs almost immediately , and gradually improving the quality of results over time .

['data mining', 'database applications', 'motifs', 'randomized algorithms', 'time series']

motifs NOUN ROOT motifs
motifs NOUN pobj of
motifs NOUN dobj discover
motifs ADV ccomp find
motifs NOUN dobj producing

approximating a collection of frequent sets one of the most well-studied problems in data mining is computing the collection of frequent item sets in large transactional databases . one obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users . even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem . in this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem : what are the k sets that best approximate a collection of frequent item sets ? our measure of approximating a collection of sets by k sets is defined to be the size of the collection covered by the the k sets , i.e. , the part of the collection that is included in one of the k sets . we also specify a bound on the number of extra sets that are allowed to be covered . we examine different problem variants for which we demonstrate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms . we give empirical evidence showing that the approximation methods work well in practice .

['foundations of data mining', 'mining frequent itemsets', 'nonnumerical algorithms and problems']


fully automatic cross-associations large , sparse binary matrices arise in numerous data mining applications , such as the analysis of market baskets , web graphs , social networks , co-citations , as well as information retrieval , collaborative filtering , sparse matrix reordering , etc. . virtually all popular methods for the analysis of such matrices -- e.g. , k-means clustering , metis graph partitioning , svd\/pca and frequent itemset mining -- require the user to specify various parameters , such as the number of clusters , number of principal components , number of partitions , and `` support . '' choosing suitable values for such parameters is a challenging problem.cross-association is a joint decomposition of a binary matrix into disjoint row and column groups such that the rectangular intersections of groups are homogeneous . starting from first principles , we furnish a clear , information-theoretic criterion to choose a good cross-association as well as its parameters , namely , the number of row and column groups . we provide scalable algorithms to approach the optimal . our algorithm is parameter-free , and requires no user intervention . in practice it scales linearly with the problem size , and is thus applicable to very large matrices . finally , we present experiments on multiple synthetic and real-life datasets , where our method gives high-quality , intuitive results .

['content analysis and indexing', 'cross-association', 'mdl']


on interactive visualization of high-dimensional data using the hyperbolic plane we propose a novel projection based visualization method for high-dimensional datasets by combining concepts from mds and the geometry of the hyperbolic spaces . our approach hyperbolic multi-dimensional scaling ( h-mds ) extends earlier work ( 7 ) using hyperbolic spaces for visualization of tree structures data ( `` hyperbolic tree browser '' ) . by borrowing concepts from multi-dimensional scaling we map proximity data directly into the 2-dimensional hyperbolic space ( h2 ) . this removes the restriction to `` quasihierarchical '' , graph-based data -- limiting previous work . since a suitable distance function can convert all kinds of data to proximity ( or distance-based ) data this type of data can be considered the most general . we used the circular poincarã© model of the h2 which allows effective human-computer interaction : by moving the `` focus '' via mouse the user can navigate in the data without loosing the `` context '' . in h2 the `` fish-eye '' behavior originates not simply by a non-linear view transformation but rather by extraordinary , non-euclidean properties of the h2 . especially , the exponential growth of length and area of the underlying space makes the h2 a prime target for mapping hierarchical and ( now also ) high-dimensional data . we present several high-dimensional mapping examples including synthetic and real world data and a successful application for unstructured text . by analyzing and integrating multiple film critiques from news : rec . art . movies . reviews and the internet movie database , each movie becomes placed within the h2 . here the idea is , that related films share more words in their reviews than unrelated . their semantic proximity leads to a closer arrangement . the result is a kind of high-level content structured display allowing the user to explore the `` space of movies '' .

['focus+context', 'h-mds', 'hyperbolic multi-dimensional scaling', 'infoviz', 'interaction styles', 'semantic browsing', 'text mining', 'visualizing high-dimensional data']


similarity analysis on government regulations government regulations are semi-structured text documents that are often voluminous , heavily cross-referenced between provisions and even ambiguous . multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes . in this work , we propose a framework for regulation management and similarity analysis . an online repository for legal documents is created with the help of text mining tool , and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts . our similarity analysis core identifies relevant provisions and brings them to the user 's attention , and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions . preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons .

['legal informatics', 'regulations', 'similarity analysis', 'text mining']

regulations NOUN pobj of
regulations NOUN pobj of

locating secret messages in images steganography involves hiding messages in innocuous media such as images , while steganalysis is the field of detecting these secret messages . the ultimate goal of steganalysis is two-fold : making a binary classification of a file as stego-bearing or innocent , and secondly , locating the hidden message with an aim to extracting , sterilizing or manipulating it . almost all steganalysis approaches ( known as attacks ) focus on the first of these two issues . in this paper , we explore the difficult related problem : given that we know an image file contains steganography , locate which pixels contain the message . we treat the hidden message location problem as outlier detection using probability\/energy measures of images motivated by the image restoration community . pixels contributing the most to the energy calculations of an image are deemed outliers . typically , of the top third of one percent of most energized pixels ( outliers ) , we find that 87 % are stego-bearing in color images and 61 % in grayscale images . in all image types only 1 % of all pixels are stego-bearing indicating our techniques provides a substantial lift over random guessing .

['outlier detection', 'steganalysis', 'steganography']

steganography NOUN pobj in
steganalysis NOUN nsubj is
steganalysis NOUN pobj of
steganalysis NOUN compound approaches
steganography NOUN dobj contains

closegraph : mining closed frequent graph patterns recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees , lattices , and graphs . as a general data structure , graph can model complicated relations among data with wide applications in bioinformatics , web exploration , and etc. . however , mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs . instead of mining all the subgraphs , we propose to mine closed frequent graph patterns . a graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. a closed graph pattern mining algorithm , closegraph , is developed by exploring several interesting pruning methods . our performance study shows that closegraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining , especially in the presence of large graph patterns .

['canonical label', 'closed pattern', 'database applications', 'frequent graph', 'graph representation']


identifying early buyers from purchase data market research has shown that consumers exhibit a variety of different purchasing behaviors ; specifically , some tend to purchase products earlier than other consumers . identifying such early buyers can help personalize marketing strategies , potentially improving their effectiveness . in this paper , we present a non-parametric approach to the problem of identifying early buyers from purchase data . our formulation takes as inputs the detailed purchase information of each consumer , with which we construct a weighted directed graph whose nodes correspond to consumers and whose edges correspond to purchases consumers have in common ; the edge weights indicate how frequently consumers purchase products earlier than other consumers . identifying early buyers corresponds to the problem of finding a subset of nodes in the graph with maximum difference between the weights of the outgoing and incoming edges . this problem is a variation of the maximum cut problem in a directed graph . we provide an approximation algorithm based on semidefinite programming ( sdp ) relaxations pioneered by goemans and williamson , and analyze its performance . we apply the algorithm to real purchase data from amazon.com , providing new insights into consumer behaviors .

['consumer behavior', 'early buyers', 'semidefinite programming', 'social network']

early buyers ADJ dobj identifying

sampling from large graphs given a huge real graph , how can we derive a representative sample ? there are many known algorithms to compute interesting measures ( shortest paths , centrality , betweenness , etc. ) , but several of them become impractical for large graphs . thus graph sampling is essential . the natural questions to ask are ( a ) which sampling method to use , ( b ) how small can the sample size be , and ( c ) how to scale up the measurements of the sample ( e.g. , the diameter ) , to get estimates for the large graph . the deeper , underlying question is subtle : how do we measure success ? . we answer the above questions , and test our answers by thorough experiments on several , diverse datasets , spanning thousands nodes and edges . we consider several sampling methods , propose novel methods to check the goodness of sampling , and develop a set of scaling laws that describe relations between the properties of the original and the sample . in addition to the theoretical contributions , the practical conclusions from our work are : sampling strategies based on edge selection do not perform well ; simple uniform random node selection performs surprisingly well . overall , best performing methods are the ones based on random-walks and `` forest fire '' ; they match very accurately both static as well as evolutionary graph patterns , with sample sizes down to about 15 % of the original graph .

['graph mining', 'graph sampling', 'scaling laws']


sampling-based sequential subgroup mining subgroup discovery is a learning task that aims at finding interesting rules from classified examples . the search is guided by a utility function , trading off the coverage of rules against their statistical unusualness . one shortcoming of existing approaches is that they do not incorporate prior knowledge . to this end a novel generic sampling strategy is proposed . it allows to turn pattern mining into an iterative process . in each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns . the result of this technique is a small diverse set of understandable rules that characterise a specified property of interest . as another contribution this article derives a simple connection between subgroup discovery and classifier induction . for a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling . the proposed techniques are empirically compared to state of the art subgroup discovery algorithms .

['prior knowledge', 'sampling', 'subgroup discovery']

sampling NOUN amod strategy

turning cartwheels : an alternating algorithm for mining redescriptions we present an unusual algorithm involving classification trees -- cartwheels -- where two trees are grown in opposite directions so that they are joined at their leaves . this approach finds application in a new data mining task we formulate , called redescription mining . a redescription is a shift-of-vocabulary , or a different way of communicating information about a given subset of data ; the goal of redescription mining is to find subsets of data that afford multiple descriptions . we highlight the importance of this problem in domains such as bioinformatics , which exhibit an underlying richness and diversity of data descriptors ( e.g. , genes can be studied in a variety of ways ) . cartwheels exploits the duality between class partitions and path partitions in an induced classification tree to model and mine redescriptions . it helps integrate multiple forms of characterizing datasets , situates the knowledge gained from one dataset in the context of others , and harnesses high-level abstractions for uncovering cryptic and subtle features of data . algorithm design decisions , implementation details , and experimental results are presented .

['classification trees', 'data mining in biological domains', 'learning', 'redescriptions']

redescriptions NOUN pobj for
redescriptions NOUN conj model

very sparse random projections there has been considerable interest in random projections , an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space . let a in rn x d be our n points in d dimensions . the method multiplies a by a random matrix r in rd x k , reducing the d dimensions down to just k for speeding up the computation . r typically consists of entries of standard normal n ( 0,1 ) . it is well known that random projections preserve pairwise distances ( in the expectation ) . achlioptas proposed sparse random projections by replacing the n ( 0,1 ) entries in r with entries in -1,0,1 with probabilities 1\/6 , 2\/3 , 1\/6 , achieving a threefold speedup in processing time . we recommend using r of entries in -1,0,1 with probabilities 1\/2 âˆš d , 1-1 âˆš d , 1\/2 âˆš d for achieving a significant âˆš d-fold speedup , with little loss in accuracy .

['random projections', 'rates of convergence', 'sampling']


clinical and financial outcomes analysis with existing hospital patient records existing patient records are a valuable resource for automated outcomes analysis and knowledge discovery . however , key clinical data in these records is typically recorded in unstructured form as free text and images , and most structured clinical information is poorly organized . time-consuming interpretation and analysis is required to convert these records into structured clinical data . thus , only a tiny fraction of this resource is utilized . we present remind , a bayesian framework for reliable extraction and meaningful inference from nonstructured data . remind integrates and blends the structured and unstructured clinical data in patient records to automatically created high-quality structured clinical data . this structuring allows existing patient records to be mined for quality assurance , regulatory compliance , and to relate financial and clinical factors . we demonstrate remind on two medical applications : ( a ) extract `` recurrence '' , the key outcome for measuring treatment effectiveness , for colon cancer patients ( ii ) extract key diagnoses and complications for acute myocardial infarction ( heart attack ) patients , and demonstrate the impact of these clinical factors on financial outcomes .

['bayes nets', 'data mining', 'database applications', 'hmms', 'temporal reasoning']


knowledge-based data mining we describe techniques for combining two types of knowledge systems : expert and machine learning . both the expert system and the learning system represent information by logical decision rules or trees . unlike the classical views of knowledge-base evaluation or refinement , our view accepts the contents of the knowledge base as completely correct . the knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules . an expert system called seas was built to discover sales leads for computer products and solutions . the system interviews executives by asking questions , and based on the responses , recommends products that may improve a business ' operations . leveraging this expert system , we record the results of the interviews and the program 's recommendations . the very same data stored by the expert system is used to find new predictive rules . among the potential advantages of this approach are ( a ) the capability to spot new sales trends and ( b ) the substitution of less expensive probabilistic rules that use database data instead of interviews .

['database applications', 'decision rule induction', 'expert systems', 'sales leads']


automatic identification of quasi-experimental designs for discovering causal knowledge researchers in the social and behavioral sciences routinely rely on quasi-experimental designs to discover knowledge from large data-bases . quasi-experimental designs ( qeds ) exploit fortuitous circumstances in non-experimental data to identify situations ( sometimes called `` natural experiments '' ) that provide the equivalent of experimental control and randomization . qeds allow researchers in domains as diverse as sociology , medicine , and marketing to draw reliable inferences about causal dependencies from non-experimental data . unfortunately , identifying and exploiting qeds has remained a painstaking manual activity , requiring researchers to scour available databases and apply substantial knowledge of statistics . however , recent advances in the expressiveness of databases , and increases in their size and complexity , provide the necessary conditions to automatically identify qeds . in this paper , we describe the first system to discover knowledge by applying quasi-experimental designs that were identified automatically . we demonstrate that qeds can be identified in a traditional database schema and that such identification requires only a small number of extensions to that schema , knowledge about quasi-experimental design encoded in first-order logic , and a theorem-proving engine . we describe several key innovations necessary to enable this system , including methods for automatically constructing appropriate experimental units and for creating aggregate variables on those units . we show that applying the resulting designs can identify important causal dependencies in real domains , and we provide examples from academic publishing , movie making and marketing , and peer-production systems . finally , we discuss the integration of qeds with other approaches to causal discovery , including joint modeling and directed experimentation .

['causal discovery', 'quasi-experimental design']


training linear svms in linear time linear support vector machines ( svms ) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification , word-sense disambiguation , and drug design . these applications involve a large number of examples n as well as a large number of features n , while each example has only s ( ( n non-zero features . this paper presents a cutting plane algorithm for training linear svms that provably has training time 0 ( s , n ) for classification problems and o ( sn log ( n ) ) for ordinal regression problems . the algorithm is based on an alternative , but equivalent formulation of the svm optimization problem . empirically , the cutting-plane algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets .

['large-scale problems', 'learning', 'ordinal regression', 'roc-area', 'support vector machines', 'training algorithms']

learning NOUN acl machine

efficient and effective explanation of change in hierarchical summaries dimension attributes in data warehouses are typically hierarchical ( e.g. , geographic locations in sales data , urls in web traffic logs ) . olap tools are used to summarize the measure attributes ( e.g. , total sales ) along a dimension hierarchy , and to characterize changes ( e.g. , trends and anomalies ) in a hierarchical summary over time . when thenumber of changes identified is large ( e.g. , total sales in many stores differed from their expected values ) , a parsimonious explanation of the most significant changes is desirable . in this paper , we propose a natural model of parsimonious explanation , as a composition of node weights along the root-to-leaf paths in a dimension hierarchy , which permits changes to be aggregated with maximal generalization along the dimension hierarchy . we formalize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious explanations on arbitrary rooted one dimensional tree hierarchies . we show that such explanations can be computed efficiently in time essentially proportional to the number of leaves and the depth of the hierarchy . further , our method can produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals , making it widely applicable . our experiments use real data sets to demonstrate the utility and robustness of our proposed model for explaining significant changes , as well as its superior parsimony compared to alternatives .

['change', 'hierarchical summary', 'olap', 'parsimonious explanations', 'statistical model']

change NOUN pobj of
olap NOUN compound tools

anomaly pattern detection in categorical datasets we propose a new method for detecting patterns of anomalies in categorical datasets . we assume that anomalies are generated by some underlying process which affects only a particular subset of the data . our method consists of two steps : we first use a `` local anomaly detector '' to identify individual records with anomalous attribute values , and then detect patterns where the number of anomalous records is higher than expected . given the set of anomalies flagged by the local anomaly detector , we search over all subsets of the data defined by any set of fixed values of a subset of the attributes , in order to detect self-similar patterns of anomalies . we wish to detect any such subset of the test data which displays a significant increase in anomalous activity as compared to the normal behavior of the system ( as indicated by the training data ) . we perform significance testing to determine if the number of anomalies in any subset of the test data is significantly higher than expected , and propose an efficient algorithm to perform this test over all such subsets of the data . we show that this algorithm is able to accurately detect anomalous patterns in real-world hospital , container shipping and network intrusion data .

['anomaly detection', 'machine learning', 'pattern detection']


learning classifiers from only positive and unlabeled data the input to an algorithm that learns a binary classifier normally consists of two sets of examples , where one set consists of positive examples of the concept to be learned , and the other set consists of negative examples . however , it is often the case that the available training data are an incomplete set of positive examples , and a set of unlabeled examples , some of which are positive and some of which are negative . the problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature . under the assumption that the labeled examples are selected randomly from the positive examples , we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive . we show how to use this result in two different ways to learn a classifier from a nontraditional training set . we then apply these two new methods to solve a real-world problem : identifying protein records that should be included in an incomplete specialized molecular biology database . our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased svm method for learning from positive and unlabeled examples .

['bioinformatics', 'supervised learning', 'text mining', 'unlabeled examples']


a sequential dual method for large scale multi-class linear svms efficient training of direct multi-class formulations of linear support vector machines is very useful in applications such as text classification with a huge number examples as well as features . this paper presents a fast dual method for this training . the main idea is to sequentially traverse through the training set and optimize the dual variables associated with one example at a time . the speed of training is enhanced further by shrinking and cooling heuristics . experiments indicate that our method is much faster than state of the art solvers such as bundle , cutting plane and exponentiated gradient methods .

['multi-class', 'support vector machines', 'text classification']


anonymizing transaction databases for publication this paper considers the problem of publishing `` transaction data '' for research purposes . each transaction is an arbitrary set of items chosen from a large universe . detailed transaction data provides an electronic image of one 's life . this has two implications . one , transaction data are excellent candidates for data mining research . two , use of transaction data would raise serious concerns over individual privacy . therefore , before transaction data is released for data mining , it must be made anonymous so that data subjects can not be re-identified . the challenge is that transaction data has no structure and can be extremely high dimensional . traditional anonymization methods lose too much information on such data . to date , there has been no satisfactory privacy notion and solution proposed for anonymizing transaction data . this paper proposes one way to address this issue .

['anonymity', 'data publishing', 'transaction database']


spectral domain-transfer learning traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain . in many real world applications , however , we wish to make use of the labeled data from one domain ( called in-domain ) to classify the unlabeled data in a different domain ( out-of-domain ) . this problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain . in general , this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain . in this paper , we formulate this domain-transfer learning problem under a novel spectral classification framework , where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure . through optimization of the cost function , the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain . we conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many state-of-the-art algorithms .

['learning', 'spectral learning', 'transfer learning']

learning VERB acl transfer
learning NOUN compound problem
learning NOUN compound problem

arnetminer : extraction and mining of academic social networks this paper addresses several key issues in the arnetminer system , which aims at extracting and mining academic social networks . specifically , the system focuses on : 1 ) extracting researcher profiles automatically from the web ; 2 ) integrating the publication data into the network from existing digital libraries ; 3 ) modeling the entire academic network ; and 4 ) providing search services for the academic network . so far , 448,470 researcher profiles have been extracted using a unified tagging approach . we integrate publications from online web databases and propose a probabilistic framework to deal with the name ambiguity problem . furthermore , we propose a unified modeling approach to simultaneously model topical aspects of papers , authors , and publication venues . search services such as expertise search and people association search have been provided based on the modeling results . in this paper , we describe the architecture and main features of the system . we also present the empirical evaluation of the proposed methods .

['association search', 'expertise search', 'information extraction', 'information search and retrieval', 'name disambiguation', 'social network', 'topic modeling']


relational learning via collective matrix factorization relational learning is concerned with predicting unknown values of a relation , given a database of entities and observed relations among entities . an example of relational learning is movie rating prediction , where entities could include users , movies , genres , and actors . relations encode users ' ratings of movies , movies ' genres , and actors ' roles in movies . a common prediction technique given one pairwise relation , for example a #users x #movies ratings matrix , is low-rank matrix factorization . in domains with multiple relations , represented as multiple matrices , we may improve predictive accuracy by exploiting information from one relation while predicting another . to this end , we propose a collective matrix factorization model : we simultaneously factor several matrices , sharing parameters among factors when an entity participates in multiple relations . each relation can have a different value type and error distribution ; so , we allow nonlinear relationships between the parameters and outputs , using bregman divergences to measure error . we extend standard alternating projection algorithms to our model , and derive an efficient newton update for the projection . furthermore , we propose stochastic optimization methods to deal with large , sparse matrices . our model generalizes several existing matrix factorization methods , and therefore yields new large-scale optimization algorithms for these problems . our model can handle any pairwise relational schema and a wide variety of error models . we demonstrate its efficiency , as well as the benefit of sharing parameters among relations .

['matrix factorization', 'relational learning', 'stochastic approximation']


mobile call graphs : beyond power-law and lognormal distributions we analyze a massive social network , gathered from the records of a large mobile phone operator , with more than a million users and tens of millions of calls . we examine the distributions of the number of phone calls per customer ; the total talk minutes per customer ; and the distinct number of calling partners per customer . we find that these distributions are skewed , and that they significantly deviate from what would be expected by power-law and lognormal distributions . to analyze our observed distributions ( of number of calls , distinct call partners , and total talk time ) , we propose powertrack , a method which fits a lesser known but more suitable distribution , namely the double pareto lognormal ( dpln ) distribution , to our data and track its parameters over time . using powertrack , we find that our graph changes over time in a way consistent with a generative process that naturally results in the dpln distributions we observe . furthermore , we show that this generative process lends itself to a natural and appealing social wealth interpretation in the context of social networks such as ours . we discuss the application of those results to our model and to forecasting .

['distribution', 'dpln', 'generative process', 'power laws']

distribution NOUN dobj fits
dpln NOUN nmod distribution
distribution NOUN appos distribution
dpln NOUN compound distributions

generating succinct titles for web urls how can a search engine automatically provide the best and most appropriate title for a result url ( link-title ) so that users will be persuaded to click on the url ? we consider the problem of automatically generating link-titles for urls and propose a general statistical framework for solving this problem . the framework is based on using information from a diverse collection of sources , each of which can be thought of as contributing one or more candidate link-titles for the url . it can also incorporate the context in which the link-title will be used , along with constraints on its length . our framework is applicable to several scenarios : obtaining succinct titles for displaying quicklinks , obtaining titles for urls that lack a good title , constructing succinct sitemaps , etc. . extensive experiments show that our method is very effective , producing results that are at least 20 % better than non-trivial baselines .

['information search and retrieval', 'quicklinks', 'sitemaps', 'web page title generation']

quicklinks NOUN dobj displaying
sitemaps NOUN dobj constructing

feedback effects between similarity and social influence in online communities a fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties . people are similar to their neighbors in a social network for two distinct reasons : first , they grow to resemble their current friends due to social influence ; and second , they tend to form new links to others who are already like them , a process often termed selection by sociologists . while both factors are present in everyday social processes , they are in tension : social influence can push systems toward uniformity of behavior , while selection can lead to fragmentation . as such , it is important to understand the relative effects of these forces , and this has been a challenge due to the difficulty of isolating and quantifying them in real settings . we develop techniques for identifying and modeling the interactions between social influence and selection , using data from online communities where both social interaction and changes in behavior over time can be measured . we find clear feedback effects between the two factors , with rising similarity between two individuals serving , in aggregate , as an indicator of future interaction -- but with similarity then continuing to increase steadily , although at a slower rate , for long periods after initial interactions . we also consider the relative value of similarity and social influence in modeling future behavior . for instance , to predict the activities that an individual is likely to do next , is it more useful to know the current activities of their friends , or of the people most similar to them ?

['online communities', 'social influence', 'social networks']


mining adaptively frequent closed unlabeled rooted trees in data streams closed patterns are powerful representatives of frequent patterns , since they eliminate redundant information . we propose a new approach for mining closed unlabeled rooted trees adaptively from data streams that change over time . our approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees , and leads to an on-line strategy and an adaptive sliding window technique for dealing with changes over time . more precisely , we first present a general methodology to identify closed patterns in a data stream , using galois lattice theory . using this methodology , we then develop three closed tree mining algorithms : an incremental one inctreenat , a sliding-window based one , wintreenat , and finally one that mines closed trees adaptively from data streams , adatreenat . to the best of our knowledge this is the first work on mining frequent closed trees in streaming data varying with time . we give a first experimental evaluation of the proposed algorithms .

['closed mining', 'concept drift', 'data streams', 'patterns', 'trees']

trees NOUN dobj mining
patterns NOUN nsubj are
patterns NOUN pobj of
trees NOUN dobj closed
trees NOUN pobj of
trees NOUN pobj of
patterns NOUN dobj identify
trees NOUN dobj closed
trees NOUN pobj on

succinct summarization of transactional databases : an overlapped hyperrectangle scheme transactional data are ubiquitous . several methods , including frequent itemsets mining and co-clustering , have been proposed to analyze transactional databases . in this work , we propose a new research problem to succinctly summarize transactional databases . solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets . we formulate this problem as a set covering problem using overlapped hyperrectangles ; we then prove that this problem and its several variations are np-hard . we develop an approximation algorithm hyper which can achieve a ln ( k ) + 1 approximation ratio in polynomial time . we propose a pruning strategy that can significantly speed up the processing of our algorithm . additionally , we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions . a detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases .

['hyperrectangle', 'set cover', 'summarization', 'transactional databases']

summarization NOUN nsubj are
hyperrectangle ADJ amod data

effective and efficient itemset pattern summarization : regression-based approaches in this paper , we propose a set of novel regression-based approaches to effectively and efficiently summarize frequent itemset patterns . specifically , we show that the problem of minimizing the restoration error for a set of itemsets based on a probabilistic model corresponds to a non-linear regression problem . we show that under certain conditions , we can transform the nonlinear regression problem to a linear regression problem . we propose two new methods , k-regression and tree-regression , to partition the entire collection of frequent itemsets in order to minimize the restoration error . the k-regression approach , employing a k-means type clustering method , guarantees that the total restoration error achieves a local minimum . the tree-regression approach employs a decision-tree type of top-down partition process . in addition , we discuss alternatives to estimate the frequency for the collection of itemsets being covered by the k representative itemsets . the experimental evaluation on both real and synthetic datasets demonstrates that our approaches significantly improve the summarization performance in terms of both accuracy ( restoration error ) , and computational cost .

['frequency restoration', 'pattern summarization', 'regression']

regression NOUN npadvmod based
regression NOUN npadvmod based
regression NOUN compound problem
regression NOUN compound problem
regression NOUN compound problem
regression NOUN appos methods
regression NOUN conj regression
regression NOUN compound approach
regression NOUN compound approach

combinational collaborative filtering for personalized community recommendation rapid growth in the amount of data available on social networking sites has made information retrieval increasingly challenging for users . in this paper , we propose a collaborative filtering method , combinational collaborative filtering ( ccf ) , to perform personalized community recommendations by considering multiple types of co-occurrences in social data at the same time . this filtering method fuses semantic and user information , then applies a hybrid training strategy that combines gibbs sampling and expectation-maximization algorithm . to handle the large-scale dataset , parallel computing is used to speed up the model training . through an empirical study on the orkut dataset , we show ccf to be both effective and scalable .

['collaborative filtering', 'miscellaneous', 'personalized recommendation', 'probabilistic models']


reconstructing chemical reaction networks : data mining meets system identification we present an approach to reconstructing chemical reaction networks from time series measurements of the concentrations of the molecules involved . our solution strategy combines techniques from numerical sensitivity analysis and probabilistic graphical models . by modeling a chemical reaction system as a markov network ( undirected graphical model ) , we show how systematically probing for sensitivities between molecular species can identify the topology of the network . given the topology , our approach next uses detailed sensitivity profiles to characterize properties of reactions such as reversibility , enzyme-catalysis , and the precise stoichiometries of the reactants and products . we demonstrate applications to reconstructing key biological systems including the yeast cell cycle . in addition to network reconstruction , our algorithm finds applications in model reduction and model comprehension . we argue that our reconstruction algorithm can serve as an important primitive for data mining in systems biology applications .

['graphical models', 'markov networks', 'network reconstruction', 'ordinary differential equations', 'systems biology']


unsupervised feature selection for principal components analysis principal components analysis ( pca ) is the predominant linear dimensionality reduction technique , and has been widely applied on datasets in all scientific domains . we consider , both theoretically and empirically , the topic of unsupervised feature selection for pca , by leveraging algorithms for the so-called column subset selection problem ( cssp ) . in words , the cssp seeks the `` best '' subset of exactly k columns from an m x n data matrix a , and has been extensively studied in the numerical linear algebra community . we present a novel two-stage algorithm for the cssp . from a theoretical perspective , for small to moderate values of k , this algorithm significantly improves upon the best previously-existing results ( 24 , 12 ) for the cssp . from an empirical perspective , we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis : finance , document-term data , and genetics . we pay particular attention to how this algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner . in all three application domains , we are able to identify k landmark features , i.e. , columns of the data matrix , that capture nearly the same amount of information as does the subspace that is spanned by the top k `` eigenfeatures . ''

['numerical linear algebra', 'pca', 'random sampling', 'subset selection']

pca NOUN appos analysis
pca NOUN pobj for

privacy-preserving cox regression for survival analysis privacy-preserving data mining ( ppdm ) is an emergent research area that addresses the incorporation of privacy preserving concerns to data mining techniques . in this paper we propose a privacy-preserving ( pp ) cox model for survival analysis , and consider a real clinical setting where the data is horizontally distributed among different institutions . the proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem . our approach differs from the commonly used random projection approach since it instead finds a projection that is optimal at preserving the properties of the data that are important for the specific problem at hand . since our proposed approach produces an sparse mapping , it also generates a pp mapping that not only projects the data to a lower dimensional space but it also depends on a smaller subset of the original features ( it provides explicit feature selection ) . real data from several european healthcare institutions are used to test our model for survival prediction of non-small-cell lung cancer patients . these results are also confirmed using publicly available benchmark datasets . our experimental results show that we are able to achieve a near-optimal performance without directly sharing the data across different data sources . this model makes it possible to conduct large-scale multi-centric survival analysis without violating privacy-preserving requirements .

['cox regression', 'miscellaneous', 'privacy-preserving data mining', 'survival analysis']


application of kernels to link analysis the application of kernel methods to link analysis is explored . in particular , kandola et al. 's neumann kernels are shown to subsume not only the co-citation and bibliographic coupling relatedness but also kleinberg 's hits importance . these popular measures of relatedness and importance correspond to the neumann kernels at the extremes of their parameter range , and hence these kernels can be interpreted as defining a spectrum of link analysis measures intermediate between co-citation\/bibliographic coupling and hits . we also show that the kernels based on the graph laplacian , including the regularized laplacian and diffusion kernels , provide relatedness measures that overcome some limitations of co-citation relatedness . the property of these kernel-based link analysis measures is examined with a network of bibliographic citations . practical issues in applying these methods to real data are discussed , and possible solutions are proposed .

['co-citation coupling', 'graph kernel', 'hits', 'information search and retrieval', 'link analysis']

hits NOUN compound importance
hits NOUN conj coupling

partial least squares regression for graph mining attributed graphs are increasingly more common in many application domains such as chemistry , biology and text processing . a central issue in graph mining is how to collect informative subgraph patterns for a given learning task . we propose an iterative mining method based on partial least squares regression ( pls ) . to apply pls to graph data , a sparse version of pls is developed first and then it is combined with a weighted pattern mining algorithm . the mining algorithm is iteratively called with different weight vectors , creating one latent component per one mining call . our method , graph pls , is efficient and easy to implement , because the weight vector is updated with elementary matrix calculations . in experiments , our graph pls algorithm showed competitive prediction accuracies in many chemical datasets and its efficiency was significantly superior to graph boosting ( gboost ) and the naive method based on frequent graph mining .

['chemoinformatics', 'graph boosting', 'graph mining', 'partial least squares regression']


get another label ? improving data quality and data mining using multiple , noisy labelers this paper addresses the repeated acquisition of labels for data items when the labeling is imperfect . we examine the improvement ( or lack thereof ) in data quality via repeated labeling , and focus especially on the improvement of training labels for supervised induction . with the outsourcing of small tasks becoming easier , for example via rent-a-coder or amazon 's mechanical turk , it often is possible to obtain less-than-expert labeling at low cost . with low-cost labeling , preparing the unlabeled part of the data can become considerably more expensive than labeling . we present repeated-labeling strategies of increasing complexity , and show several main results . ( i ) repeated-labeling can improve label quality and model quality , but not always . ( ii ) when labels are noisy , repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap . ( iii ) as soon as the cost of processing the unlabeled data is not free , even the simple strategy of labeling everything multiple times can give considerable advantage . ( iv ) repeatedly labeling a carefully chosen set of points is generally preferable , and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved . the bottom line : the results show clearly that when labeling is not perfect , selective acquisition of multiple labels is a strategy that data miners should have in their repertoire ; for certain label-quality\/cost regimes , the benefit is substantial .

['data preprocessing', 'data selection']


factorization meets the neighborhood : a multifaceted collaborative filtering model recommender systems provide users with personalized suggestions for products or services . these systems often rely on collaborating filtering ( cf ) , where past transactions are analyzed in order to establish connections between users and products . the two more successful approaches to cf are latent factor models , which directly profile both users and products , and neighborhood models , which analyze similarities between products or users . in this work we introduce some innovations to both approaches . the factor and neighborhood models can now be smoothly merged , thereby building a more accurate combined model . further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users . the methods are tested on the netflix data . results are better than those previously published on that dataset . in addition , we suggest a new evaluation metric , which highlights the differences among methods , based on their performance at a top-k recommendation task .

['collaborative filtering', 'recommender systems']


using ghost edges for classification in sparsely labeled networks we address the problem of classification in partially labeled networks ( a.k.a. within-network classification ) where observed class labels are sparse . techniques for statistical relational learning have been shown to perform well on network classification tasks by exploiting dependencies between class labels of neighboring nodes . however , relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning ( during training phase ) and\/or inference ( during testing phase ) . this situation arises in real-world problems when observed labels are sparse . in this paper , we propose a novel approach to within-network classification that combines aspects of statistical relational learning and semi-supervised learning to improve classification performance in sparse networks . our approach works by adding `` ghost edges '' to a network , which enable the flow of information from labeled to unlabeled nodes . through experiments on real-world data sets , we demonstrate that our approach performs well across a range of conditions where existing approaches , such as collective classification and semi-supervised learning , fail . on all tasks , our approach improves area under the roc curve ( auc ) by up to 15 points over existing approaches . furthermore , we demonstrate that our approach runs in time proportional to l â€¢ e , where l is the number of labeled nodes and e is the number of edges .

['collective classification', 'random walk', 'semi-supervised learning', 'statistical relational learning']


mining frequent item sets by opportunistic projection in this paper , we present a novel algorithm opportune project for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree . our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures , array-based or tree-based , to represent projected transaction subsets , and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets . more importantly , we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets , which makes our algorithm both cpu time efficient and memory saving . basically , the algorithm grows the frequent item set tree by depth first search , whereas breadth first search is used to build the upper portion of the tree if necessary . we test our algorithm versus several other algorithms on real world datasets , such as bms-pos , and on ibm artificial datasets . the empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold , but also highly scalable to very large databases .

['association rules', 'frequent patterns']


fast vertical mining using diffsets a number of vertical mining algorithms have been proposed recently for association mining , which have shown to be very effective and usually outperform horizontal approaches . the main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids ( tids ) and automatic pruning of irrelevant data . the main problem with these approaches is when intermediate results of vertical tid lists become too large for memory , thus affecting the algorithm scalability . in this paper we present a novel vertical data representation called diffset , that only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns . we show that diffsets drastically cut down the size of memory required to store intermediate results . we show how diffsets , when incorporated into previous vertical mining methods , increase the performance significantly .

['association rule mining', 'database applications', 'diffsets', 'frequent itemsets']

diffsets NOUN dobj using
diffsets NOUN nsubj cut
diffsets NOUN nsubj increase

learning subspace kernels for classification kernel methods have been applied successfully in many data mining tasks . subspace kernel learning was recently proposed to discover an effective low-dimensional subspace of a kernel feature space for improved classification . in this paper , we propose to construct a subspace kernel using the hilbert-schmidt independence criterion ( hsic ) . we show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem . one limitation of the existing subspace kernel learning formulations is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification . to overcome this limitation , we propose a joint optimization framework , in which we learn the subspace kernel and subsequent classifiers simultaneously . in addition , we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel . following the idea from multiple kernel learning , we extend the proposed formulations to the case when multiple kernels are available and need to be combined . we show that the integration of subspace kernels can be formulated as a semidefinite program ( sdp ) which is computationally expensive . to improve the efficiency of the sdp formulation , we propose an equivalent semi-infinite linear program ( silp ) formulation which can be solved efficiently by the column generation technique . experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms .

['classification', 'hilbert-schmidt independence criterion', 'subspace kernel', 'support vector machines']

classification NOUN pobj for
classification NOUN conj learning
classification NOUN pobj for

identifying biologically relevant genes via multiple heterogeneous data sources selection of genes that are differentially expressed and critical to a particular biological process has been a major challenge in post-array analysis . recent development in bioinformatics has made various data sources available such as mrna and mirna expression profiles , biological pathway and gene annotation , etc. . efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance . in this work , we studied a novel problem of multi-source gene selection : given multiple heterogeneous data sources ( or data sets ) , select genes from expression profiles by integrating information from various data sources . we investigated how to effectively employ information contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection . we designed and conducted experiments to systematically compare the proposed approach with representative methods in terms of statistical and biological significance , and showed the efficacy and potential of the proposed approach with promising findings .

['bioinformatics', 'gene selection', 'information integration']

bioinformatics NOUN pobj in

colibri : fast mining of large static and dynamic graphs low-rank approximations of the adjacency matrix of a graph are essential in finding patterns ( such as communities ) and detecting anomalies . additionally , it is desirable to track the low-rank structure as the graph evolves over time , efficiently and within limited storage . real graphs typically have thousands or millions of nodes , but are usually very sparse . however , standard decompositions such as svd do not preserve sparsity . this has led to the development of methods such as cur and cmd , which seek a non-orthogonal basis by sampling the columns and\/or rows of the sparse matrix . however , these approaches will typically produce overcomplete bases , which wastes both space and time . in this paper we propose the family of colibri methods to deal with these challenges . our version for static graphs , colibri-s , iteratively finds a non-redundant basis and we prove that it has no loss of accuracy compared to the best competitors ( cur and cmd ) , while achieving significant savings in space and time : on real data , colibri-s requires much less space and is orders of magnitude faster ( in proportion to the square of the number of non-redundant columns ) . additionally , we propose an efficient update algorithm for dynamic , time-evolving graphs , colibri-d . our evaluation on a large , real network traffic dataset shows that colibri-d is over 100 times faster than the best published competitor ( cmd ) .

['graph mining', 'low-rank approximation', 'scalability']


hypergraph spectral learning for multi-label classification a hypergraph is a generalization of the traditional graph in which the edges are arbitrary non-empty subsets of the vertex set . it has been applied successfully to capture high-order relations in various domains . in this paper , we propose a hypergraph spectral learning formulation for multi-label classification , where a hypergraph is constructed to exploit the correlation information among different labels . we show that the proposed formulation leads to an eigenvalue problem , which may be computationally expensive especially for large-scale problems . to reduce the computational cost , we propose an approximate formulation , which is shown to be equivalent to a least squares problem under a mild condition . based on the approximate formulation , efficient algorithms for solving least squares problems can be applied to scale the formulation to very large data sets . in addition , existing regularization techniques for least squares can be incorporated into the model for improved generalization performance . we have conducted experiments using large-scale benchmark data sets , and experimental results show that the proposed hypergraph spectral learning formulation is effective in capturing the high-order relations in multi-label problems . results also indicate that the approximate formulation is much more efficient than the original one , while keeping competitive classification performance .

['canonical correlation analysis', 'efficiency', 'hypergraph', 'least squares', 'multi-label classification', 'regularization', 'spectral learning']

hypergraph DET csubj is
hypergraph NOUN nsubj is
hypergraph NOUN nmod formulation
hypergraph NOUN nsubjpass constructed
regularization NOUN compound techniques
hypergraph NOUN nmod formulation

extracting shared subspace for multi-label classification multi-label problems arise in various domains such as multi-topic document categorization and protein function prediction . one natural way to deal with such problems is to construct a binary classifier for each label , resulting in a set of independent binary classification problems . since the multiple labels share the same input space , and the semantics conveyed by different labels are usually correlated , it is essential to exploit the correlation information contained in different labels . in this paper , we consider a general framework for extracting shared structures in multi-label classification . in this framework , a common subspace is assumed to be shared among multiple labels . we show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem , though the problem is non-convex . for high-dimensional problems , direct computation of the solution is expensive , and we develop an efficient algorithm for this case . one appealing feature of the proposed framework is that it includes several well-known algorithms as special cases , thus elucidating their intrinsic relationships . we have conducted extensive experiments on eleven multi-topic web page categorization tasks , and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms .

['least squares', 'multi-label classification', 'shared subspace']


community evolution in dynamic multi-mode networks a multi-mode network typically consists of multiple heterogeneous social actors among which various types of interactions could occur . identifying communities in a multi-mode network can help understand the structural properties of the network , address the data shortage and unbalanced problems , and assist tasks like targeted marketing and finding influential actors within or between groups . in general , a network and the membership of groups often evolve gradually . in a dynamic multi-mode network , both actor membership and interactions can evolve , which poses a challenging problem of identifying community evolution . in this work , we try to address this issue by employing the temporal information to analyze a multi-mode network . a spectral framework and its scalability issue are carefully studied . experiments on both synthetic data and real-world large scale networks demonstrate the efficacy of our algorithm and suggest its generality in solving problems with complex relationships .

['community evolution', 'dynamic heterogeneous network', 'dynamic network analysis', 'evolution', 'multi-mode networks']

evolution NOUN nsubj consists
evolution NOUN dobj identifying

to buy or not to buy : mining airfare data to minimize ticket purchase price as product prices become increasingly available on the world wide web , consumers attempt to understand how corporations vary these prices over time . however , corporations change prices based on proprietary algorithms and hidden variables ( e.g. , the number of unsold seats on a flight ) . is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions ? this paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period . when trained on this data , hamlet -- our multi-strategy data mining algorithm -- generated a predictive model that saved 341 simulated passengers $ 198,074 by advising them when to buy and when to postpone ticket purchases . remarkably , a clairvoyant algorithm with complete knowledge of future prices could save at most $ 320,572 in our simulation , thus hamlet 's savings were 61.8 % of optimal . the algorithm 's savings of $ 198,074 represents an average savings of 23.8 % for the 341 passengers for whom savings are possible . overall , hamlet saved 4.4 % of the ticket price averaged over the entire set of 4,488 simulated passengers . our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum .

['airline price prediction', 'internet', 'learning', 'price mining', 'web mining']


learning from multi-topic web documents for contextual advertisement contextual advertising on web pages has become very popular recently and it poses its own set of unique text mining challenges . often advertisers wish to either target ( or avoid ) some specific content on web pages which may appear only in a small part of the page . learning for these targeting tasks is difficult since most training pages are multi-topic and need expensive human labeling at the sub-document level for accurate training . in this paper we investigate ways to learn for sub-document classification when only page level labels are available - these labels only indicate if the relevant content exists in the given page or not . we propose the application of multiple-instance learning to this task to improve the effectiveness of traditional methods . we apply sub-document classification to two different problems in contextual advertising . one is `` sensitive content detection '' where the advertiser wants to avoid content relating to war , violence , pornography , etc. even if they occur only in a small part of a page . the second problem involves opinion mining from review sites - the advertiser wants to detect and avoid negative opinion about their product when positive , negative and neutral sentiments co-exist on a page . in both these scenarios we present experimental results to show that our proposed system is able to get good block level labeling for free and improve the performance of traditional learning methods .

['contextual advertising', 'general', 'opinion mining', 'sensitive content detection', 'sub-document classification']


growing decision trees on support-less association rules

['association rules', 'database applications', 'decision support', 'decision tree']


cluster-based concept invention for statistical relational learning we use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning . entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features . for example , in citeseer , papers can be clustered based on words or citations giving `` topics '' , and authors can be clustered based on documents they co-author giving `` communities '' . such cluster-derived concepts become part of more complex feature expressions . out of the large number of generated features , those which improve predictive accuracy are kept in the model , as decided by statistical feature selection criteria . we present results demonstrating improved accuracy on two tasks , venue prediction and link prediction , using citeseer data .

['clustering', 'feature generation', 'learning', 'relational learning']

learning NOUN pobj for
clustering NOUN xcomp use
learning NOUN pobj in

spotting out emerging artists using geo-aware analysis of p2p query strings record label companies would like to identify potential artists as early as possible in their careers , before other companies approach the artists with competing contracts . the vast number of candidates makes the process of identifying the ones with high success potential time consuming and laborious . this paper demonstrates how datamining of p2p query strings can be used in order to mechanize most of this detection process . using a unique intercepting system over the gnutella network , we were able to capture an unprecedented amount of geographically identified ( geo-aware ) queries , allowing us to investigate the diffusion of music related queries in time and space . our solution is based on the observation that emerging artists , especially rappers , have a discernible stronghold of fans in their hometown area , where they are able to perform and market their music . in a file sharing network , this is reflected as a delta function spatial distribution of content queries . using this observation , we devised a detection algorithm for emerging artists , that looks for performers with sharp increase in popularity in a small geographic region though still unnoticable nation wide . the algorithm can suggest a short list of artists with breakthrough potential , from which we showed that about 30 % translate the potential to national success .

['emerging artists', 'model development', 'p2p queries']


topical query decomposition we introduce the problem of query decomposition , where we are given a query and a document retrieval system , and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query . ideally , these queries should represent coherent , conceptually well-separated topics . we provide an abstract formulation of the query decomposition problem , and we tackle it from two different perspectives . we first show how the problem can be instantiated as a specific variant of a set cover problem , for which we provide an efficient greedy algorithm . next , we show how the same problem can be seen as a constrained clustering problem , with a very particular kind of constraint , i.e. , clustering with predefined clusters . we develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming . our experiments , conducted on a set of actual queries in a web scale search engine , confirm the effectiveness of the proposed solutions .

['clustering', 'communications applications', 'query recommendation', 'set cover']

clustering NOUN compound problem
clustering VERB advcl seen
clustering NOUN pobj on

training structural svms with kernels using sampled cuts discriminative training for structured outputs has found increasing applications in areas such as natural language processing , bioinformatics , information retrieval , and computer vision . focusing on large-margin methods , the most general ( in terms of loss function and model structure ) training algorithms known to date are based on cutting-plane approaches . while these algorithms are very efficient for linear models , their training complexity becomes quadratic in the number of examples when kernels are used . to overcome this bottleneck , we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels . we prove that these algorithms have improved time complexity while providing approximation guarantees . in empirical evaluations , our algorithms produced solutions with training and test error rates close to those of exact solvers . even on binary classification problems where highly optimized conventional training methods exist ( e.g. svm-light ) , our methods are about an order of magnitude faster than conventional training methods on large datasets , while remaining competitive in speed on datasets of medium size .

['kernels', 'large-scale problems', 'learning', 'support vector machines']

kernels NOUN pobj with
kernels NOUN nsubjpass used
kernels NOUN pobj with

classification with partial labels in this paper , we address the problem of learning when some cases are fully labeled while other cases are only partially labeled , in the form of partial labels . partial labels are represented as a set of possible labels for each training example , one of which is the correct label . we introduce a discriminative learning approach that incorporates partial label information into the conventional margin-based learning framework . the partial label learning problem is formulated as a convex quadratic optimization minimizing the l2-norm regularized empirical risk using hinge loss . we also present an efficient algorithm for classification in the presence of partial labels . experiments with different data sets show that partial label information improves the performance of classification when there is traditional fully-labeled data , and also yields reasonable performance in the absence of any fully labeled data .

['partial labels', 'support vectors']


joint optimization of wrapper generation and template detection many websites have large collections of pages generated dynamically from an underlying structured source like a database . the data of a category are typically encoded into similar pages by a common script or template . in recent years , some value-added services , such as comparison shopping and vertical search in a specific domain , have motivated the research of extraction technologies with high accuracy . almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of url . however , we observed that it is hard to distinguish different templates using dynamic urls today . moreover , since extraction accuracy heavily depends on how consistent input pages are , we argue that it is risky to determine whether pages share a common template solely based on urls . instead , we propose a new approach that utilizes similarity between pages to detect templates . our approach separates pages with notable inner differences and then generates wrappers , respectively . experimental results show that our proposed approach is feasible and effective for improving extraction accuracy .

['information extraction', 'miscellaneous', 'template detection', 'wrapper']

wrapper NOUN compound system

visually mining and monitoring massive time series moments before the launch of every space vehicle , engineering discipline specialists must make a critical go\/no-go decision . the cost of a false positive , allowing a launch in spite of a fault , or a false negative , stopping a potentially successful launch , can be measured in the tens of millions of dollars , not including the cost in morale and other more intangible detriments . the aerospace corporation is responsible for providing engineering assessments critical to the go\/no-go decision for every department of defense space vehicle . these assessments are made by constantly monitoring streaming telemetry data in the hours before launch . we will introduce viztree , a novel time-series visualization tool to aid the aerospace analysts who must make these engineering assessments . viztree was developed at the university of california , riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry . the use of a single tool for both aspects of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task . our visualization approach works by transforming the time series into a symbolic representation , and encoding the data in a modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties . we demonstrate the utility of our system by comparing it with state-of-the-art batch algorithms on several real and synthetic datasets .

['anomaly detection', 'motif discovery', 'pattern discovery', 'time series', 'visualization']

visualization NOUN compound tool
visualization NOUN compound approach

composition attacks and auxiliary information in data privacy privacy is an increasingly important aspect of data publishing . reasoning about privacy , however , is fraught with pitfalls . one of the most significant is the auxiliary information ( also called external knowledge , background knowledge , or side information ) that an adversary gleans from other channels such as the web , public records , or domain knowledge . this paper explores how one can reason about privacy in the face of rich , realistic sources of auxiliary information . specifically , we investigate the effectiveness of current anonymization schemes in preserving privacy when multiple organizations independently release anonymized data about overlapping populations . 1 . we investigate composition attacks , in which an adversary uses independent anonymized releases to breach privacy . we explain why recently proposed models of limited auxiliary information fail to capture composition attacks . our experiments demonstrate that even a simple instance of a composition attack can breach privacy in practice , for a large class of currently proposed techniques . the class includes k-anonymity and several recent variants . 2 . on a more positive note , certain randomization-based notions of privacy ( such as differential privacy ) provably resist composition attacks and , in fact , the use of arbitrary side information . this resistance enables `` stand-alone '' design of anonymization schemes , without the need for explicitly keeping track of other releases . we provide a precise formulation of this property , and prove that an important class of relaxations of differential privacy also satisfy the property . this significantly enlarges the class of protocols known to enable modular design .

['adversarial attacks', 'anonymization', 'general', 'privacy']

privacy NOUN compound privacy
privacy NOUN pobj in
privacy NOUN pobj about
privacy NOUN pobj about
anonymization NOUN compound schemes
privacy NOUN dobj preserving
privacy NOUN dobj breach
privacy NOUN dobj breach
privacy NOUN pobj of
privacy NOUN pobj as
anonymization NOUN compound schemes
privacy NOUN pobj of

feature selection methods for text classification we consider feature selection for text classification both theoretically and empirically . our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features . to the best of our knowledge , this is the first feature selection method with such guarantees . in addition , the analysis leads to insights as to when and why this feature selection strategy will perform well in practice . we then use the techtc-100 , 20-newsgroups , and reuters-rcv2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies . our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used feature selection strategies . in addition , it performs better on certain datasets under very aggressive feature selection .

['feature selection', 'miscellaneous', 'miscellaneous', 'random sampling', 'regularized least squares classification', 'text classification']


integrating feature and instance selection for text classification instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data . feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances . so far , these two methods have mostly been considered in isolation . in this paper , we present a new algorithm , which we call fis ( feature and instance selection ) that targets both problems simultaneously in the context of text classificationour experiments on the reuters and 20-newsgroups datasets show that fis considerably reduces both the number of features and the number of instances . the accuracy of a range of classifiers including naã¯ve bayes , tan and lb considerably improves when using the fis preprocessed datasets , matching and exceeding that of support vector machines , which is currently considered to be one of the best text classification methods . in all cases the results are much better compared to mutual information based feature selection . the training and classification speed of all classifiers is also greatly improved .

['applications', 'design methodology']


using graph-based metrics with empirical risk minimization to speed up active learning on networked data active and semi-supervised learning are important techniques when labeled data are scarce . recently a method was suggested for combining active learning with a semi-supervised learning algorithm that uses gaussian fields and harmonic functions . this classifier is relational in nature : it relies on having the data presented as a partially labeled graph ( also known as a within-network learning problem ) . this work showed yet again that empirical risk minimization ( erm ) was the best method to find the next instance to label and provided an efficient way to compute erm with the semi-supervised classifier . the computational problem with erm is that it relies on computing the risk for all possible instances . if we could limit the candidates that should be investigated , then we can speed up active learning considerably . in the case where the data is graphical in nature , we can leverage the graph structure to rapidly identify instances that are likely to be good candidates for labeling . this paper describes a novel hybrid approach of using of community finding and social network analytic centrality measures to identify good candidates for labeling and then using erm to find the best instance in this candidate set . we show on real-world data that we can limit the erm computations to a fraction of instances with comparable performance .

['active learning', 'betweenness centrality', 'closeness centrality', 'clustering', 'community finding', 'empirical risk minimization', 'semi-supervised learning', 'social network analysis', 'statistical relational learning', 'within-network learning']


knowledge transfer via multiple model local structure mapping the effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribution that generates the training examples and the one from which test examples are to be drawn . the task can be especially difficult when the training examples are from one or several domains different from the test domain . in this paper , we propose a locally weighted ensemble framework to combine multiple models for transfer learning , where the weights are dynamically assigned according to a model 's predictive power on each test example . it can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model , which can then be applied on a different domain . importantly , different from many previously proposed methods , none of the base learning method is required to be specifically designed for transfer learning . we show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer . we then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain , and then weighting each model locally according to its consistency with the neighborhood structure around the test example . experimental results on text classification , spam filtering and intrusion detection data sets demonstrate significant improvements in classification accuracy gained by the framework . on a transfer learning task of newsgroup message categorization , the proposed locally weighted ensemble framework achieves 97 % accuracy when the best single model predicts correctly only on 73 % of the test examples . in summary , the improvement in accuracy is over 10 % and up to 30 % across different problems .

['classification', 'ensemble', 'semi-supervised learning', 'transfer learning']

classification NOUN compound algorithms
ensemble ADJ amod framework
classification NOUN compound model
ensemble ADJ amod framework
classification NOUN pobj on
classification NOUN compound accuracy
ensemble ADJ amod framework

mining preferences from superior and inferior examples mining user preferences plays a critical role in many important applications such as customer relationship management ( crm ) , product and service recommendation , and marketing campaigns . in this paper , we identify an interesting and practical problem of mining user preferences : in a multidimensional space where the user preferences on some categorical attributes are unknown , from some superior and inferior examples provided by a user , can we learn about the user 's preferences on those categorical attributes ? we model the problem systematically and show that mining user preferences from superior and inferior examples is challenging . although the problem has great potential in practice , to the best of our knowledge , it has not been explored systematically before . as the first attempt to tackle the problem , we propose a greedy method and show that our method is practical using real data sets and synthetic data sets .

['inferior examples', 'preferences', 'skyline', 'superior examples']

preferences NOUN nsubj plays
preferences NOUN pobj of
preferences VERB nsubj are
preferences NOUN pobj about
preferences NOUN nsubj challenging

a generalized maximum entropy approach to bregman co-clustering and matrix approximation co-clustering is a powerful data mining technique with varied applications such as text clustering , microarray analysis and recommender systems . recently , an information-theoretic co-clustering approach applicable to empirical joint probability distributions was proposed . in many situations , co-clustering of more general matrices is desired . in this paper , we present a substantially generalized co-clustering framework wherein any bregman divergence can be used in the objective function , and various conditional expectation based constraints can be considered based on the statistics that need to be preserved . analysis of the co-clustering problem leads to the minimum bregman information principle , which generalizes the maximum entropy principle , and yields an elegant meta algorithm that is guaranteed to achieve local optimality . our methodology yields new algorithms and also encompasses several previously known clustering and co-clustering algorithms based on alternate minimization .

['bregman divergences', 'co-clustering', 'learning', 'matrix approximation']


information-theoretic co-clustering two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text , web-log and market-basket data analysis . a basic problem in contingency table analysis is co-clustering : simultaneous clustering of the rows and columns . a novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in information theory -- the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters . we present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages . using the practical example of simultaneous word-document clustering , we demonstrate that our algorithm works well in practice , especially in the presence of sparsity and high-dimensionality .

['clustering', 'co-clustering', 'information search and retrieval', 'information theory', 'mutual information']

clustering VERB ROOT clustering
clustering NOUN ROOT clustering
clustering NOUN dobj clustering
clustering ADJ amod problem
clustering ADJ nsubj maximizes
clustering ADJ amod algorithm
clustering NOUN pobj of

fast approximate spectral clustering spectral clustering refers to a flexible class of clustering procedures that can produce high-quality clusterings on small data sets but which has limited applicability to large-scale problems due to its computational complexity of o ( n3 ) in general , with n the number of data points . we extend the range of spectral clustering by developing a general framework for fast approximate spectral clustering in which a distortion-minimizing local transformation is first applied to the data . this framework is based on a theoretical analysis that provides a statistical characterization of the effect of local distortion on the mis-clustering rate . we develop two concrete instances of our general framework , one based on local k-means clustering ( kasp ) and one based on random projection trees ( rasp ) . extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy . specifically , our algorithms outperform k-means by a large margin in terms of accuracy , and run several times faster than approximate spectral clustering based on the nystrom method , with comparable accuracy and significantly smaller memory footprint . remarkably , our algorithms make it possible for a single machine to spectral cluster data sets with a million observations within several minutes .

['data quantization', 'learning', 'spectral clustering', 'unsupervised learning']


microscopic evolution of social networks we present a detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals . for the first time at such a large scale , we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks . using a methodology based on the maximum-likelihood principle , we investigate a wide variety of network formation strategies , and show that edge locality plays a critical role in evolution of networks . our findings supplement earlier network models based on the inherently non-local preferential attachment . based on our observations , we develop a complete model of network evolution , where nodes arrive at a prespecified rate and select their lifetimes . each node then independently initiates edges according to a `` gap '' process , selecting a destination for each edge according to a simple triangle-closing model free of any parameters . we show analytically that the combination of the gap distribution with the node lifetime leads to a power law out-degree distribution that accurately reflects the true network in all four cases . finally , we give model parameter settings that allow automatic evolution and generation of realistic synthetic networks of arbitrary scale .

['graph generators', 'maximum likelihood', 'network evolution', 'social networks', 'transitivity', 'triadic closure']


mining time-changing data streams most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution . unfortunately , most of the large databases available for mining today violate this assumption . they were gathered over months or years , and the underlying processes generating them changed during this time , sometimes radically . although a number of algorithms have been proposed for learning time-changing concepts , they generally do not scale well to very large databases . in this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams , based on the ultra-fast vfdt decision tree learner . this algorithm , called cvfdt , stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable , and replacing the old with the new when the new becomes more accurate . cvfdt learns a model which is similar in accuracy to the one that would be learned by reapplying vfdt to a moving window of examples every time a new example arrives , but with o ( 1 ) complexity per example , as opposed to o ( w ) , where w is the size of the window . experiments on a set of large time-changing data streams demonstrate the utility of this approach .

['concept drift', 'data streams', 'decision trees', 'hoeffding bounds', 'incremental learning', 'subsampling']


a streaming ensemble algorithm ( sea ) for large-scale classification ensemble methods have recently garnered a great deal of attention in the machine learning community . techniques such as boosting and bagging have proven to be highly effective but require repeated resampling of the training data , making them inappropriate in a data mining context . the methods presented in this paper take advantage of plentiful data , building separate classifiers on sequential chunks of training points . these classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy . the result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data , requires approximately constant memory , and adjusts quickly to concept drift .

['database applications', 'ensemble classification', 'streaming data']


domain-constrained semi-supervised mining of tracking models in sensor networks accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application . specifically , the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points . conventional data mining and machine learning methods can be applied to solve this problem . however , all of them require large amounts of labeled training data , which can be quite expensive . in this paper , we propose a probabilistic semi supervised learning approach to reduce the calibration effort and increase the tracking accuracy . our method is based on semi-supervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively . to make our method more efficient , we exploit a generalized em algorithm coupled with domain constraints . we validate our method through extensive experiments in a real sensor network using crossbow mica2 sensors . the results demonstrate the advantages of methods compared to other state-of-the-art object-tracking algorithms .

['calibration', 'crf', 'em', 'learning', 'localization', 'sensor networks', 'tracking']

tracking NOUN compound models
localization NOUN nsubj is
localization NOUN compound problem
learning NOUN conj mining
learning NOUN compound approach
calibration NOUN compound effort
tracking NOUN compound accuracy
em PRON dobj exploit
tracking VERB compound algorithms

quantification and semi-supervised classification methods for handling changes in class distribution in realistic settings the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier . further complicating this scenario is the fact that labeled data is often scarce and expensive . in this paper we address the problem where the class distribution changes and only unlabeled examples are available from the new distribution . we design and evaluate a number of methods for coping with this problem and compare the performance of these methods . our quantification-based methods estimate the class distribution of the unlabeled data from the changed distribution and adjust the original classifier accordingly , while our semi-supervised methods build a new classifier using the examples from the new ( unlabeled ) distribution which are supplemented with predicted class values . we also introduce a hybrid method that utilizes both quantification and semi-supervised learning . all methods are evaluated using accuracy and f-measure on a set of benchmark data sets . our results demonstrate that our methods yield substantial improvements in accuracy and f-measure .

['class distribution', 'classification', 'concept drift', 'quantification', 'semi-supervised learning']

quantification NOUN ROOT quantification
classification NOUN compound methods
quantification NOUN npadvmod based
quantification NOUN dobj utilizes

empirical bayes screening for multi-item associations this paper considers the framework of the so-called `` market basket problem '' , in which a database of transactions is mined for the occurrence of unusually frequent item sets . in our case , `` unusually frequent '' involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently . the focus is on obtaining reliable estimates of this measure of interestingness for all item sets , even item sets with relatively low frequencies . for example , in a medical database of patient histories , unusual item sets including the item `` patient death '' ( or other serious adverse event ) might hopefully be flagged with as few as 5 or 10 occurrences of the item set , it being unacceptable to require that item sets occur in as many as 0.1 % of millions of patient reports before the data mining algorithm detects a signal . similar considerations apply in fraud detection applications . thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support , and adopt a criterion based on the results of fitting an empirical bayes model to the item set counts . the model allows us to define a 95 % bayesian lower confidence limit for the `` interestingness '' measure of every item set , whereupon the item sets can be ranked according to their empirical bayes confidence limits . for item sets of size j ) 2 , we also distinguish between multi-item associations that can be explained by the observed j ( j-1 ) \/ 2 pairwise associations , and item sets that are significantly more frequent than their pairwise associations would suggest . such item sets can uncover complex or synergistic mechanisms generating multi-item associations . this methodology has been applied within the u.s. food and drug administration ( fda ) to databases of adverse drug reaction reports and within at&t to customer international calling histories . we also present graphical techniques for exploring and understanding the modeling results .

['association rules', 'empirical bayes methods', 'gamma-poisson model', 'knowledge discovery', 'market basket problem', 'shrinkage estimation', 'statistical models']


mining statistically important equivalence classes and delta-discriminative emerging patterns the support-confidence framework is the most common measure used in itemset mining algorithms , for its antimonotonicity that effectively simplifies the search lattice . this computational convenience brings both quality and statistical flaws to the results as observed by many previous studies . in this paper , we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chi-square , risk ratio , odds ratio , etc. . our algorithm is based on the concept of equivalence classes . an equivalence class is a set of frequent itemsets that always occur together in the same set of transactions . therefore , itemsets within an equivalence class all share the same level of statistical significance regardless of the variety of test statistics . as an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generators , we just mine closed patterns and generators , taking a simultaneous depth-first search scheme . this parallel approach has not been exploited by any prior work . we evaluate our algorithm on two aspects . in general , we compare to lcm and fpclose which are the best algorithms tailored for mining only closed patterns . in particular , we compare to epminer which is the most recent algorithm for mining a type of relative risk patterns , known as minimal emerging patterns . experimental results show that our algorithm is faster than all of them , sometimes even multiple orders of magnitude faster . these statistically ranked patterns and the efficiency have a high potential for real-life applications , especially in biomedical and financial fields where classical test statistics are of dominant interest .

['equivalence classes', 'itemsets with ranked statistical merit']


fast mining of high dimensional expressive contrast patterns using zero-suppressed binary decision diagrams patterns of contrast are a very important way of comparing multi-dimensional datasets . such patterns are able to capture regions of high difference between two classes of data , and are useful for human experts and the construction of classifiers . however , mining such patterns is particularly challenging when the number of dimensions is large . this paper describes a new technique for mining several varieties of contrast pattern , based on the use of zero-suppressed binary decision diagrams ( zbdds ) , a powerful data structure for manipulating sparse data . we study the mining of both simple contrast patterns , such as emerging patterns , and more novel and complex contrasts , which we call disjunctive emerging patterns . a performance study demonstrates our zbdd technique is highly scalable , substantially improves on state of the art mining for emerging patterns and can be effective for discovering complex contrasts from datasets with thousands of attributes .

['contrast patterns', 'disjunctive emerging patterns', 'zero-suppressed binary decision diagrams']


on detecting differences between groups understanding the differences between contrasting groups is a fundamental task in data analysis . this realization has led to the development of a new special purpose data mining technique , contrast-set mining . we undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques . to our surprise we observed that straightforward application of an existing commercial rule-discovery system , magnum opus , could successfully perform the contrast-set-mining task . this led to the realization that contrast-set mining is a special case of the more general rule-discovery task . we present the results of our study together with a proof of this conclusion .

['contrast-set discovery', 'information search and retrieval', 'learning', 'retailing', 'rule discovery']


collaborative crawling : mining user experiences for topical resource discovery the rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent years . in this problem , it is desired to find web pages which satisfy a predicate specified by the user . such a predicate could be a keyword query , a topical query , or some arbitrary contraint . several techniques such as focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery . all these crawlers are linkage based , since they use the hyperlink behavior in order to perform resource discovery . recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process . in this paper , we will approach the problem of resource discovery from an entirely different perspective ; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate . this user behavior can be mined from the freely available traces of large public domain proxies on the world wide web . we refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources . such a strategy is extremely effective because the topical consistency in world wide web browsing patterns turns out to very reliable . in addition , the user-centered crawling system can be combined with linkage based systems to create an overall system which works more effectively than a system based purely on either user behavior or hyperlinks .

['world wide web']


predicting bounce rates in sponsored search advertisements this paper explores an important and relatively unstudied quality measure of a sponsored search advertisement : bounce rate . the bounce rate of an ad can be informally defined as the fraction of users who click on the ad but almost immediately move on to other tasks . a high bounce rate can lead to poor advertiser return on investment , and suggests search engine users may be having a poor experience following the click . in this paper , we first provide quantitative analysis showing that bounce rate is an effective measure of user satisfaction . we then address the question , can we predict bounce rate by analyzing the features of the advertisement ? an affirmative answer would allow advertisers and search engines to predict the effectiveness and quality of advertisements before they are shown . we propose solutions to this problem involving large-scale learning methods that leverage features drawn from ad creatives in addition to their keywords and landing pages .

['applications', 'bounce rate', 'machine learning', 'online advertisement']


probabilistic latent semantic visualization : topic model for visualizing documents we propose a visualization method based on a topic model for discrete data such as documents . unlike conventional visualization methods based on pairwise distances such as multi-dimensional scaling , we consider a mapping from the visualization space into the space of documents as a generative process of documents . in the model , both documents and topics are assumed to have latent coordinates in a two - or three-dimensional euclidean space , or visualization space . the topic proportions of a document are determined by the distances between the document and the topics in the visualization space , and each word is drawn from one of the topics according to its topic proportions . a visualization , i.e. latent coordinates of documents , can be obtained by fitting the model to a given set of documents using the em algorithm , resulting in documents with similar topics being embedded close together . we demonstrate the effectiveness of the proposed model by visualizing document and movie data sets , and quantitatively compare it with conventional visualization methods .

['probabilistic latent semantic analysis', 'topic model', 'visualization']

visualization NOUN ROOT visualization
visualization NOUN compound method
visualization NOUN compound methods
visualization NOUN compound space
visualization NOUN compound space
visualization NOUN compound space
visualization NOUN nsubjpass obtained
visualization NOUN compound methods

structured learning for non-smooth ranking losses learning to rank from relevance judgment is an active research area . itemwise score regression , pairwise preference satisfaction , and listwise structured learning are the major techniques in use . listwise structured learning has been applied recently to optimize important non-decomposable ranking criteria like auc ( area under roc curve ) and map ( mean average precision ) . we propose new , almost-linear-time algorithms to optimize for two other criteria widely used to evaluate search systems : mrr ( mean reciprocal rank ) and ndcg ( normalized discounted cumulative gain ) in the max-margin structured learning framework . we also demonstrate that , for different ranking criteria , one may need to use different feature maps . search applications should not be optimized in favor of a single criterion , because they need to cater to a variety of queries . e.g. , mrr is best for navigational queries , while ndcg is best for informational queries . a key contribution of this paper is to fold multiple ranking loss functions into a multi-criteria max-margin optimization . the result is a single , robust ranking model that is close to the best accuracy of learners trained on individual criteria . in fact , experiments over the popular letor and trec data sets show that , contrary to conventional wisdom , a test criterion is often not best served by training with the same individual criterion .

['max-margin structured learning to rank', 'non-decomposable loss functions']


fast collapsed gibbs sampling for latent dirichlet allocation in this paper we introduce a novel collapsed gibbs sampling method for the widely used latent dirichlet allocation ( lda ) model . our new method results in significant speedups on real world text corpora . conventional gibbs sampling schemes for lda require o ( k ) operations per sample where k is the number of topics in the model . our proposed method draws equivalent samples but requires on average significantly less then k operations per sample . on real-word corpora fastlda can be as much as 8 times faster than the standard collapsed gibbs sampler for lda . no approximations are necessary , and we show that our fast sampling scheme produces exactly the same results as the standard ( but slower ) sampling scheme . experiments on four real world data sets demonstrate speedups for a wide range of collection sizes . for the pubmed collection of over 8 million documents with a required computation time of 6 cpu months for lda , our speedup of 5.7 can save 5 cpu months of computation .

['latent dirichlet allocation', 'probabilistic algorithms', 'sampling']

sampling NOUN acl gibbs
sampling VERB acl gibbs
sampling NOUN amod scheme
sampling VERB advcl produces

beyond blacklists : learning to detect malicious web sites from suspicious urls malicious web sites are a cornerstone of internet criminal activities . as a result , there has been broad interest in developing systems to prevent the end user from visiting such sites . in this paper , we describe an approach to this problem based on automated url classification , using statistical methods to discover the tell-tale lexical and host-based properties of malicious web site urls . these methods are able to learn highly predictive models by extracting and automatically analyzing tens of thousands of features potentially indicative of suspicious urls . the resulting classifiers obtain 95-99 % accuracy , detecting large numbers of malicious web sites from their urls , with only modest false positives .

['abuse and crime involving computers', 'l1-regularization', 'malicious web sites', 'security and protection', 'supervised learning']


structured correspondence topic models for mining captioned figures in biological literature a major source of information ( often the most crucial and informative part ) in scholarly articles from scientific journals , proceedings and books are the figures that directly provide images and other graphical illustrations of key experimental results and other scientific contents . in biological articles , a typical figure often comprises multiple panels , accompanied by either scoped or global captioned text . moreover , the text in the caption contains important semantic entities such as protein names , gene ontology , tissues labels , etc. , relevant to the images in the figure . due to the avalanche of biological literature in recent years , and increasing popularity of various bio-imaging techniques , automatic retrieval and summarization of biological information from literature figures has emerged as a major unsolved challenge in computational knowledge extraction and management in the life science . we present a new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figures , and we derive an efficient inference algorithm based on collapsed gibbs sampling for information retrieval and visualization . the resulting program constitutes one of the key ir engines in our slif system that has recently entered the final round ( 4 out 70 competing systems ) of the elsevier grand challenge on knowledge enhancement in the life science . here we present various evaluations on a number of data mining tasks to illustrate our method .

['bioinformatics', 'gibbs sampling', 'information retrieval', 'learning', 'multimodal data', 'topic models']


efficiently learning the accuracy of labeling sources for selective sampling many scalable data mining tasks rely on active learning to provide the most useful accurately labeled instances . however , what if there are multiple labeling sources ( ` oracles ' or ` experts ' ) with different but unknown reliabilities ? with the recent advent of inexpensive and scalable online annotation tools , such as amazon 's mechanical turk , the labeling process has become more vulnerable to noise - and without prior knowledge of the accuracy of each individual labeler . this paper addresses exactly such a challenge : how to jointly learn the accuracy of labeling sources and obtain the most informative labels for the active learning task at hand minimizing total labeling effort . more specifically , we present iethresh ( interval estimate threshold ) as a strategy to intelligently select the expert ( s ) with the highest estimated labeling accuracy . iethresh estimates a confidence interval for the reliability of each expert and filters out the one ( s ) whose estimated upper-bound confidence interval is below a threshold - which jointly optimizes expected accuracy ( mean ) and need to better estimate the expert 's accuracy ( variance ) . our framework is flexible enough to work with a wide range of different noise levels and outperforms baselines such as asking all available experts and random expert selection . in particular , iethresh achieves a given level of accuracy with less than half the queries issued by all-experts labeling and less than a third the queries required by random expert selection on datasets such as the uci mushroom one . the results show that our method naturally balances exploration and exploitation as it gains knowledge of which experts to rely upon , and selects them with increasing frequency .

['active learning', 'estimation', 'labeler selection', 'noisy labelers']


mining broad latent query aspects from search sessions search queries are typically very short , which means they are often underspecified or have senses that the user did not think of . a broad latent query aspect is a set of keywords that succinctly represents one particular sense , or one particular information need , that can aid users in reformulating such queries . we extract such broad latent aspects from query reformulations found in historical search session logs . we propose a framework under which the problem of extracting such broad latent aspects reduces to that of optimizing a formal objective function under constraints on the total number of aspects the system can store , and the number of aspects that can be shown in response to any given query . we present algorithms to find a good set of aspects , and also to pick the best k aspects matching any query . empirical results on real-world search engine logs show significant gains over a strong baseline that uses single-keyword reformulations : a gain of 14 % and 23 % in terms of human-judged accuracy and click-through data respectively , and around 20 % in terms of consistency among aspects predicted for `` similar '' queries . this demonstrates both the importance of broad query aspects , and the efficacy of our algorithms for extracting them .

['latent user intent', 'query aspects', 'search sessions']


bbm : bayesian browsing model from petabyte-scale data given a quarter of petabyte click log data , how can we estimate the relevance of each url for a given query ? in this paper , we propose the bayesian browsing model ( bbm ) , a new modeling technique with following advantages : ( a ) it does exact inference ; ( b ) it is single-pass and parallelizable ; ( c ) it is effective . we present two sets of experiments to test model effectiveness and efficiency . on the first set of over 50 million search instances of 1.1 million distinct queries , bbm out-performs the state-of-the-art competitor by 29.2 % in log-likelihood while being 57 times faster . on the second click-log set , spanning a quarter of petabyte data , we showcase the scalability of bbm : we implemented it on a commercial mapreduce cluster , and it took only 3 hours to compute the relevance for 1.15 billion distinct query-url pairs .

['bayesian models', 'click log analysis', 'web search']


constrained optimization for validation-guided conditional random field learning conditional random fields ( crfs ) are a class of undirected graphical models which have been widely used for classifying and labeling sequence data . the training of crfs is typically formulated as an unconstrained optimization problem that maximizes the conditional likelihood . however , maximum likelihood training is prone to overfitting . to address this issue , we propose a novel constrained nonlinear optimization formulation in which the prediction accuracy of cross-validation sets are included as constraints . instead of requiring multiple passes of training , the constrained formulation allows the cross-validation be handled in one pass of constrained optimization . the new formulation is discontinuous , and classical lagrangian based constraint handling methods are not applicable . a new constrained optimization algorithm based on the recently proposed extended saddle point theory is developed to learn the constrained crf model . experimental results on gene and stock-price prediction tasks show that the constrained formulation is able to significantly improve the generalization ability of crf training .

['conditional random fields', 'constrained optimization', 'cross validation', 'extended saddle points', 'financial']


primal sparse max-margin markov networks max-margin markov networks ( m3n ) have shown great promise in structured prediction and relational learning . due to the kkt conditions , the m3n enjoys dual sparsity . however , the existing m3n formulation does not enjoy primal sparsity , which is a desirable property for selecting significant features and reducing the risk of over-fitting . in this paper , we present an l1-norm regularized max-margin markov network ( l1-m3n ) , which enjoys dual and primal sparsity simultaneously . to learn an l1-m3n , we present three methods including projected sub-gradient , cutting-plane , and a novel em-style algorithm , which is based on an equivalence between l1-m3n and an adaptive m3n . we perform extensive empirical studies on both synthetic and real data sets . our experimental results show that : ( 1 ) l1-m3n can effectively select significant features ; ( 2 ) l1-m3n can perform as well as the pseudo-primal sparse laplace m3n in prediction accuracy , while consistently outperforms other competing methods that enjoy either primal or dual sparsity ; and ( 3 ) the em-algorithm is more robust than the other two in pre-diction accuracy and time efficiency .

['l_1-norm max-margin markov networks', 'dual sparsity', 'primal sparsity']


meme-tracking and the dynamics of the news cycle tracking new topics , ideas , and `` memes '' across the web has been an issue of considerable interest . recent work has developed methods for tracking topic shifts over long time scales , as well as abrupt spikes in the appearance of particular named entities . however , these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events . we develop a framework for tracking short , distinctive phrases that travel relatively intact through on-line text ; developing scalable algorithms for clustering textual variants of such phrases , we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis . as our principal domain of study , we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis . we tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle . in particular , we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively , with divergent behavior around the overall peak and a `` heartbeat '' - like pattern in the handoff between news and blogs . we also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits .

['blogs', 'information cascades', 'information networks', 'meme-tracking', 'news cycle', 'news media', 'social networks']

blogs NOUN conj tracked
blogs NOUN pobj in
blogs NOUN conj news

snare : a link analytic system for graph labeling and risk detection classifying nodes in networks is a task with a wide range of applications . it can be particularly useful in anomaly and fraud detection . many resources are invested in the task of fraud detection due to the high cost of fraud , and being able to automatically detect potential fraud quickly and precisely allows human investigators to work more efficiently . many data analytic schemes have been put into use ; however , schemes that bolster link analysis prove promising . this work builds upon the belief propagation algorithm for use in detecting collusion and other fraud schemes . we propose an algorithm called snare ( social network analysis for risk evaluation ) . by allowing one to use domain knowledge as well as link knowledge , the method was very successful for pinpointing misstated accounts in our sample of general ledger data , with a significant improvement over the default heuristic in true positive rates , and a lift factor of up to 6.5 ( more than twice that of the default heuristic ) . we also apply snare to the task of graph labeling in general on publicly-available datasets . we show that with only some information about the nodes themselves in a network , we get surprisingly high accuracy of labels . not only is snare applicable in a wide variety of domains , but it is also robust to the choice of parameters and highly scalable-linearly with the number of edges in a graph .

['anomaly detection', 'belief propagation', 'social networks']


drosophila gene expression pattern annotation using sparse features and term-term interactions the drosophila gene expression pattern images document the spatial and temporal dynamics of gene expression and they are valuable tools for explicating the gene functions , interaction , and networks during drosophila embryogenesis . to provide text-based pattern searching , the images in the berkeley drosophila genome project ( bdgp ) study are annotated with ontology terms manually by human curators . we present a systematic approach for automating this task , because the number of images needing text descriptions is now rapidly increasing . we consider both improved feature representation and novel learning formulation to boost the annotation performance . for feature representation , we adapt the bag-of-words scheme commonly used in visual recognition problems so that the image group information in the bdgp study is retained . moreover , images from multiple views can be integrated naturally in this representation . to reduce the quantization error caused by the bag-of-words representation , we propose an improved feature representation scheme based on the sparse learning technique . in the design of learning formulation , we propose a local regularization framework that can incorporate the correlations among terms explicitly . we further show that the resulting optimization problem admits an analytical solution . experimental results show that the representation based on sparse learning outperforms the bag-of-words representation significantly . results also show that incorporation of the term-term correlations improves the annotation performance consistently .

['bag-of-words', 'gene expression pattern', 'image annotation', 'regularization', 'sparse learning']

regularization NOUN compound framework

isax : indexing and mining terabyte sized time series current research in indexing and mining time series data has produced many interesting algorithms and representations . however , the algorithms and the size of data considered have generally not been representative of the increasingly massive datasets encountered in science , engineering , and business domains . in this work , we show how a novel multi-resolution symbolic representation can be used to index datasets which are several orders of magnitude larger than anything else considered in the literature . our approach allows both fast exact search and ultra fast approximate search . we show how to exploit the combination of both types of search as sub-routines in data mining algorithms , allowing for the exact mining of truly massive real world datasets , containing millions of time series .

['indexing', 'representations', 'time series']

indexing NOUN nmod data
representations NOUN conj algorithms

dynammo : mining and summarization of coevolving sequences with missing values given multiple time sequences with missing values , we propose dynammo which summarizes , compresses , and finds latent variables . the idea is to discover hidden variables and learn their dynamics , making our algorithm able to function even when there are missing values . we performed experiments on both real and synthetic datasets spanning several megabytes , including motion capture sequences and chlorine levels in drinking water . we show that our proposed dynammo method ( a ) can successfully learn the latent variables and their evolution ; ( b ) can provide high compression for little loss of reconstruction accuracy ; ( c ) can extract compact but powerful features for segmentation , interpretation , and forecasting ; ( d ) has complexity linear on the duration of sequences .

['bayesian network', 'expectation maximization', 'missing value', 'time series']


query result clustering for object-level search query result clustering has recently attracted a lot of attention to provide users with a succinct overview of relevant results . however , little work has been done on organizing the query results for object-level search . object-level search result clustering is challenging because we need to support diverse similarity notions over object-specific features ( such as the price and weight of a product ) of heterogeneous domains . to address this challenge , we propose a hybrid subspace clustering algorithm called hydra . algorithm hydra captures the user perception of diverse similarity notions from millions of web pages and disambiguates different senses using feature-based subspace locality measures . our proposed solution , by combining wisdom of crowds and wisdom of data , achieves robustness and efficiency over existing approaches . we extensively evaluate our proposed framework and demonstrate how to enrich user experiences in object-level search using a real-world product search scenarios .

['object-level search', 'performance evaluation', 'subspace clustering']


mining discrete patterns via binary matrix factorization mining discrete patterns in binary data is important for subsampling , compression , and clustering . we consider rank-one binary matrix approximations that identify the dominant patterns of the data , while preserving its discrete property . a best approximation on such data has a minimum set of inconsistent entries , i.e. , mismatches between the given binary data and the approximate matrix . due to the hardness of the problem , previous accounts of such problems employ heuristics and the resulting approximation may be far away from the optimal one . in this paper , we show that the rank-one binary matrix approximation can be reformulated as a 0-1 integer linear program ( ilp ) . however , the ilp formulation is computationally expensive even for small-size matrices . we propose a linear program ( lp ) relaxation , which is shown to achieve a guaranteed approximation error bound . we further extend the proposed formulations using the regularization technique , which is commonly employed to address overfitting . the lp formulation is restricted to medium-size matrices , due to the large number of variables involved for large matrices . interestingly , we show that the proposed approximate formulation can be transformed into an instance of the minimum s-t cut problem , which can be solved efficiently by finding maximum flows . our empirical study shows the efficiency of the proposed algorithm based on the maximum flow . results also confirm the established theoretical bounds .

['binary matrix factorization', 'integer linear program', 'maximum flow', 'minimum cut', 'rank-one', 'regularization']

regularization NOUN compound technique

relational learning via latent social dimensions social media such as blogs , facebook , flickr , etc. , presents data in a network format rather than classical iid distribution . to address the interdependency among data instances , relational learning has been proposed , and collective inference based on network connectivity is adopted for prediction . however , connections in social media are often multi-dimensional . an actor can connect to another actor for different reasons , e.g. , alumni , colleagues , living in the same city , sharing similar interests , etc. . collective inference normally does not differentiate these connections . in this work , we propose to extract latent social dimensions based on network information , and then utilize them as features for discriminative learning . these social dimensions describe diverse affiliations of actors hidden in the network , and the discriminative learning can automatically determine which affiliations are better aligned with the class labels . such a scheme is preferred when multiple diverse relations are associated with the same network . we conduct extensive experiments on social media data ( one from a real-world blog site and the other from a popular content sharing site ) . our model outperforms representative relational learning methods based on collective inference , especially when few labeled data are available . the sensitivity of this model and its connection to existing methods are also examined .

['behavior prediction', 'modularity', 'relational learning', 'social dimensions', 'social media']


large-scale sparse logistic regression logistic regression is a well-known classification method that has been used widely in many applications of data mining , machine learning , computer vision , and bioinformatics . sparse logistic regression embeds feature selection in the classification framework using the l1-norm regularization , and is attractive in many applications involving high-dimensional data . in this paper , we propose lassplore for solving large-scale sparse logistic regression . specifically , we formulate the problem as the l1-ball constrained smooth convex optimization , and propose to solve the problem using the nesterov 's method , an optimal first-order black-box method for smooth convex optimization . one of the critical issues in the use of the nesterov 's method is the estimation of the step size at each of the optimization iterations . previous approaches either applies the constant step size which assumes that the lipschitz gradient is known in advance , or requires a sequence of decreasing step size which leads to slow convergence in practice . in this paper , we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate . empirical comparisons with several state-of-the-art algorithms demonstrate the efficiency of the proposed lassplore algorithm for large-scale problems .

['l1-ball constraint', 'adaptive line search', 'logistic regression', "nesterov's method", 'sparse learning']


information theoretic regularization for semi-supervised boosting we present novel semi-supervised boosting algorithms that incrementally build linear combinations of weak classifiers through generic functional gradient descent using both labeled and unlabeled training data . our approach is based on extending information regularization framework to boosting , bearing loss functions that combine log loss on labeled data with the information-theoretic measures to encode unlabeled data . even though the information-theoretic regularization terms make the optimization non-convex , we propose simple sequential gradient descent optimization algorithms , and obtain impressively improved results on synthetic , benchmark and real world tasks over supervised boosting algorithms which use the labeled data alone and a state-of-the-art semi-supervised boosting algorithm .

['ensemble', 'learning', 'semi-supervised learning']


cut-and-stitch : efficient parallel learning of linear dynamical systems on smps multi-core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing . our goal is to make use of the multi-core as well as multi-processor architectures to speed up data mining algorithms . specifically , we present a parallel algorithm for approximate learning of linear dynamical systems ( lds ) , also known as kalman filters ( kf ) . ldss are widely used in time series analysis such as motion capture modeling , visual tracking etc. . we propose cut-and-stitch ( cas ) , a novel method to handle the data dependencies from the chain structure of hidden variables in lds , so as to parallelize the em-based parameter learning algorithm . we implement the algorithm using openmp on both a supercomputer and a quad-core commercial desktop . the experimental results show that parallel algorithms using cut-and-stitch achieve comparable accuracy and almost linear speedups over the serial version . in addition , cut-and-stitch can be generalized to other models with similar linear structures such as hidden markov models ( hmm ) and switching kalman filters ( skf ) .

['expectation maximization', 'kalman filters', 'linear dynamical systems', 'multi-core', 'openmp', 'optimization']

openmp NOUN dobj using

direct mining of discriminative and essential frequent patterns via model-based search tree frequent patterns provide solutions to datasets that do not have well-structured feature vectors . however , frequent pattern mining is non-trivial since the number of unique patterns is exponential but many are non-discriminative and correlated . currently , frequent pattern mining is performed in two sequential steps : enumerating a set of frequent patterns , followed by feature selection . although many methods have been proposed in the past few years on how to perform each separate step efficiently , there is still limited success in eventually finding highly compact and discriminative patterns . the culprit is due to the inherent nature of this widely adopted two-step approach . this paper discusses these problems and proposes a new and different method . it builds a decision tree that partitions the data onto different nodes . then at each node , it directly discovers a discriminative pattern to further divide its examples into purer subsets . since the number of examples towards leaf level is relatively small , the new approach is able to examine patterns with extremely low global support that could not be enumerated on the whole dataset by the two-step method . the discovered feature vectors are more accurate on some of the most difficult graph as well as frequent itemset problems than most recently proposed algorithms but the total size is typically 50 % or more smaller . importantly , the minimum support of some discriminative patterns can be extremely low ( e.g. 0.03 % ) . in order to enumerate these low support patterns , state-of-the-art frequent pattern algorithm either can not finish due to huge memory consumption or have to enumerate 101 to 103 times more patterns before they can even be found . software and datasets are available by contacting the author .

['frequent pattern', 'graph mining', 'itemsets', 'scalability']


analyzing patterns of user content generation in online social networks various online social networks ( osns ) have been developed rapidly on the internet . researchers have analyzed different properties of such osns , mainly focusing on the formation and evolution of the networks as well as the information propagation over the networks . in knowledge-sharing osns , such as blogs and question answering systems , issues on how users participate in the network and how users `` generate\/contribute '' knowledge are vital to the sustained and healthy growth of the networks . however , related discussions have not been reported in the research literature . in this work , we empirically study workloads from three popular knowledge-sharing osns , including a blog system , a social bookmark sharing network , and a question answering social network to examine these properties . our analysis consistently shows that ( 1 ) users ' posting behavior in these networks exhibits strong daily and weekly patterns , but the user active time in these osns does not follow exponential distributions ; ( 2 ) the user posting behavior in these osns follows stretched exponential distributions instead of power-law distributions , indicating the influence of a small number of core users can not dominate the network ; ( 3 ) the distributions of user contributions on high-quality and effort-consuming contents in these osns have smaller stretch factors for the stretched exponential distribution . our study provides insights into user activity patterns and lays out an analytical foundation for further understanding various properties of these osns .

['social networks', 'stretched exponential distribution', 'user generated content', 'user/machine systems']


collective annotation of wikipedia entities in web text to take the first step beyond keyword-based search toward entity-based search , suitable token spans ( `` spots '' ) on documents must be identified as references to real-world entities from an entity catalog . several systems have been proposed to link spots on web pages to entities in wikipedia . they are largely based on local compatibility between the text around the spot and textual metadata associated with the entity . two recent systems exploit inter-label dependencies , but in limited ways . we propose a general collective disambiguation approach . our premise is that coherent documents refer to entities from one or a few related topics or domains . we give formulations for the trade-off between local spot-to-entity compatibility and measures of global coherence between entities . optimizing the overall entity assignment is np-hard . we investigate practical solutions based on local hill-climbing , rounding integer linear programs , and pre-clustering entities followed by local optimization within clusters . in experiments involving over a hundred manually-annotated web pages and tens of thousands of spots , our approaches significantly outperform recently-proposed algorithms .

['collective inference', 'entity annotation/disambiguation', 'information search and retrieval', 'wikipedia']

wikipedia NOUN pobj in

mining social networks for personalized email prioritization email is one of the most prevalent communication tools today , and solving the email overload problem is pressingly urgent . a good way to alleviate email overload is to automatically prioritize received messages according to the priorities of each user . however , research on statistical learning methods for fully personalized email prioritization ( pep ) has been sparse due to privacy issues , since people are reluctant to share personal messages and importance judgments with the research community . it is therefore important to develop and evaluate pep methods under the assumption that only limited training examples can be available , and that the system can only have the personal email data of each user during the training and testing of the model for that user . this paper presents the first study ( to the best of our knowledge ) under such an assumption . specifically , we focus on analysis of personal social networks to capture user groups and to obtain rich features that represent the social roles from the viewpoint of a particular user . we also developed a novel semi-supervised ( transductive ) learning algorithm that propagates importance labels from training examples to test examples through message and user nodes in a personal email network . these methods together enable us to obtain an enriched vector representation of each new email message , which consists of both standard features of an email message ( such as words in the title or body , sender and receiver ids , etc. ) and the induced social features from the sender and receivers of the message . using the enriched vector representation as the input in svm classifiers to predict the importance level for each test message , we obtained significant performance improvement over the baseline system ( without induced social features ) in our experiments on a multi-user data collection . we obtained significant performance improvement over the baseline system ( without induced social features ) in our experiments on a multi-user data collection : the relative error reduction in mae was 31 % in micro-averaging , and 14 % in macro-averaging .

['applications', 'clustering', 'email prioritization', 'miscellaneous', 'social network', 'text mining']


semi-supervised learning with data calibration for long-term time series forecasting many time series prediction methods have focused on single step or short term prediction problems due to the inherent difficulty in controlling the propagation of errors from one prediction step to the next step . yet , there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecasting capabilities for strategic decision making . training an accurate model that produces reliable long term predictions would require an extensive amount of historical data , which are either unavailable or expensive to acquire . for some of these domains , there are alternative ways to generate potential scenarios for the future using computer-driven simulation models , such as global climate and traffic demand models . however , the data generated by these models are currently utilized in a supervised learning setting , where a predictive model trained on past observations is used to estimate the future values . in this paper , we present a semi-supervised learning framework for long-term time series forecasting based on hidden markov model regression . a covariance alignment method is also developed to deal with the issue of inconsistencies between historical and model simulation data . we evaluated our approach on data sets from a variety of domains , including climate modeling . our experimental results demonstrate the efficacy of the approach compared to other supervised learning methods for long-term time series forecasting .

['miscellaneous', 'semi-supervised learning', 'time series prediction']


a visual-analytic toolkit for dynamic interaction graphs in this article we describe a visual-analytic tool for the interrogation of evolving interaction network data such as those found in social , bibliometric , www and biological applications . the tool we have developed incorporates common visualization paradigms such as zooming , coarsening and filtering while naturally integrating information extracted by a previously described event-driven framework for characterizing the evolution of such networks . the visual front-end provides features that are specifically useful in the analysis of interaction networks , capturing the dynamic nature of both individual entities as well as interactions among them . the tool provides the user with the option of selecting multiple views , designed to capture different aspects of the evolving graph from the perspective of a node , a community or a subset of nodes of interest . standard visual templates and cues are used to highlight critical changes that have occurred during the evolution of the network . a key challenge we address in this work is that of scalability - handling large graphs both in terms of the efficiency of the back-end , and in terms of the efficiency of the visual layout and rendering . two case studies based on bibliometric and wikipedia data are presented to demonstrate the utility of the toolkit for visual knowledge discovery .

['dynamic interaction networks', 'graph visualization', 'visual analytics']


new ensemble methods for evolving data streams advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applications demanding such processing increases . online mining when such data streams evolve over time , that is when concepts drift or change completely , is becoming one of the core issues . when tackling non-stationary concepts , ensembles of classifiers have several advantages over single classifier methods : they are easy to scale and parallelize , they can adapt to change quickly by pruning under-performing parts of the ensemble , and they therefore usually also generate more accurate concept descriptions . this paper proposes a new experimental data stream framework for studying concept drift , and two new variants of bagging : adwin bagging and adaptive-size hoeffding tree ( asht ) bagging . using the new experimental framework , an evaluation study on synthetic and real-world datasets comprising up to ten million examples shows that the new ensemble methods perform very well compared to several known methods .

['concept drift', 'data streams', 'decision trees', 'ensemble methods']


the cost of privacy : destruction of data-mining utility in anonymized data publishing re-identification is a major privacy threat to public datasets containing individual records . many privacy protection algorithms rely on generalization and suppression of `` quasi-identifier '' attributes such as zip code and birthdate . their objective is usually syntactic sanitization : for example , k-anonymity requires that each `` quasi-identifier '' tuple appear in at least k records , while l-diversity requires that the distribution of sensitive attributes for each quasi-identifier have high entropy . the utility of sanitized data is also measured syntactically , by the number of generalization steps applied or the number of records with the same quasi-identifier . in this paper , we ask whether generalization and suppression of quasi-identifiers offer any benefits over trivial sanitization which simply separates quasi-identifiers from sensitive attributes . previous work showed that k-anonymous databases can be useful for data mining , but k-anonymization does not guarantee any privacy . by contrast , we measure the tradeoff between privacy ( how much can the adversary learn from the sanitized records ? ) and utility , measured as accuracy of data-mining algorithms executed on the same sanitized records . for our experimental evaluation , we use the same datasets from the uci machine learning repository as were used in previous research on generalization and suppression . our results demonstrate that even modest privacy gains require almost complete destruction of the data-mining utility . in most cases , trivial sanitization provides equivalent utility and better privacy than k-anonymity , l-diversity , and similar methods based on generalization and suppression .

['anonymity', 'privacy', 'utility']

utility NOUN pobj of
privacy NOUN compound threat
privacy NOUN compound protection
anonymity NOUN nsubj requires
utility NOUN nsubjpass measured
privacy NOUN dobj guarantee
privacy NOUN pobj between
utility NOUN conj learn
privacy NOUN compound gains
utility NOUN pobj of
utility NOUN dobj provides
privacy NOUN conj utility
anonymity NOUN pobj than

probabilistic frequent itemset mining in uncertain databases probabilistic frequent itemset mining in uncertain transaction databases semantically and computationally differs from traditional techniques applied to standard `` certain '' transaction databases . the consideration of existential uncertainty of item ( sets ) , indicating the probability that an item ( set ) occurs in a transaction , makes traditional techniques inapplicable . in this paper , we introduce new probabilistic formulations of frequent itemsets based on possible world semantics . in this probabilistic context , an itemset x is called frequent if the probability that x occurs in at least minsup transactions is above a given threshold ï„ . to the best of our knowledge , this is the first approach addressing this problem under possible worlds semantics . in consideration of the probabilistic formulations , we present a framework which is able to solve the probabilistic frequent itemset mining ( pfim ) problem efficiently . an extensive experimental evaluation investigates the impact of our proposed techniques and shows that our approach is orders of magnitude faster than straight-forward approaches .

['frequent itemset mining', 'probabilistic data', 'probabilistic frequent itemsets', 'uncertain databases']


ranking-based clustering of heterogeneous information networks with star network schema a heterogeneous information network is an information network composed of multiple types of objects . clustering on such a network may lead to better understanding of both hidden structures of the network and the individual role played by every object in each cluster . however , although clustering on homogeneous networks has been studied over decades , clustering on heterogeneous networks has not been addressed until recently . a recent study proposed a new algorithm , rankclus , for clustering on bi-typed heterogeneous networks . however , a real-world network may consist of more than two types , and the interactions among multi-typed objects play a key role at disclosing the rich semantics that a network carries . in this paper , we study clustering of multi-typed heterogeneous networks with a star network schema and propose a novel algorithm , netclus , that utilizes links across multityped objects to generate high-quality net-clusters . an iterative enhancement method is developed that leads to effective ranking-based clustering in such heterogeneous networks . our experiments on dblp data show that netclus generates more accurate clustering results than the baseline topic model algorithm plsa and the recently proposed algorithm , rankclus . further , netclus generates informative clusters , presenting good ranking and cluster membership information for each attribute object in each net-cluster .

['clustering', 'heterogeneous information network']

clustering NOUN nsubj schema
clustering VERB csubj lead
clustering VERB nsubjpass studied
clustering VERB nsubjpass addressed
clustering VERB pcomp for
clustering NOUN dobj study
clustering NOUN pobj to
clustering NOUN compound results

on compressing social networks motivated by structural properties of the web graph that support efficient data structures for in memory adjacency queries , we study the extent to which a large network can be compressed . boldi and vigna ( www 2004 ) , showed that web graphs can be compressed down to three bits of storage per edge ; we study the compressibility of social networks where again adjacency queries are a fundamental primitive . to this end , we propose simple combinatorial formulations that encapsulate efficient compressibility of graphs . we show that some of the problems are np-hard yet admit effective heuristics , some of which can exploit properties of social networks such as link reciprocity . our extensive experiments show that social networks and the web graph exhibit vastly different compressibility characteristics .

['compression', 'linear arrangement', 'reciprocity', 'social networks']

reciprocity NOUN pobj as

modeling and predicting user behavior in sponsored search implicit user feedback , including click-through and subsequent browsing behavior , is crucial for evaluating and improving the quality of results returned by search engines . several recent studies ( 1 , 2 , 3 , 13 , 25 ) have used post-result browsing behavior including the sites visited , the number of clicks , and the dwell time on site in order to improve the ranking of search results . in this paper , we first study user behavior on sponsored search results ( i.e. , the advertisements displayed by search engines next to the organic results ) , and compare this behavior to that of organic results . second , to exploit post-result user behavior for better ranking of sponsored results , we focus on identifying patterns in user behavior and predict expected on-site actions in future instances . in particular , we show how post-result behavior depends on various properties of the queries , advertisement , sites , and users , and build a classifier using properties such as these to predict certain aspects of the user behavior . additionally , we develop a generative model to mimic trends in observed user activity using a mixture of pareto distributions . we conduct experiments based on billions of real navigation trails collected by a major search engine 's browser toolbar .

['general', 'implicit feedback', 'sponsored search', 'user behavior']


a parallel learning algorithm for text classification text classification is the process of classifying documents into predefined categories based on their content . existing supervised learning algorithms to automatically classify text need sufficient labeled documents to learn accurately . applying the expectation-maximization ( em ) algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled documents to augment the available labeled documents . unfortunately , the time needed to learn with these large unlabeled documents is too high . this paper introduces a novel parallel learning algorithm for text classification task . the parallel algorithm is based on the combination of the em algorithm and the naive bayes classifier . our goal is to improve the computational time in learning and classifying process . we studied the performance of our parallel algorithm on a large linux pc cluster called pirun cluster . we report both timing and accuracy results . these results indicate that the proposed parallel algorithm is capable of handling large document collections .

['cluster computing', 'naive bayes', 'parallel expectation-maximization algorithm', 'text classification']


probabilistic workflow mining in several organizations , it has become increasingly popular to document and log the steps that makeup a typical business process . in some situations , a normative workflow model of such processes is developed , and it becomes important to know if such a model is actually being followed by analyzing the available activity logs . in other scenarios , no model is available and , with the purpose of evaluating cases or creating new production policies , one is interested in learning a workflow representation of such activities . in either case , machine learning tools that can mine workflow models are of great interest and still relatively unexplored . we present here a probabilistic workflow model and a corresponding learning algorithm that runs in polynomial time . we illustrate the algorithm on example data derived from a real world workflow .

['causal models', 'graphical models', 'probability and statistics', 'workflow mining']


on the potential of domain literature for clustering and bayesian network learning thanks to its increasing availability , electronic literature can now be a major source of information when developing complex statistical models where data is scarce or contains much noise . this raises the question of how to integrate information from domain literature with statistical data . because quantifying similarities or dependencies between variables is a basic building block in knowledge discovery , we consider here the following question . which vector representations of text and which statistical scores of similarity or dependency support best the use of literature in statistical models ? for the text source , we assume to have annotations for the domain variables as short free-text descriptions and optionally to have a large literature repository from which we can further expand the annotations . for evaluation , we contrast the variables similarities or dependencies obtained from text using different annotation sources and vector representations with those obtained from measurement data or expert assessments . specifically , we consider two learning problems : clustering and bayesian network learning . firstly , we report performance ( against an expert reference ) for clustering yeast genes from textual annotations . secondly , we assess the agreement between text-based and data-based scores of variable dependencies when learning bayesian network substructures for the task of modeling the joint distribution of clinical measurements of ovarian tumors .

['bayesian networks', 'clustering', 'clustering', 'data mining', 'text mining']

clustering NOUN appos problems
clustering NOUN pcomp for

is there a grand challenge or x-prize for data mining ? this panel will discuss possible exciting and motivating grand challenge problems for data mining , focusing on bioinformatics , multimedia mining , link mining , text mining , and web mining .

['bioinformatics', 'grand challenge', 'image mining', 'link mining', 'multimedia mining', 'text mining', 'video mining', 'web mining', 'x-prize']

bioinformatics NOUN pobj on

tracking multiple topics for finding interesting articles we introduce multiple topic tracking ( mtt ) for iscore to better recommend news articles for users with multiple interests and to address changes in user interests over time . as an extension of the basic rocchio algorithm , traditional topic detection and tracking , and single-pass clustering , mtt maintains multiple interest profiles to identify interesting articles for a specific user given user-feedback . focusing on only interesting topics enables iscore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy . also by relating a topic 's interestingness to an article . s interestingness , iscore is able to achieve higher quality results than traditional methods such as the rocchio algorithm . we identify several operating parameters that work well for mtt . using the same parameters , we show that mtt alone yields high quality results for recommending interesting articles from several corpora . the inclusion of mtt improves iscore 's performance by 9 % in recommending news articles from the yahoo ! news rss feeds and the trec11 adaptive filter article collection . and through a small user study , we show that iscore can still perform well when only provided with little user feedback .

['content analysis and indexing', 'news filtering', 'news recommendation', 'personalization']


spatial-temporal causal modeling for climate change attribution attribution of climate change to causal factors has been based predominantly on simulations using physical climate models , which have inherent limitations in describing such a complex and chaotic system . we propose an alternative , data centric , approach that relies on actual measurements of climate observations and human and natural forcing factors . specifically , we develop a novel method to infer causality from spatial-temporal data , as well as a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate events , such as heatwaves . our experimental results on a real world dataset indicate that changes in temperature are not solely accounted for by solar radiance , but attributed more significantly to co2 and other greenhouse gases . combined with extreme value modeling , we also show that there has been a significant increase in the intensity of extreme temperatures , and that such changes in extreme temperature are also attributable to greenhouse gases . these preliminary results suggest that our approach can offer a useful alternative to the simulation-based approach to climate modeling and attribution , and provide valuable insights from a fresh perspective .

['climate change attribution', 'extreme value modeling', 'graphical granger modeling', 'spatio-temporal causal modeling']


efficient influence maximization in social networks influence maximization is the problem of finding a small subset of nodes ( seed nodes ) in a social network that could maximize the spread of influence . in this paper , we study the efficient influence maximization from two complementary directions . one is to improve the original greedy algorithm of ( 5 ) and its improvement ( 7 ) to further reduce its running time , and the second is to propose new degree discount heuristics that improves influence spread . we evaluate our algorithms by experiments on two large academic collaboration graphs obtained from the online archival database arxiv.org . our experimental results show that ( a ) our improved greedy algorithm achieves better running time comparing with the improvement of ( 7 ) with matching influence spread , ( b ) our degree discount heuristics achieve much better influence spread than classic degree and centrality-based heuristics , and when tuned for a specific influence cascade model , it achieves almost matching influence thread with the greedy algorithm , and more importantly ( c ) the degree discount heuristics run only in milliseconds while even the improved greedy algorithms run in hours in our experiment graphs with a few tens of thousands of nodes . based on our results , we believe that fine-tuned heuristics may provide truly scalable solutions to the influence maximization problem with satisfying influence spread and blazingly fast running time . therefore , contrary to what implied by the conclusion of ( 5 ) that traditional heuristics are outperformed by the greedy approximation algorithm , our results shed new lights on the research of heuristic algorithms .

['heuristic algorithms', 'influence maximization', 'nonnumerical algorithms and problems', 'social networks']


connections between the lines : augmenting social networks with text network data is ubiquitous , encoding collections of relationships between entities such as people , places , genes , or corporations . while many resources for networks of interesting entities are emerging , most of these can only annotate connections in a limited fashion . although relationships between entities are rich , it is impractical to manually devise complete characterizations of these relationships for every pair of entities on large , real-world corpora . in this paper we present a novel probabilistic topic model to analyze text corpora and infer descriptions of its entities and of relationships between those entities . we develop variational methods for performing approximate inference on our model and demonstrate that our model can be practically deployed on large corpora such as wikipedia . we show qualitatively and quantitatively that our model can construct and annotate graphs of relationships and make useful predictions .

['graphical models', 'social network learning', 'statistical topic models']


influence and correlation in social networks in many online social systems , social ties between users play an important role in dictating their behavior . one of the ways this can happen is through social influence , the phenomenon that the actions of a user can induce his\/her friends to behave in a similar way . in systems where social influence exists , ideas , modes of behavior , or new technologies can diffuse through the network like an epidemic . therefore , identifying and understanding social influence is of tremendous interest from both analysis and design points of view . this is a difficult task in general , since there are factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network . distinguishing influence from these is essentially the problem of distinguishing correlation from causality , a notoriously hard statistical problem . in this paper we study this problem systematically . we define fairly general models that replicate the aforementioned sources of social correlation . we then propose two simple tests that can identify influence as a source of social correlation when the time series of user actions is available . we give a theoretical justification of one of the tests by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation . we also simulate our tests on a number of examples designed by randomly generating actions of nodes on a real social network ( from flickr ) according to one of several models . simulation results confirm that our test performs well on these data . finally , we apply them to real tagging data on flickr , exhibiting that while there is significant social correlation in tagging behavior on this system , this correlation can not be attributed to social influence .

['correlation', 'social influence', 'social networks', 'tagging']

correlation NOUN conj influence
correlation NOUN dobj induce
correlation NOUN dobj distinguishing
correlation NOUN pobj of
correlation NOUN pobj of
correlation NOUN pobj of
tagging VERB amod data
correlation NOUN attr is
tagging VERB pcomp in
correlation NOUN nsubjpass attributed

named entity mining from click-through data using weakly supervised latent dirichlet allocation this paper addresses named entity mining ( nem ) , in which we mine knowledge about named entities such as movies , games , and books from a huge amount of data . nem is potentially useful in many applications including web search , online advertisement , and recommender system . there are three challenges for the task : finding suitable data source , coping with the ambiguities of named entity classes , and incorporating necessary human supervision into the mining process . this paper proposes conducting nem by using click-through data collected at a web search engine , employing a topic model that generates the click-through data , and learning the topic model by weak supervision from humans . specifically , it characterizes each named entity by its associated queries and urls in the click-through data . it uses the topic model to resolve ambiguities of named entity classes by representing the classes as topics . it employs a method , referred to as weakly supervised latent dirichlet allocation ( ws-lda ) , to accurately learn the topic model with partially labeled named entities . experiments on a large scale click-through data containing over 1.5 billion query-url pairs show that the proposed approach can conduct very accurate nem and significantly outperforms the baseline .

['named entity recognition', 'search log mining', 'topic model', 'web mining']


audience selection for on-line brand advertising : privacy-friendly social network targeting this paper describes and evaluates privacy-friendly methods for extracting quasi-social networks from browser behavior on user-generated content sites , for the purpose of finding good audiences for brand advertising ( as opposed to click maximizing , for example ) . targeting social-network neighbors resonates well with advertisers , and on-line browsing behavior data counterintuitively can allow the identification of good audiences anonymously . besides being one of the first papers to our knowledge on data mining for on-line brand advertising , this paper makes several important contributions . we introduce a framework for evaluating brand audiences , in analogy to predictive-modeling holdout evaluation . we introduce methods for extracting quasi-social networks from data on visitations to social networking pages , without collecting any information on the identities of the browsers or the content of the social-network pages . we introduce measures of brand proximity in the network , and show that audiences with high brand proximity indeed show substantially higher brand affinity . finally , we provide evidence that the quasi-social network embeds a true social network , which along with results from social theory offers one explanation for the increase in brand affinity of the selected audiences .

['on-line advertising', 'predictive modeling', 'privacy', 'social networks', 'user-generated content']


large-scale behavioral targeting behavioral targeting ( bt ) leverages historical user behavior to select the ads most relevant to users to display . the state-of-the-art of bt derives a linear poisson regression model from fine-grained user behavioral data and predicts click-through rate ( ctr ) from user history . we designed and implemented a highly scalable and efficient solution to bt using hadoop mapreduce framework . with our parallel algorithm and the resulting system , we can build above 450 bt-category models from the entire yahoo 's user base within one day , the scale that one can not even imagine with prior systems . moreover , our approach has yielded 20 % ctr lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset . specifically , our major contributions include : ( 1 ) a mapreduce statistical learning algorithm and implementation that achieve optimal data parallelism , task parallelism , and load balance in spite of the typically skewed distribution of domain data . ( 2 ) an in-place feature vector generation algorithm with linear time complexity o ( n ) regardless of the granularity of sliding target window . ( 3 ) an in-memory caching scheme that significantly reduces the number of disk ios to make large-scale learning practical . ( 4 ) highly efficient data structures and sparse representations of models and data to enable fast model updates . we believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general . finally , we report comprehensive experimental results , using industrial proprietary codebase and datasets .

['behavioral targeting', 'design methodology', 'grid computing', 'large-scale']

large-scale ADJ compound behavioral

turning down the noise in the blogosphere in recent years , the blogosphere has experienced a substantial increase in the number of posts published daily , forcing users to cope with information overload . the task of guiding users through this flood of information has thus become critical . to address this issue , we present a principled approach for picking a set of posts that best covers the important stories in the blogosphere . we define a simple and elegant notion of coverage and formalize it as a submodular optimization problem , for which we can efficiently compute a near-optimal solution . in addition , since people have varied interests , the ideal coverage algorithm should incorporate user preferences in order to tailor the selected posts to individual tastes . we define the problem of learning a personalized coverage function by providing an appropriate user-interaction model and formalizing an online learning framework for this task . we then provide a no-regret algorithm which can quickly learn a user 's preferences from limited feedback . we evaluate our coverage and personalization algorithms extensively over real blog data . results from a user study show that our simple coverage algorithm does as well as most popular blog aggregation sites , including google blog search , yahoo ! buzz , and digg . furthermore , we demonstrate empirically that our algorithm can successfully adapt to user preferences . we believe that our technique , especially with personalization , can dramatically reduce information overload .

['blogs', 'learning', 'personalization']

learning VERB pcomp of
learning NOUN compound framework
personalization NOUN conj coverage
personalization NOUN pobj with

issues in evaluation of stream learning algorithms learning from data streams is a research area of increasing importance . nowadays , several stream learning algorithms have been developed . most of them learn decision models that continuously evolve over time , run in resource-aware environments , detect and react to changes in the environment generating data . one important issue , not yet conveniently addressed , is the design of experimental work to evaluate and compare decision models that evolve over time . there are no golden standards for assessing performance in non-stationary environments . this paper proposes a general framework for assessing predictive stream learning algorithms . we defend the use of predictive sequential methods for error estimate - the prequential error . the prequential error allows us to monitor the evolution of the performance of models that evolve over time . nevertheless , it is known to be a pessimistic estimator in comparison to holdout estimates . to obtain more reliable estimators we need some forgetting mechanism . two viable alternatives are : sliding windows and fading factors . we observe that the prequential error converges to an holdout estimator when estimated over a sliding window or using fading factors . we present illustrative examples of the use of prequential error estimators , using fading factors , for the tasks of : i ) assessing performance of a learning algorithm ; ii ) comparing learning algorithms ; iii ) hypothesis testing using mcnemar test ; and iv ) change detection using page-hinkley test . in these tasks , the prequential error estimated using fading factors provide reliable estimators . in comparison to sliding windows , fading factors are faster and memory-less , a requirement for streaming applications . this paper is a contribution to a discussion in the good-practices on performance assessment when learning dynamic models that evolve over time .

['data streams', 'evaluation design']


summarizing itemset patterns : a profile-based approach frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets , sequences , and graphs . however , the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability , due to the huge number of patterns generated by the mining process . in this paper , we examine how to summarize a collection of itemset patterns using only k representatives , a small number of patterns that a user can handle easily . the k representatives should not only cover most of the frequent patterns but also approximate their supports . a generative model is built to extract and profile these representatives , under which the supports of the patterns can be easily recovered without consulting the original dataset . based on the restoration error , we propose a quality measure function to determine the optimal value of parameter k. polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement . empirical studies indicate that we can obtain compact summarization in real datasets .

['frequent pattern', 'probabilistic model', 'summarization']

summarization NOUN dobj obtain

time series shapelets : a new primitive for data mining classification of time series has been attracting great interest over the past decade . recent empirical evidence has strongly suggested that the simple nearest neighbor algorithm is very difficult to beat for most time series problems . while this may be considered good news , given the simplicity of implementing the nearest neighbor algorithm , there are some negative consequences of this . first , the nearest neighbor algorithm requires storing and searching the entire dataset , resulting in a time and space complexity that limits its applicability , especially on resource-limited sensors . second , beyond mere classification accuracy , we often wish to gain some insight into the data . in this work we introduce a new time series primitive , time series shapelets , which addresses these limitations . informally , shapelets are time series subsequences which are in some sense maximally representative of a class . as we shall show with extensive empirical evaluations in diverse domains , algorithms based on the time series shapelet primitives can be interpretable , more accurate and significantly faster than state-of-the-art classifiers .

['classification', 'pattern extraction']

classification NOUN compound accuracy

differentially private recommender systems : building privacy into the net we consider the problem of producing recommendations from collective user behavior while simultaneously providing guarantees of privacy for these users . specifically , we consider the netflix prize data set , and its leading algorithms , adapted to the framework of differential privacy . unlike prior privacy work concerned with cryptographically securing the computation of recommendations , differential privacy constrains a computation in a way that precludes any inference about the underlying records from its output . such algorithms necessarily introduce uncertainty -- i.e. , noise -- to computations , trading accuracy for privacy . we find that several of the leading approaches in the netflix prize competition can be adapted to provide differential privacy , without significantly degrading their accuracy . to adapt these algorithms , we explicitly factor them into two parts , an aggregation\/learning phase that can be performed with differential privacy guarantees , and an individual recommendation phase that uses the learned correlations and an individual 's data to provide personalized recommendations . the adaptations are non-trivial , and involve both careful analysis of the per-record sensitivity of the algorithms to calibrate noise , as well as new post-processing steps to mitigate the impact of this noise . we measure the empirical trade-off between accuracy and privacy in these adaptations , and find that we can provide non-trivial formal privacy guarantees while still outperforming the cinematch baseline netflix provides .

['differential privacy', 'netflix', 'recommender systems']

netflix NOUN compound prize
netflix NOUN compound prize
netflix NOUN nsubj provides

clustering event logs using iterative partitioning the importance of event logs , as a source of information in systems and network management can not be overemphasized . with the ever increasing size and complexity of today 's event logs , the task of analyzing event logs has become cumbersome to carry out manually . for this reason recent research has focused on the automatic analysis of these log files . in this paper we present iplom ( iterative partitioning log mining ) , a novel algorithm for the mining of clusters from event logs . through a 3-step hierarchical partitioning process iplom partitions log data into its respective clusters . in its 4th and final stage iplom produces cluster descriptions or line formats for each of the clusters produced . unlike other similar algorithms iplom is not based on the apriori algorithm and it is able to find clusters in data whether or not its instances appear frequently . evaluations show that iplom outperforms the other algorithms statistically significantly , and it is also able to achieve an average f-measure performance 78 % when the closest other algorithm achieves an f-measure performance of 10 % .

['event log mining', 'fault management', 'telecommunications']


combining email models for false positive reduction machine learning and data mining can be effectively used to model , classify and discover interesting information for a wide variety of data including email . the email mining toolkit , emt , has been designed to provide a wide range of analyses for arbitrary email sources . depending upon the task , one can usually achieve very high accuracy , but with some amount of false positive tradeoff . generally false positives are prohibitively expensive in the real world . in the case of spam detection , for example , even if one email is misclassified , this may be unacceptable if it is a very important email . much work has been done to improve specific algorithms for the task of detecting unwanted messages , but less work has been report on leveraging multiple algorithms and correlating models in this particular domain of email analysis . emt has been updated with new correlation functions allowing the analyst to integrate a number of emt 's user behavior models available in the core technology . we present results of combining classifier outputs for improving both accuracy and reducing false positives for the problem of spam detection . we apply these methods to a very large email data set and show results of different combination methods on these corpora . we introduce a new method to compare multiple and combined classifiers , and show how it differs from past work . the method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combinations of classifiers to automatically choose the best combination .

['aggregators', 'data mining', 'email mining', 'false positive reduction', 'model combination', 'multiple classifiers', 'spam']

spam NOUN compound detection
spam NOUN compound detection

finding a team of experts in social networks given a task t , a pool of individuals x with different skills , and a social network g that captures the compatibility among these individuals , we study the problem of finding x , a subset of x , to perform the task . we call this the team formation problem . we require that members of x ' not only meet the skill requirements of the task , but can also work effectively together as a team . we measure effectiveness using the communication cost incurred by the subgraph in g that only involves x ' . we study two variants of the problem for two different communication-cost functions , and show that both variants are np-hard . we explore their connections with existing combinatorial problems and give novel algorithms for their solution . to the best of our knowledge , this is the first work to consider the team formation problem in the presence of a social network of individuals . experiments on the dblp dataset show that our framework works well in practice and gives useful and intuitive results .

['social networks', 'team formation']


